{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/vladsavelyev/deeplearning/blob/master/karpathy/nanogpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "fname = 'tinyshakespeare.txt'\n",
    "if not os.path.exists(fname):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "with open('tinyshakespeare.txt') as f:\n",
    "    text = f.read()\n",
    "print(len(text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "class Split(NamedTuple):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "    def get_batch(self, batch_size: int) -> 'Split':\n",
    "        dataset_size = self.x.shape[0]\n",
    "        block_size = self.x.shape[1]\n",
    "        ix = torch.randint((dataset_size - block_size), (batch_size,), device=device)\n",
    "        return Split(self.x[ix], self.y[ix])\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, text: str, block_size: int, split_pct=0.9):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(chars)\n",
    "        self.block_size = block_size\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        data = torch.tensor(self.encode(text), dtype=torch.long, device=device)\n",
    "        n = int(split_pct * len(data))\n",
    "        self.train = self.build_split(data[:n])\n",
    "        self.test = self.build_split(data[n:])\n",
    "        \n",
    "    def encode(self, s: str):\n",
    "        return [self.stoi[ch] for ch in s]\n",
    "   \n",
    "    def decode(self, ints):\n",
    "        return ''.join(self.itos[i] for i in ints)\n",
    "\n",
    "    def build_split(self, data: torch.Tensor) -> Split:\n",
    "        x = torch.stack([\n",
    "            data[ix:ix + self.block_size]\n",
    "            for ix in range(len(data) - self.block_size)\n",
    "        ])\n",
    "        y = torch.stack([\n",
    "            data[ix + 1:ix + 1 + self.block_size]\n",
    "            for ix in range(len(data) - self.block_size)\n",
    "        ])\n",
    "        return Split(x, y)\n",
    "\n",
    "dataset = Dataset(text, block_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 3])\n",
      "xb: ['s i', 'his', 'thi', 'thi']\n",
      "targets:\n",
      "torch.Size([4, 3])\n",
      "yb: [' is', 'is ', 'his', 'his']\n",
      "\n",
      "When input is \"s\", the target is \" \"\n",
      "When input is \"s \", the target is \"i\"\n",
      "When input is \"s i\", the target is \"s\"\n",
      "When input is \"h\", the target is \"i\"\n",
      "When input is \"hi\", the target is \"s\"\n",
      "When input is \"his\", the target is \" \"\n",
      "When input is \"t\", the target is \"h\"\n",
      "When input is \"th\", the target is \"i\"\n",
      "When input is \"thi\", the target is \"s\"\n",
      "When input is \"t\", the target is \"h\"\n",
      "When input is \"th\", the target is \"i\"\n",
      "When input is \"thi\", the target is \"s\"\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def print_batch_info(dataset, x, y):\n",
    "    print('inputs:')\n",
    "    print(x.shape)\n",
    "    print('xb:', [dataset.decode(t.tolist()) for t in x])\n",
    "    print('targets:')\n",
    "    print(y.shape)\n",
    "    print('yb:', [dataset.decode(t.tolist()) for t in y])\n",
    "    print()\n",
    "    batch_size = x.shape[0]\n",
    "    block_size = x.shape[1]\n",
    "    for b in range(batch_size):  # batch dimension\n",
    "        for t in range(block_size):  # time dimension\n",
    "            context = x[b, :t + 1]\n",
    "            target = y[b, t]\n",
    "            print(\n",
    "                f'When input is \"{dataset.decode(context.tolist())}\", '\n",
    "                f'the target is \"{dataset.decode([target.item()])}\"')\n",
    "\n",
    "toy = Dataset('this is a test data!', block_size=3, split_pct=0.5)\n",
    "xb, yb = toy.train.get_batch(batch_size=4)\n",
    "print_batch_info(toy, xb, yb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising model...\n",
      "Model with 4161 params: AttentionModel(\n",
      "  (token_embedding_table): Embedding(65, 8)\n",
      "  (pos_embedding_table): Embedding(65, 8)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (1): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (2): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (3): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (projection): Linear(in_features=8, out_features=8, bias=True)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=8, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (1): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (2): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (3): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (projection): Linear(in_features=8, out_features=8, bias=True)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=8, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (1): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (2): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "          (3): Head(\n",
      "            (key): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (query): Linear(in_features=8, out_features=2, bias=False)\n",
      "            (value): Linear(in_features=8, out_features=2, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (projection): Linear(in_features=8, out_features=8, bias=True)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=8, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)  # (batch, time, channels)\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(b * t, c), targets.view(b * t)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        # idx is (b, t) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions:\n",
    "            logits, _ = self(idx)\n",
    "            # focus only on the last time step:\n",
    "            logits = logits[:, -1, :]  # becomes (b, c)\n",
    "            probs = logits.softmax(-1)\n",
    "            # sample from the distribution:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (b, t+1)\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size: int, embedding_dim: int, block_size: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.ones((block_size, block_size)).tril())\n",
    "\n",
    "    def forward(self, x):  # x.shape=(B, T, C=vocab_size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # B, T, C -> B, T, H\n",
    "        q = self.query(x)   # B, T, C -> B, T, H\n",
    "        # compute attention scores (\"affinities\")\n",
    "        w = k @ q.transpose(-2, -1) * C**-0.5  # (B, T, H) @ (B, H, T) -> B, T, T\n",
    "\n",
    "        # w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        # w = w.softmax(-1)  # B, T, T\n",
    "\n",
    "        v = self.value(x)  # B, T, H\n",
    "        out = w @ v  # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size, embedding_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, embedding_dim, block_size)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim, bias=False),  # projection layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, block_size: int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim // num_heads\n",
    "        # Communication part (share info with other tokens):\n",
    "        self.sa = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            head_size=head_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            block_size=block_size,\n",
    "        )\n",
    "        # Computation part (fully-connected nn on self):\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, block_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(num_heads=4, embedding_dim=embedding_dim, block_size=block_size),\n",
    "            Block(num_heads=4, embedding_dim=embedding_dim, block_size=block_size),\n",
    "            Block(num_heads=4, embedding_dim=embedding_dim, block_size=block_size),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):  # (B, T)\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.pos_embedding_table(\n",
    "            torch.arange(self.block_size, device=device)\n",
    "        )  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # (B, T, H=C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(b * t, c), targets.view(b * t)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        # idx is (b, t) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions:\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step:\n",
    "            logits = logits[:, -1, :]  # becomes (b, c)\n",
    "            probs = logits.softmax(-1)\n",
    "            # sample from the distribution:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (b, t+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "print('Initialising model...')\n",
    "m = AttentionModel(\n",
    "    dataset.vocab_size, \n",
    "    block_size=dataset.block_size, \n",
    "    embedding_dim=8,\n",
    ")\n",
    "m.to(device)\n",
    "print(f'Model with {sum(p.nelement() for p in m.parameters())} params: {m}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimising model...\n",
      "Step 0: train_loss=4.3299, test_loss=4.335618\n",
      "Step 100: train_loss=3.4326, test_loss=3.466338\n",
      "Step 200: train_loss=3.2624, test_loss=3.299008\n",
      "Step 300: train_loss=3.1437, test_loss=3.172911\n",
      "Step 400: train_loss=3.0011, test_loss=3.018553\n",
      "Step 500: train_loss=2.9006, test_loss=2.914062\n",
      "Step 600: train_loss=2.8298, test_loss=2.842434\n",
      "Step 700: train_loss=2.7721, test_loss=2.784171\n",
      "Step 800: train_loss=2.7307, test_loss=2.740764\n",
      "Step 900: train_loss=2.7092, test_loss=2.709004\n",
      "Step 1000: train_loss=2.6693, test_loss=2.669655\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m: nn.Module, split: Split, batch_size: int, eval_iters: int = 200) -> int:\n",
    "    m.eval()\n",
    "    out = torch.tensor([\n",
    "        m(*split.get_batch(batch_size))[1]\n",
    "        for _ in range(eval_iters)\n",
    "    ], device=device).mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "batch_size = 32\n",
    "n_steps = 1001\n",
    "lr = 1e-3\n",
    "\n",
    "losses = []\n",
    "last_outs = []\n",
    "last_grads = []\n",
    "linear_w_update_rates_by_parameter = {}\n",
    "DEBUG = False\n",
    "\n",
    "print('Optimising model...')\n",
    "optimiser = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "for step_i in range(n_steps):\n",
    "    if step_i == 0 or step_i % (n_steps // 10) == 0:\n",
    "        train_loss = estimate_loss(m, dataset.train, batch_size)\n",
    "        test_loss = estimate_loss(m, dataset.test, batch_size)\n",
    "        print(f'Step {step_i}: {train_loss=:.4f}, {test_loss=:4f}')\n",
    "\n",
    "    logits, loss = m(*dataset.train.get_batch(batch_size))\n",
    "    if DEBUG:\n",
    "        logits.retain_grad()\n",
    "        for p in [p for p in m.parameters() if len(p.shape) == 2]:\n",
    "            p.retain_grad()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if DEBUG:\n",
    "        with torch.no_grad():\n",
    "            ps = [p for p in m.parameters() if len(p.shape) == 2]\n",
    "            for pi, p in enumerate(ps):\n",
    "                if pi not in linear_w_update_rates_by_parameter:\n",
    "                    linear_w_update_rates_by_parameter[pi] = []\n",
    "                linear_w_update_rates_by_parameter[pi].append(\n",
    "                    (lr * ps[0].grad.std() / ps[0].data.std()).abs().log10().item()\n",
    "                )\n",
    "            if step_i % (n_steps // 10) == 0 or n_steps == 1:\n",
    "                last_outs.append(logits)\n",
    "                last_grads.append(logits.grad)\n",
    "                assert logits.grad is not None\n",
    "\n",
    "# print('Evaluating model...')\n",
    "# print(f'Final test split loss: {m(*dataset.test)[1]:4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = torch.tensor(losses)\n",
    "window_size = n_steps // 500\n",
    "if n_steps > 500:\n",
    "    round_size = len(t) // window_size * window_size\n",
    "    t1 = t[:round_size].view(-1, window_size).mean(1)\n",
    "    t2 = t[round_size:].view(-1, 1).mean(1)\n",
    "    t = torch.cat([t1, t2], dim=0)\n",
    "plt.plot(torch.arange(len(t)) * window_size, t);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generating from model...\n",
      "First Citizen:\n",
      "Before we proceed as os bereou stas o\n",
      "\n",
      "QLSouve bourare -thid y ' rethe s thowt y -\n",
      "ferelamoco biuritmanik,\n",
      "\n",
      "Mssest, thens F Ios be; IM,\n",
      "H:\n",
      "LTMuromis an onee orl met tor my hafo, sr itd lo QOaathe!\n",
      "Aerisen; f h; fey li\n"
     ]
    }
   ],
   "source": [
    "print('> Generating from model...')\n",
    "idx = torch.tensor([dataset.encode(text[:32])]).to(device)\n",
    "res = m.generate(idx, 200)\n",
    "print(dataset.decode(res[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generating from model...\n",
      "First Citizen:\n",
      "Before we proceedel m:\n",
      "\n",
      "\n",
      "TARETATCRA\n",
      "\n",
      "LRUa dome, to hor sarAIR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Txor te ,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PN' om:\n",
      "CELRTIi omeno,\n",
      " ENe,, OBA\n",
      "\n",
      "is se ISn tae' womed h mce bnre!G ith.\n",
      "Z\n",
      "Notfos iBThheat t laaridf fa.\n",
      "AArthaslimd d tthasals s RA' n\n"
     ]
    }
   ],
   "source": [
    "print('> Generating from model...')\n",
    "idx = torch.tensor([dataset.encode(text[:32])]).to(device)\n",
    "res = m.generate(idx, 200)\n",
    "print(dataset.decode(res[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histogram of weights for each linear layer (not over time).\n",
    "legends = []\n",
    "linear_layers = [\n",
    "    b.ffwd.net[0] for b in m.blocks\n",
    "] + [m.lm_head]\n",
    "for li, l in enumerate(linear_layers):\n",
    "    w_count, w = torch.histogram(l.weight.data, density=True)\n",
    "    plt.plot(w[:-1].detach(), w_count.detach())\n",
    "    legends.append(f'layer {li} ({l.__class__.__name__})')\n",
    "plt.legend(legends)\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(0, 0.45);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histogram of weights GRADIENTS for each linear layer (not over time).\n",
    "legends = []\n",
    "for li, l in enumerate(linear_layers):\n",
    "    print(\n",
    "        f'weight {str(tuple(l.weight.shape)):<10}: '\n",
    "        f'mean: {l.weight.grad.mean().item():+.4f}, std: {l.weight.grad.std().item():.4f}, '\n",
    "        f'grad:data: {(l.weight.grad / l.weight.data).mean().item():+.4f}'\n",
    "    )\n",
    "    w_grad_count, w_grad = torch.histogram(l.weight.grad, density=True)\n",
    "    plt.plot(w_grad[:-1].detach(), w_grad_count.detach())\n",
    "    legends.append(f'layer {li} ({l.__class__.__name__})')\n",
    "plt.legend(legends)\n",
    "plt.xlim((-0.04, 0.04));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Final linear layer outputs histogram for each step.\n",
    "# Linear layers outs should stay close to unit gaussian.\n",
    "legends = []\n",
    "for i, t in enumerate(last_outs):\n",
    "    legends.append(f'step {i * (n_steps // 10) + 1}')\n",
    "    counts, xs = torch.histogram(t, density=True)\n",
    "    plt.plot(xs[:-1].detach(), counts.detach())\n",
    "    sat = (t.abs() > 0.97).float().mean().item()\n",
    "    print(f'step {i * (n_steps // 10) + 1:<4}: '\n",
    "          f'mean={t.mean().item():+.4f}  '\n",
    "          f'std={t.std().item():+.4f}  '\n",
    "          f'sat={sat:+.4f}')\n",
    "plt.legend(legends)\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(0, 0.45);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hist of tanh outputs over time. \n",
    "# Tanh of a gaussian with long tails will have peaks close to -1 and 1, \n",
    "# meaning that such neurons won't train and will saturate i.e. any small \n",
    "# change to a large input of tanh wouldn't change the output.\n",
    "# So we should not expect to see very high peaks of tanh.\n",
    "legends = []\n",
    "for i, t in enumerate(last_outs):\n",
    "    t = t.tanh()\n",
    "    legends.append(f'step {i * (n_steps // 10) + 1}')\n",
    "    counts, xs = torch.histogram(t, density=True)\n",
    "    plt.plot(xs[:-1].detach(), counts.detach())\n",
    "    sat = (t.abs() > 0.97).float().mean().item()\n",
    "    print(f'step {i * (n_steps // 10) + 1:<4}: '\n",
    "          f'mean={t.mean().item():+.4f}  '\n",
    "          f'std={t.std().item():+.4f}  '\n",
    "          f'sat={sat:+.4f}')\n",
    "plt.legend(legends)\n",
    "plt.ylim((0, 2));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histogram of tanh output gradients. Following the reasoning above, we \n",
    "# shouldn't see a lot at zero.\n",
    "legends = []\n",
    "for i, t in enumerate(last_grads):\n",
    "    counts, xs = torch.histogram(t, density=True)\n",
    "    plt.plot(xs[:-1].detach(), counts.detach())\n",
    "    legends.append(f'step {i * (n_steps // 10)}')\n",
    "plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Generating from model...\n",
      "First Citizen:\n",
      "Before we proceed our entreatenting,\n",
      "I would my ragg.\n",
      "\n",
      "GLOUCESTER:\n",
      "Nay.\n",
      "O, itdoties, as were us of thy's weary,\n",
      "And feed the they adding, one to lies,\n",
      "Geroas with old nothing not theer deep,\n",
      "God-Jriach time minictions to texes advistagen:\n",
      "So Glouy, I would have feather hushis grace sing of times,\n",
      "Parge the purince at\n",
      "the donoaring vil claur, you biriet your revery?\n",
      "\n",
      "LEONTES:\n",
      "No! my puries pace die, I do stral,\n",
      "Les\n"
     ]
    }
   ],
   "source": [
    "print('> Generating from model...')\n",
    "idx = torch.tensor([dataset.encode(text[:32])]).to(device)\n",
    "res = m.generate(idx, 400)\n",
    "print(dataset.decode(res[0].tolist()))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "P3TuHYl_2RrX",
    "outputId": "f7f3eeb7-0278-46b6-97ef-1f7294b16b0f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eo0JK8C32RrX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# version 1\n",
    "# we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow0 = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1]\n",
    "        xbow0[b, t] = torch.mean(xprev, 0)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8eqrLfdm2RrX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])"
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((3, 3)))\n",
    "tril / tril.sum(dim=1, keepdim=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Deu4ULwh2RrY",
    "outputId": "cb0bd7d3-8312-4259-b4ae-e04239918100"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = tril / tril.sum(dim=1, keepdim=True)\n",
    "xbow1 = wei @ x  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow0, xbow1)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VAdhXUTY2RrY",
    "outputId": "2871a6ca-bdbe-4bec-e77c-e1be6f023403"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = tril.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei.softmax(-1)\n",
    "xbow2 = wei @ x\n",
    "torch.allclose(xbow0, xbow2)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "D7jML-Ae2RrY",
    "outputId": "b9739101-cd5f-4cfa-e4ac-e3170ae3cc9e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.shape=torch.Size([4, 8, 16]) q.shape=torch.Size([4, 8, 16]) k.transpose(-2, -1).shape=torch.Size([4, 16, 8])\n",
      "x.shape=torch.Size([4, 8, 32]) v.shape=torch.Size([4, 8, 16])\n",
      "wei.shape=torch.Size([4, 8, 8]) out.shape=torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)     # B, T, 16\n",
    "q = query(x)   # B, T, 16\n",
    "print(f'{k.shape=} {q.shape=} {k.transpose(-2, -1).shape=}')\n",
    "wei = q @ k.transpose(-2, -1)  # B, T, 16 @ B, 16, T -> B, T, T\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei.softmax(-1)\n",
    "\n",
    "v = value(x)\n",
    "print(f'{x.shape=} {v.shape=}')\n",
    "out = wei @ v\n",
    "print(f'{wei.shape=} {out.shape=}')\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hvy9fDXA2RrY",
    "outputId": "d4285ccd-adee-47f6-e163-bf1cd8f790f8"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}