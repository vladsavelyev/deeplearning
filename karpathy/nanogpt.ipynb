{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/vladsavelyev/deeplearning/blob/master/karpathy/nanogpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, requests\n",
    "fname = 'tinyshakespeare.txt'\n",
    "if not os.path.exists(fname):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "with open('tinyshakespeare.txt') as f:\n",
    "    text = f.read()\n",
    "print(len(text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "class Split(NamedTuple):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "    def get_batch(self, batch_size: int) -> 'Split':\n",
    "        dataset_size = self.x.shape[0]\n",
    "        block_size = self.x.shape[1]\n",
    "        ix = torch.randint((dataset_size - block_size), (batch_size,), device=device)\n",
    "        return Split(self.x[ix], self.y[ix])\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, text: str, block_size: int, split_pct=0.9):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(chars)\n",
    "        self.block_size = block_size\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        data = torch.tensor(self.encode(text), dtype=torch.long, device=device)\n",
    "        n = int(split_pct * len(data))\n",
    "        self.train = self.build_split(data[:n])\n",
    "        self.test = self.build_split(data[n:])\n",
    "        \n",
    "    def encode(self, s: str):\n",
    "        return [self.stoi[ch] for ch in s]\n",
    "   \n",
    "    def decode(self, ints):\n",
    "        return ''.join(self.itos[i] for i in ints)\n",
    "\n",
    "    def build_split(self, data: torch.Tensor) -> Split:\n",
    "        x = torch.stack([\n",
    "            data[ix:ix + self.block_size]\n",
    "            for ix in range(len(data) - self.block_size)\n",
    "        ])\n",
    "        y = torch.stack([\n",
    "            data[ix + 1:ix + 1 + self.block_size]\n",
    "            for ix in range(len(data) - self.block_size)\n",
    "        ])\n",
    "        return Split(x, y)\n",
    "\n",
    "dataset = Dataset(text, block_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def print_batch_info(dataset, x, y):\n",
    "    print('inputs:')\n",
    "    print(x.shape)\n",
    "    print('xb:', [dataset.decode(t.tolist()) for t in x])\n",
    "    print('targets:')\n",
    "    print(y.shape)\n",
    "    print('yb:', [dataset.decode(t.tolist()) for t in y])\n",
    "    print()\n",
    "    batch_size = x.shape[0]\n",
    "    block_size = x.shape[1]\n",
    "    for b in range(batch_size):  # batch dimension\n",
    "        for t in range(block_size):  # time dimension\n",
    "            context = x[b, :t + 1]\n",
    "            target = y[b, t]\n",
    "            print(\n",
    "                f'When input is \"{dataset.decode(context.tolist())}\", '\n",
    "                f'the target is \"{dataset.decode([target.item()])}\"')\n",
    "\n",
    "toy = Dataset('this is a test data!', block_size=3, split_pct=0.5)\n",
    "xb, yb = toy.train.get_batch(batch_size=4)\n",
    "print_batch_info(toy, xb, yb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)  # (batch, time, channels)\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(b * t, c), targets.view(b * t)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        # idx is (b, t) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions:\n",
    "            logits, _ = self(idx)\n",
    "            # focus only on the last time step:\n",
    "            logits = logits[:, -1, :]  # becomes (b, c)\n",
    "            probs = logits.softmax(-1)\n",
    "            # sample from the distribution:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (b, t+1)\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size: int, embedding_dim: int, block_size: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.ones((block_size, block_size)).tril())\n",
    "\n",
    "    def forward(self, x):  # x.shape=(B, T, C=vocab_size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # B, T, C -> B, T, H\n",
    "        q = self.query(x)   # B, T, C -> B, T, H\n",
    "        # compute attention scores (\"affinities\")\n",
    "        w = k @ q.transpose(-2, -1) * C**-0.5  # (B, T, H) @ (B, H, T) -> B, T, T\n",
    "\n",
    "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        w = w.softmax(-1)  # B, T, T\n",
    "\n",
    "        v = self.value(x)  # B, T, H\n",
    "        out = w @ v  # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size, embedding_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, embedding_dim, block_size)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim, bias=False),  # projection layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, block_size: int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim // num_heads\n",
    "        # Communication part (share info with other tokens):\n",
    "        self.sa = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            head_size=head_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            block_size=block_size,\n",
    "        )\n",
    "        # Computation part (fully-connected nn on self):\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, block_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(num_heads=4, embedding_dim=embedding_dim, block_size=block_size),\n",
    "            Block(num_heads=4, embedding_dim=embedding_dim, block_size=block_size),\n",
    "            Block(num_heads=4, embedding_dim=embedding_dim, block_size=block_size),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):  # (B, T)\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.pos_embedding_table(\n",
    "            torch.arange(self.block_size, device=device)\n",
    "        )  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # (B, T, H=C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(b * t, c), targets.view(b * t)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        # idx is (b, t) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions:\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step:\n",
    "            logits = logits[:, -1, :]  # becomes (b, c)\n",
    "            probs = logits.softmax(-1)\n",
    "            # sample from the distribution:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (b, t+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "print('Initialising model...')\n",
    "m = AttentionModel(\n",
    "    dataset.vocab_size, \n",
    "    block_size=dataset.block_size, \n",
    "    embedding_dim=64,\n",
    ")\n",
    "m.to(device)\n",
    "print(f'Model with {sum(p.nelement() for p in m.parameters())} params: {m}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m: nn.Module, split: Split, batch_size: int, eval_iters: int = 200) -> int:\n",
    "    m.eval()\n",
    "    out = torch.tensor([\n",
    "        m(*split.get_batch(batch_size))[1]\n",
    "        for _ in range(eval_iters)\n",
    "    ], device=device).mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "batch_size = 32\n",
    "n_steps = 5001\n",
    "lr = 1e-3\n",
    "\n",
    "losses = []\n",
    "last_outs = []\n",
    "last_grads = []\n",
    "linear_w_update_rates_by_parameter = {}\n",
    "DEBUG = False\n",
    "\n",
    "print('Optimising model...')\n",
    "optimiser = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "for step_i in range(n_steps):\n",
    "    if step_i == 0 or step_i % (n_steps // 10) == 0:\n",
    "        train_loss = estimate_loss(m, dataset.train, batch_size)\n",
    "        test_loss = estimate_loss(m, dataset.test, batch_size)\n",
    "        print(f'Step {step_i}: {train_loss=:.4f}, {test_loss=:4f}')\n",
    "\n",
    "    logits, loss = m(*dataset.train.get_batch(batch_size))\n",
    "    if DEBUG:\n",
    "        logits.retain_grad()\n",
    "        for p in [p for p in m.parameters() if len(p.shape) == 2]:\n",
    "            p.retain_grad()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if DEBUG:\n",
    "        with torch.no_grad():\n",
    "            ps = [p for p in m.parameters() if len(p.shape) == 2]\n",
    "            for pi, p in enumerate(ps):\n",
    "                if pi not in linear_w_update_rates_by_parameter:\n",
    "                    linear_w_update_rates_by_parameter[pi] = []\n",
    "                linear_w_update_rates_by_parameter[pi].append(\n",
    "                    (lr * ps[0].grad.std() / ps[0].data.std()).abs().log10().item()\n",
    "                )\n",
    "            if step_i % (n_steps // 10) == 0 or n_steps == 1:\n",
    "                last_outs.append(logits)\n",
    "                last_grads.append(logits.grad)\n",
    "                assert logits.grad is not None\n",
    " \n",
    "# print('Evaluating model...')\n",
    "# print(f'Final test split loss: {m(*dataset.test)[1]:4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = torch.tensor(losses)\n",
    "window_size = n_steps // 500\n",
    "if n_steps > 500:\n",
    "    round_size = len(t) // window_size * window_size\n",
    "    t1 = t[:round_size].view(-1, window_size).mean(1)\n",
    "    t2 = t[round_size:].view(-1, 1).mean(1)\n",
    "    t = torch.cat([t1, t2], dim=0)\n",
    "plt.plot(torch.arange(len(t)) * window_size, t);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histogram of weights for each linear layer (not over time).\n",
    "legends = []\n",
    "linear_layers = [\n",
    "    b.ffwd.net[0] for b in m.blocks\n",
    "] + [m.lm_head]\n",
    "for li, l in enumerate(linear_layers):\n",
    "    w_count, w = torch.histogram(l.weight.data, density=True)\n",
    "    plt.plot(w[:-1].detach(), w_count.detach())\n",
    "    legends.append(f'layer {li} ({l.__class__.__name__})')\n",
    "plt.legend(legends)\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(0, 0.45);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histogram of weights GRADIENTS for each linear layer (not over time).\n",
    "legends = []\n",
    "for li, l in enumerate(linear_layers):\n",
    "    print(\n",
    "        f'weight {str(tuple(l.weight.shape)):<10}: '\n",
    "        f'mean: {l.weight.grad.mean().item():+.4f}, std: {l.weight.grad.std().item():.4f}, '\n",
    "        f'grad:data: {(l.weight.grad / l.weight.data).mean().item():+.4f}'\n",
    "    )\n",
    "    w_grad_count, w_grad = torch.histogram(l.weight.grad, density=True)\n",
    "    plt.plot(w_grad[:-1].detach(), w_grad_count.detach())\n",
    "    legends.append(f'layer {li} ({l.__class__.__name__})')\n",
    "plt.legend(legends)\n",
    "plt.xlim((-0.04, 0.04));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Final linear layer outputs histogram for each step.\n",
    "# Linear layers outs should stay close to unit gaussian.\n",
    "legends = []\n",
    "for i, t in enumerate(last_outs):\n",
    "    legends.append(f'step {i * (n_steps // 10) + 1}')\n",
    "    counts, xs = torch.histogram(t, density=True)\n",
    "    plt.plot(xs[:-1].detach(), counts.detach())\n",
    "    sat = (t.abs() > 0.97).float().mean().item()\n",
    "    print(f'step {i * (n_steps // 10) + 1:<4}: '\n",
    "          f'mean={t.mean().item():+.4f}  '\n",
    "          f'std={t.std().item():+.4f}  '\n",
    "          f'sat={sat:+.4f}')\n",
    "plt.legend(legends)\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(0, 0.45);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hist of tanh outputs over time. \n",
    "# Tanh of a gaussian with long tails will have peaks close to -1 and 1, \n",
    "# meaning that such neurons won't train and will saturate i.e. any small \n",
    "# change to a large input of tanh wouldn't change the output.\n",
    "# So we should not expect to see very high peaks of tanh.\n",
    "legends = []\n",
    "for i, t in enumerate(last_outs):\n",
    "    t = t.tanh()\n",
    "    legends.append(f'step {i * (n_steps // 10) + 1}')\n",
    "    counts, xs = torch.histogram(t, density=True)\n",
    "    plt.plot(xs[:-1].detach(), counts.detach())\n",
    "    sat = (t.abs() > 0.97).float().mean().item()\n",
    "    print(f'step {i * (n_steps // 10) + 1:<4}: '\n",
    "          f'mean={t.mean().item():+.4f}  '\n",
    "          f'std={t.std().item():+.4f}  '\n",
    "          f'sat={sat:+.4f}')\n",
    "plt.legend(legends)\n",
    "plt.ylim((0, 2));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histogram of tanh output gradients. Following the reasoning above, we \n",
    "# shouldn't see a lot at zero.\n",
    "legends = []\n",
    "for i, t in enumerate(last_grads):\n",
    "    counts, xs = torch.histogram(t, density=True)\n",
    "    plt.plot(xs[:-1].detach(), counts.detach())\n",
    "    legends.append(f'step {i * (n_steps // 10)}')\n",
    "plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting update rate over step, for each layer. Ideal update rate should be \n",
    "# around 1/1000 which corresponds to the factor of -3 on the plot. Meaning, \n",
    "# that the update (learning rate std times gradient std?) is 1/1000th of data.\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for pi, t in linear_w_update_rates_by_parameter.items():\n",
    "    plt.plot(t)\n",
    "    legends.append(f'Parameter {pi}')\n",
    "plt.plot([0, len(linear_w_update_rates_by_parameter[1])], [-3, -3], 'k')\n",
    "# plt.legend(legends);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('> Generating from model...')\n",
    "idx = torch.tensor([dataset.encode(text[:32])]).to(device)\n",
    "res = m.generate(idx, 400)\n",
    "print(dataset.decode(res[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Generating from model...\n",
      "First Citizen:\n",
      "Before we proceed our entreatenting,\n",
      "I would my ragg.\n",
      "\n",
      "GLOUCESTER:\n",
      "Nay.\n",
      "O, itdoties, as were us of thy's weary,\n",
      "And feed the they adding, one to lies,\n",
      "Geroas with old nothing not theer deep,\n",
      "God-Jriach time minictions to texes advistagen:\n",
      "So Glouy, I would have feather hushis grace sing of times,\n",
      "Parge the purince at\n",
      "the donoaring vil claur, you biriet your revery?\n",
      "\n",
      "LEONTES:\n",
      "No! my puries pace die, I do stral,\n",
      "Les\n"
     ]
    }
   ],
   "source": [
    "print('> Generating from model...')\n",
    "idx = torch.tensor([dataset.encode(text[:32])]).to(device)\n",
    "res = m.generate(idx, 400)\n",
    "print(dataset.decode(res[0].tolist()))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "P3TuHYl_2RrX",
    "outputId": "f7f3eeb7-0278-46b6-97ef-1f7294b16b0f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eo0JK8C32RrX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# version 1\n",
    "# we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow0 = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1]\n",
    "        xbow0[b, t] = torch.mean(xprev, 0)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8eqrLfdm2RrX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])"
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((3, 3)))\n",
    "tril / tril.sum(dim=1, keepdim=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Deu4ULwh2RrY",
    "outputId": "cb0bd7d3-8312-4259-b4ae-e04239918100"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = tril / tril.sum(dim=1, keepdim=True)\n",
    "xbow1 = wei @ x  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow0, xbow1)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VAdhXUTY2RrY",
    "outputId": "2871a6ca-bdbe-4bec-e77c-e1be6f023403"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = tril.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei.softmax(-1)\n",
    "xbow2 = wei @ x\n",
    "torch.allclose(xbow0, xbow2)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "D7jML-Ae2RrY",
    "outputId": "b9739101-cd5f-4cfa-e4ac-e3170ae3cc9e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.shape=torch.Size([4, 8, 16]) q.shape=torch.Size([4, 8, 16]) k.transpose(-2, -1).shape=torch.Size([4, 16, 8])\n",
      "x.shape=torch.Size([4, 8, 32]) v.shape=torch.Size([4, 8, 16])\n",
      "wei.shape=torch.Size([4, 8, 8]) out.shape=torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)     # B, T, 16\n",
    "q = query(x)   # B, T, 16\n",
    "print(f'{k.shape=} {q.shape=} {k.transpose(-2, -1).shape=}')\n",
    "wei = q @ k.transpose(-2, -1)  # B, T, 16 @ B, 16, T -> B, T, T\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei.softmax(-1)\n",
    "\n",
    "v = value(x)\n",
    "print(f'{x.shape=} {v.shape=}')\n",
    "out = wei @ v\n",
    "print(f'{wei.shape=} {out.shape=}')\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hvy9fDXA2RrY",
    "outputId": "d4285ccd-adee-47f6-e163-bf1cd8f790f8"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}