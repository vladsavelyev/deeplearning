{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## makemore: becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sFElPqq8PPp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# There no change in the first several cells from last lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ChBbac4y8PPq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt  # for making figures\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# download the names.txt file from github\n",
    "!wget https: // raw.githubusercontent.com / karpathy / makemore / master / names.txt"
   ],
   "metadata": {
    "id": "x6GhEWW18aCS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dyld[59552]: Library not loaded: '/usr/local/opt/libunistring/lib/libunistring.2.dylib'\r\n",
      "  Referenced from: '/usr/local/Cellar/wget/1.21.1/bin/wget'\r\n",
      "  Reason: tried: '/usr/local/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/usr/local/lib/libunistring.2.dylib' (no such file), '/usr/lib/libunistring.2.dylib' (no such file), '/usr/local/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/usr/local/lib/libunistring.2.dylib' (no such file), '/usr/lib/libunistring.2.dylib' (no such file)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "klmu3ZG08PPr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BCQomLE_8PPs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "id": "V_zt2QHr8PPs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182484, 3]) torch.Size([182484])\n",
      "torch.Size([22869, 3]) torch.Size([22869])\n",
      "torch.Size([22793, 3]) torch.Size([22793])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3  # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])  # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])  # 10%\n",
    "Xte, Yte = build_dataset(words[n2:])  # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ok boilerplate done, now we get to the action:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "id": "ZlFLjQyT8PPu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn((n_embd * block_size, n_hidden), generator=g) * \n",
    "        (5/3) / ((n_embd * block_size) ** 0.5)\n",
    ")\n",
    "# using b1 just for fun, it's useless because of BN:\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bn_gain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bn_bias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "# Note: I am initializing many of these parameters in non-standard ways\n",
    "# because sometimes initializing with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "id": "QY-y96Y48PPv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size  # a shorter variable also, for convenience\n",
    "# construct a minibatch:\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "id": "8ofj1s6d8PPv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(3.5976, grad_fn=<NegBackward0>)"
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "emb = C[Xb]  # embed the characters into vectors\n",
    "emb_cat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "# Linear layer 1\n",
    "h_pre_bn = emb_cat @ W1 + b1  # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bn_mean_i = 1 / n * h_pre_bn.sum(0, keepdim=True)\n",
    "bn_diff = h_pre_bn - bn_mean_i\n",
    "bn_diff_squared = bn_diff ** 2\n",
    "# note: Bessel's correction (dividing by n-1, not n)\n",
    "bn_var = 1 / (n - 1) * bn_diff_squared.sum(0, keepdim=True)  \n",
    "bn_var_inv = (bn_var + 1e-5) ** -0.5\n",
    "bn_raw = bn_diff * bn_var_inv\n",
    "h_pre_act = bn_gain * bn_raw + bn_bias\n",
    "# Non-linearity\n",
    "h = torch.tanh(h_pre_act)  # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2  # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum ** -1  # if I use (1.0 / counts_sum) instead then I can't get backprop to be a bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "log_probs = probs.log()\n",
    "loss = -log_probs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [\n",
    "    # afaik there is no cleaner way\n",
    "    log_probs, probs, counts, counts_sum, counts_sum_inv,\n",
    "    norm_logits, logit_maxes, logits, h, h_pre_act, bn_raw,\n",
    "    bn_var_inv, bn_var, bn_diff_squared, bn_diff, h_pre_bn, bn_mean_i,\n",
    "    emb_cat, emb\n",
    "]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "id": "mO-8aqxK8PPw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (200) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[569], line 106\u001B[0m\n\u001B[1;32m    100\u001B[0m d_h_pre_act \u001B[38;5;241m=\u001B[39m dh \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m h\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# cmp('h_pre_act', d_h_pre_act, h_pre_act)\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m# h_pre_act = bn_gain * bn_raw + bn_bias\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m# print(f'{h_pre_act.shape=} {bn_gain.shape=} {bn_raw.shape=} {bn_bias.shape=}')\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;66;03m# print(f'{bn_raw.sum(0, keepdim=True).shape=}, {((d_h_pre_act * bn_raw).sum(0, keepdim=True)).shape=}')\u001B[39;00m\n\u001B[0;32m--> 106\u001B[0m d_bn_gain \u001B[38;5;241m=\u001B[39m (\u001B[43md_h_pre_act\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbn_raw\u001B[49m)\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;241m0\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;66;03m# cmp('bn_gain', d_bn_gain, bn_gain)\u001B[39;00m\n\u001B[1;32m    109\u001B[0m d_bn_bias \u001B[38;5;241m=\u001B[39m d_h_pre_act\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;241m0\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (200) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt: torch.Tensor, t: torch.Tensor):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    diff = (dt - t.grad).abs()\n",
    "    maxdiff_idx = torch.argmax(diff)\n",
    "    print(\n",
    "        f'{s:15s}'\n",
    "        f' | exact: {str(ex):5s}'\n",
    "        f' | approx: {str(app):5s}'\n",
    "        f' | max diff: {diff.flatten()[maxdiff_idx]}'\n",
    "        f' | max expected: {t.grad.flatten()[maxdiff_idx]}'\n",
    "        f' | max calculated: {dt.flatten()[maxdiff_idx]}'\n",
    "    )\n",
    "\n",
    "# Exercise 1: backprop through the whole thing manually, backpropagating through exactly \n",
    "# all the variables as they are defined in the forward pass above, one by one.\n",
    "\n",
    "# -----------------\n",
    "# loss = -log_probs[range(n), Yb].mean()\n",
    "# 1. x.mean() = x.sum() / size\n",
    "# 2. x.sum() sums all elements in x. However, we are interested in only how a single element impacts \n",
    "# the objective function. So for us, x.sum() = x1 + a1 + a2 + a3..., with a1, a2, a3, ... being constants\n",
    "# that do not affect the gradient at all. So the gradient of x.sum() with respect to x1 is 1.0.\n",
    "# 3. For x.mean(), we should divide that gradient by the total number of elements of x. So the effect of\n",
    "# one element x1 is only its share in the array x, which is 1.0 / len(x).\n",
    "# 4. So the gradient of loss with respect to one element in log_probs is: -1.0 / log_probs[range(n), Yb].nelement(),\n",
    "# but only for those elements in log_probs[range(n), Yb]; for the rest of elements in log_probs, it's 0.\n",
    "# 5. For every element in log_probs, it would be:\n",
    "d_log_probs = torch.zeros_like(log_probs)\n",
    "size = log_probs[range(n), Yb].nelement()\n",
    "d_log_probs[range(n), Yb] = -1.0 / size\n",
    "# cmp('log_probs', d_log_probs, log_probs)\n",
    "\n",
    "# log_probs = probs.log()\n",
    "# derivative of log(x) is 1/x\n",
    "d_probs = probs**-1 * d_log_probs\n",
    "# cmp('probs', d_probs, probs)\n",
    "\n",
    "# probs = counts * counts_sum_inv\n",
    "# counts_sum_inv is (32, 1), so its second dimension is broadcast to make it (32, 27).\n",
    "# So this multiplication is in fact equivalent to something like: \n",
    "# probs = counts * [counts_sum_inv, counts_sum_inv, counts_sum_inv, ...].\n",
    "# The derivative of just the multiplication is just counts (times d_probs for the chain rule),\n",
    "# and the derivative of broadcasting is accumulating the gradient 27 times (each element in \n",
    "# counts_sum_inv impacts probs one time for each column in counts), so we can call\n",
    "# .sum(dim=1, keepdim=True) to accumulate and make it finally the same shape as counts_sum_inv.\n",
    "d_counts_sum_inv = (d_probs * counts).sum(dim=1, keepdim=True)\n",
    "# cmp('counts_sum_inv', d_counts_sum_inv, counts_sum_inv)\n",
    "\n",
    "# counts_sum_inv = counts_sum ** -1\n",
    "d_counts_sum = d_counts_sum_inv * -counts_sum**-2\n",
    "# cmp('counts_sum', d_counts_sum, counts_sum)\n",
    "\n",
    "# counts used twice:\n",
    "# 1. counts_sum = counts.sum(1, keepdim=True)\n",
    "d_counts1 = torch.ones_like(counts) * d_counts_sum\n",
    "# 2. probs = counts * counts_sum_inv = counts * (counts_sum ** -1) = counts * (counts.sum(1, keepdim=True) ** -1)\n",
    "d_counts2 = counts_sum_inv * d_probs\n",
    "# ...so we calculate its derivative twice and summing it up\n",
    "d_counts = d_counts1 + d_counts2\n",
    "# cmp('counts', d_counts, counts)\n",
    "\n",
    "# counts = norm_logits.exp() -> d_counts /d_norm_logits = norm_logits.exp() = counts\n",
    "d_norm_logits = counts * d_counts\n",
    "# cmp('norm_logits', d_norm_logits, norm_logits)\n",
    "\n",
    "# norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "d_logit_maxes = -d_norm_logits.sum(dim=1, keepdim=True)\n",
    "# cmp('logit_maxes', d_logit_maxes, logit_maxes)\n",
    "\n",
    "# logits is used twice:\n",
    "# 1. norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "d_logits1 = d_norm_logits\n",
    "# 2. logit_maxes = logits.max(1, keepdim=True).values\n",
    "indices = logits.max(1).indices\n",
    "d_logits2 = torch.zeros_like(logits)\n",
    "d_logits2[range(32), indices] = d_logit_maxes.view(32)\n",
    "# another way of doing it:\n",
    "d_logits2 = F.one_hot(indices, num_classes=logits.shape[1]) * d_logit_maxes\n",
    "# finally, summing up all impacts\n",
    "d_logits = d_logits1 + d_logits2\n",
    "# cmp('logits', d_logits, logits)\n",
    "\n",
    "# logits = h @ W2 + b2  # output layer\n",
    "# print(f'{d_logits.shape=}, {h.shape=}, {W2.shape=}')\n",
    "# print(f'{W2.transpose(0, 1).shape=}, {(d_logits @ W2.transpose(0, 1)).shape=}')\n",
    "dh = d_logits @ W2.transpose(0, 1)\n",
    "# cmp('h', dh, h)\n",
    "\n",
    "# print(f'{W2.shape=}, {h.shape=}, {(h.transpose(0, 1) @ d_logits).shape=}')\n",
    "dW2 = h.transpose(0, 1) @ d_logits\n",
    "# cmp('W2', dW2, W2)\n",
    "\n",
    "# print(f'{b2.shape=}')\n",
    "db2 = d_logits.sum(dim=0)\n",
    "# cmp('b2', db2, b2)\n",
    "\n",
    "# h = torch.tanh(h_pre_act)  # hidden layer\n",
    "d_h_pre_act = dh * (1 - h**2)\n",
    "# cmp('h_pre_act', d_h_pre_act, h_pre_act)\n",
    "\n",
    "# h_pre_act = bn_gain * bn_raw + bn_bias\n",
    "# print(f'{h_pre_act.shape=} {bn_gain.shape=} {bn_raw.shape=} {bn_bias.shape=}')\n",
    "# print(f'{bn_raw.sum(0, keepdim=True).shape=}, {((d_h_pre_act * bn_raw).sum(0, keepdim=True)).shape=}')\n",
    "d_bn_gain = (d_h_pre_act * bn_raw).sum(0, keepdim=True)\n",
    "# cmp('bn_gain', d_bn_gain, bn_gain)\n",
    "\n",
    "d_bn_bias = d_h_pre_act.sum(0, keepdim=True)\n",
    "# cmp('bn_bias', d_bn_bias, bn_bias)\n",
    "\n",
    "d_bn_raw = d_h_pre_act * bn_gain\n",
    "# cmp('bn_raw', d_bn_raw, bn_raw)\n",
    "\n",
    "# bn_raw = bn_diff * bn_var_inv\n",
    "# print(f'{bn_raw.shape=} = {bn_diff.shape=} * {bn_var_inv.shape=}')\n",
    "d_bn_var_inv = (d_bn_raw * bn_diff).sum(0, keepdim=True)\n",
    "# cmp('bn_var_inv', d_bn_var_inv, bn_var_inv)\n",
    "\n",
    "d_bn_diff1 = d_bn_raw * bn_var_inv\n",
    "\n",
    "# bn_var_inv = (bn_var + 1e-5) ** -0.5\n",
    "d_bn_var = -0.5 * (bn_var + 1e-5)**-1.5 * d_bn_var_inv\n",
    "# cmp('bn_var', d_bn_var, bn_var)\n",
    "\n",
    "# bn_var = 1 / (n - 1) * bn_diff_squared.sum(0, keepdim=True)  \n",
    "# print(f'{bn_var.shape=}, {bn_diff_squared.shape=}')\n",
    "# print(f'{torch.ones_like(bn_diff_squared).shape=}')\n",
    "d_bn_diff_squared = torch.ones_like(bn_diff_squared) * 1 / (n - 1) * d_bn_var\n",
    "# cmp('bn_diff_squared', d_bn_diff_squared, bn_diff_squared)\n",
    "\n",
    "# bn_diff_squared = bn_diff ** 2\n",
    "d_bn_diff2 = 2 * bn_diff * d_bn_diff_squared\n",
    "d_bn_diff = d_bn_diff1 + d_bn_diff2\n",
    "# cmp('bn_diff', d_bn_diff, bn_diff)\n",
    "\n",
    "# bn_diff = h_pre_bn - bn_mean_i\n",
    "# print(f'{h_pre_bn.shape=}, {bn_mean_i.shape=}, {bn_diff.shape=}')\n",
    "d_bn_mean_i = -d_bn_diff.sum(0, keepdim=True)\n",
    "# cmp('bn_mean_i', d_bn_mean_i, bn_mean_i)\n",
    "\n",
    "# bn_mean_i = 1 / n * h_pre_bn.sum(0, keepdim=True)\n",
    "d_h_pre_bn1 = d_bn_diff\n",
    "# print(f'{d_h_pre_bn1.shape=}, {h_pre_bn.shape=}, {d_bn_mean_i.shape=}')\n",
    "d_h_pre_bn2 = torch.ones_like(h_pre_bn) * 1 / n * d_bn_mean_i\n",
    "d_h_pre_bn = d_h_pre_bn1 + d_h_pre_bn2\n",
    "# cmp('h_pre_bn', d_h_pre_bn, h_pre_bn)\n",
    "\n",
    "# h_pre_bn = emb_cat @ W1 + b1  # hidden layer pre-activation\n",
    "# print(f'{h_pre_bn.shape=} = {emb_cat.shape=} @ {W1.shape=} + {b1.shape=}')\n",
    "d_emb_cat = d_h_pre_bn @ W1.T\n",
    "dW1 = emb_cat.T @ d_h_pre_bn\n",
    "db1 = d_h_pre_bn.sum(0)\n",
    "# print(f'{b1.shape=} {db1.shape=} {h_pre_bn.shape=}')\n",
    "# cmp('emb_cat', d_emb_cat, emb_cat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "\n",
    "# emb_cat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "d_emb = d_emb_cat.view(emb.shape)\n",
    "# print(f'{emb.shape=}, {emb_cat.shape=}, {d_emb.shape=}')\n",
    "# cmp('emb', d_emb, emb)\n",
    "\n",
    "# emb = C[Xb]  # embed the characters into vectors\n",
    "# equivalent to: emb = F.one_hot(Xb, num_classes=27).float() @ C\n",
    "print(f'{emb.shape=} = {F.one_hot(Xb, num_classes=27).shape=} @ {C.shape=}')\n",
    "print()\n",
    "print(f'We have: {d_emb.shape=} and {F.one_hot(Xb, num_classes=27).shape=}')\n",
    "print(f'We need shape: {C.shape=}')\n",
    "permuted = F.one_hot(Xb, num_classes=27).permute(2, 0, 1)\n",
    "print(f'First, we permute the one-hot vector: {permuted.shape=}')\n",
    "print(f'Then we dot-multiply: {permuted.shape} @ {d_emb.shape}')\n",
    "dC = torch.tensordot(permuted.float(), d_emb, dims=2)\n",
    "print(f'Then we dot-multiply: {dC.shape}')\n",
    "assert dC.shape == C.shape\n",
    "print(f'{dC.shape=}')\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebLtYji_8PPw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdim=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "id": "-gCXbB4C8PPx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_logits.shape=torch.Size([32, 27]), probs.shape=torch.Size([32, 27]), logits.shape=torch.Size([32, 27])\n",
      "logits          | exact: False | approx: True  | max diff: 7.2177499532699585e-09 | max expected: 0.003694217186421156 | max calculated: 0.003694224404171109\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# loss_fast = F.cross_entropy(logits, Yb)\n",
    "d_logits = F.softmax(logits , 1)\n",
    "d_logits[torch.arange(n), Yb] -= 1\n",
    "size = d_logits[torch.arange(n), Yb].nelement()\n",
    "d_logits /= size\n",
    "print(f'{d_logits.shape=}, {probs.shape=}, {logits.shape=}')\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', d_logits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "id": "hd-MkhB68PPy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_pre_act.shape=torch.Size([32, 64]) h_pre_bn.shape=torch.Size([32, 64])\n",
      "bn_gain.shape=torch.Size([1, 64]) bn_bias.shape=torch.Size([1, 64])\n",
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "print(f'{h_pre_act.shape=} {h_pre_bn.shape=}')\n",
    "print(f'{bn_gain.shape=} {bn_bias.shape=}')\n",
    "\n",
    "# now:\n",
    "h_pre_act_fast = (\n",
    "    bn_gain * \n",
    "    (h_pre_bn - h_pre_bn.mean(0, keepdim=True)) / \n",
    "    torch.sqrt(h_pre_bn.var(0, keepdim=True, unbiased=True) + 1e-5) + \n",
    "    bn_bias\n",
    ")\n",
    "print('max diff:', (h_pre_act_fast - h_pre_act).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POdeZSKT8PPy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# d_h_pre_bn = None  # TODO. my solution is 1 (long) line\n",
    "d_h_pre_bn = (\n",
    "    bn_gain * bn_var_inv / n * (\n",
    "        n * d_h_pre_act - d_h_pre_act.sum(0) - n / (n - 1) * bn_raw * (\n",
    "            d_h_pre_act * bn_raw).sum(0)\n",
    "    )\n",
    ")  \n",
    "# -----------------\n",
    "\n",
    "# I can only get approximate to be true, my maxdiff is 9e-10\n",
    "cmp('hprebn', d_h_pre_bn, h_pre_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "id": "wPy8DhqB8PPz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.5973\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5 / 3) / (\n",
    "        (n_embd * block_size) ** 0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bn_gain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bn_bias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size  # convenience\n",
    "loss_i = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "#with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb]  # embed the characters into vectors\n",
    "    emb_cat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "    # Linear layer\n",
    "    h_pre_bn = emb_cat @ W1 + b1  # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bn_mean = h_pre_bn.mean(0, keepdim=True)\n",
    "    bn_var = h_pre_bn.var(0, keepdim=True, unbiased=True)\n",
    "    bn_var_inv = (bn_var + 1e-5) ** -0.5\n",
    "    bn_raw = (h_pre_bn - bn_mean) * bn_var_inv\n",
    "    h_pre_act = bn_gain * bn_raw + bn_bias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(h_pre_act)  # hidden layer\n",
    "    logits = h @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()  # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    d_logits = F.softmax(logits, 1)\n",
    "    d_logits[range(n), Yb] -= 1\n",
    "    d_logits /= n\n",
    "    \n",
    "    dh = d_logits @ W2.T\n",
    "    dW2 = h.T @ d_logits\n",
    "    db2 = d_logits.sum(0)\n",
    "    d_h_pre_act = dh * (1 - h**2)\n",
    "\n",
    "    d_bn_gain = (bn_raw * d_h_pre_act).sum(0, keepdim=True)\n",
    "    d_bn_bias = d_h_pre_act.sum(0, keepdim=True)\n",
    "    d_h_pre_bn = (\n",
    "        bn_gain * bn_var_inv / n * (\n",
    "            n * d_h_pre_act - d_h_pre_act.sum(0) - n / (n - 1) * bn_raw * (d_h_pre_act * bn_raw).sum(0)\n",
    "        )\n",
    "    )  \n",
    "   \n",
    "    d_emb_cat = d_h_pre_bn @ W1.T\n",
    "    dW1 = emb_cat.T @ d_h_pre_bn\n",
    "    db1 = d_h_pre_bn.sum(0)\n",
    "    d_emb = d_emb_cat.view(emb.shape)\n",
    "    permuted = F.one_hot(Xb, num_classes=27).permute(2, 0, 1)\n",
    "    dC = torch.tensordot(permuted.float(), d_emb, dims=2)\n",
    "\n",
    "    grads = [dC, dW1, db1, dW2, db2, d_bn_gain, d_bn_bias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01  # step learning rate decay\n",
    "    for pi, (p, grad) in enumerate(zip(parameters, grads)):\n",
    "        assert p.data.shape == grad.shape, (p.data.shape, grad.shape)\n",
    "        p.data += -lr * p.grad  # old way of cheems doge (using PyTorch grad from .backward())\n",
    "        # p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0:  # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    loss_i.append(loss.log10().item())\n",
    "\n",
    "    if i >= 2000:  # TODO: delete early breaking when you're ready to train the full net\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "id": "ZEpI0hMW8PPz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 10)        | exact: False | approx: True  | max diff: 1.210719347000122e-08 | max expected: -0.01540443953126669 | max calculated: -0.01540442742407322\n",
      "(30, 200)       | exact: False | approx: True  | max diff: 1.1175870895385742e-08 | max expected: -0.04835912585258484 | max calculated: -0.048359137028455734\n",
      "(200,)          | exact: False | approx: True  | max diff: 5.587935447692871e-09 | max expected: 4.6566128730773926e-09 | max calculated: -9.313225746154785e-10\n",
      "(200, 27)       | exact: False | approx: True  | max diff: 1.30385160446167e-08 | max expected: -0.01817333698272705 | max calculated: -0.018173350021243095\n",
      "(27,)           | exact: False | approx: True  | max diff: 7.450580596923828e-09 | max expected: 0.03223457559943199 | max calculated: 0.03223458305001259\n",
      "(1, 200)        | exact: False | approx: True  | max diff: 2.7939677238464355e-09 | max expected: -0.013373886235058308 | max calculated: -0.013373889029026031\n",
      "(1, 200)        | exact: False | approx: True  | max diff: 4.6566128730773926e-09 | max expected: -0.007833592593669891 | max calculated: -0.007833597250282764\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "for p,g in zip(parameters, grads):\n",
    "  cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "id": "KImLWNoh8PP0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    emb_cat = emb.view(emb.shape[0], -1)\n",
    "    h_pre_act = emb_cat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bn_mean = h_pre_act.mean(0, keepdim=True)\n",
    "    bn_var = h_pre_act.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "id": "6aFnP_Zc8PP0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.2159318923950195\n",
      "val 2.2280664443969727\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad()  # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]  # (N, block_size, n_embd)\n",
    "    emb_cat = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\n",
    "    h_pre_act = emb_cat @ W1 + b1\n",
    "    h_pre_act = bn_gain * (h_pre_act - bn_mean) * (bn_var + 1e-5) ** -0.5 + bn_bias\n",
    "    h = torch.tanh(h_pre_act)  # (N, n_hidden)\n",
    "    logits = h @ W2 + b2  # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esWqmhyj8PP1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "id": "xHeQNv3s8PP1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah.\n",
      "amille.\n",
      "khirmili.\n",
      "taty.\n",
      "hacassie.\n",
      "rahnen.\n",
      "den.\n",
      "rhc.\n",
      "laqhi.\n",
      "nellara.\n",
      "chaiivon.\n",
      "leigh.\n",
      "ham.\n",
      "pora.\n",
      "quinn.\n",
      "sulilea.\n",
      "jambiron.\n",
      "trogdiaryxian.\n",
      "cen.\n",
      "dus.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass\n",
    "        emb = C[torch.tensor([context])]  # (1,block_size,d)      \n",
    "        emb_cat = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\n",
    "        h_pre_act = emb_cat @ W1 + b1\n",
    "        h_pre_act = bn_gain * (h_pre_act - bn_mean) * (bn_var + 1e-5) ** -0.5 + bn_bias\n",
    "        h = torch.tanh(h_pre_act)  # (N, n_hidden)\n",
    "        logits = h @ W2 + b2  # (N, vocab_size)\n",
    "        # sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}