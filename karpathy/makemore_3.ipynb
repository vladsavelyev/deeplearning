{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0CHnbUIZUx6Ys7LsLVPrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladsavelyev/deeplearning/blob/master/karpathy/makemore_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "2bpdznEeBb7F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "data_path = Path('names.txt')\n",
        "if not data_path.exists():\n",
        "    r = requests.get('https://raw.githubusercontent.com/vladsavelyev/deeplearning/master/karpathy/names.txt')\n",
        "    with data_path.open('wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "with data_path.open() as f:\n",
        "    words = f.read().split()\n",
        "\n",
        "print(words[:10])\n",
        "print(f'{len(words)=}, {max(len(w) for w in words)=}, {min(len(w) for w in words)=}')\n",
        "\n",
        "vocab = ['.'] + sorted(list(set(''.join(words))))\n",
        "itos = {k: v for k, v in enumerate(vocab)}\n",
        "stoi = {v: k for k, v in itos.items()}\n",
        "print(len(vocab), vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyCom05GHH_4",
        "outputId": "5b01a63e-ed79-4dac-9d02-0b3353deebb1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
            "len(words)=32033, max(len(w) for w in words)=15, min(len(w) for w in words)=2\n",
            "27 ['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 3\n",
        "\n",
        "def build_dataset(words):\n",
        "    X, Y = [], []\n",
        "    for w in words:\n",
        "        context = [0] * block_size\n",
        "        for ch in w + '.':\n",
        "            X.append(context)\n",
        "            Y.append(stoi[ch])\n",
        "            context = context[1:] + [stoi[ch]]\n",
        "    return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(len(words) * 0.8)\n",
        "n2 = int(len(words) * 0.9)\n",
        "Xtr, Ytr = build_dataset(words[:n1])  # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])  # 10%\n",
        "Xtest, Ytest = build_dataset(words[n2:])  # 10%\n",
        "len(Xtr), len(Xdev), len(Xtest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbxVl4dFHLu9",
        "outputId": "1799cf97-b928-4579-ef1f-85ca973af999"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(182625, 22655, 22866)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP revisited\n",
        "n_embd = 10  # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(42)  # for reproducibility\n",
        "C  = torch.randn((len(vocab), n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
        "b1 = torch.randn(n_hidden,                        generator=g)\n",
        "W2 = torch.randn((n_hidden, len(vocab)),          generator=g)\n",
        "b2 = torch.randn(len(vocab),                      generator=g)\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "print('Model parameters number', sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "    p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p68UmRNITHf",
        "outputId": "42ed1761-b1e4-4e2b-bfa3-8b979b241971"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters number 11897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 200_000\n",
        "batch_size = 32\n",
        "learning_rate = 0.1\n",
        "\n",
        "lossi = []\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_loss(split):\n",
        "    x, y = {\n",
        "        'train': (Xtr, Ytr),\n",
        "        'dev': (Xdev, Ydev),\n",
        "        'test': (Xtest, Ytest),\n",
        "    }[split]\n",
        "    emb = C[x].view(-1, block_size * n_embd)\n",
        "    h = torch.tanh(emb @ W1 + b1)\n",
        "    logits = h @ W2 + b2\n",
        "    return F.cross_entropy(logits, y)\n",
        "    \n",
        "for i in range(max_steps):\n",
        "    lr = learning_rate\n",
        "    if i > max_steps / 2:\n",
        "        lr = learning_rate / 10\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(low=0, high=Xtr.shape[0], size=(batch_size,), generator=g)\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xtr[ix]].view(-1, block_size * n_embd)  # embed chars into vectors\n",
        "    h = torch.tanh(emb @ W1 + b1)  # hidden layer actications\n",
        "    logits = h @ W2 + b2  # output layer\n",
        "    loss = F.cross_entropy(logits, Ytr[ix])  # loss function\n",
        "    # equivalent to (assuming probs = F.softmax(logits, dim=1)):\n",
        "    # loss = = F.nll_loss(probs.log(), Ytr[ix])\n",
        "    # loss = -probs[torch.arange(start=0, end=len(ix)), Ytr[ix]].log().mean()\n",
        "\n",
        "    # track stats for 1000 steps\n",
        "    if i % (max_steps / 1000) == 0:\n",
        "        lossi.append(calc_loss('dev'))\n",
        "        # print 100 steps\n",
        "        if i % (max_steps / 100) == 0:\n",
        "            print(f'{i=}, {lr=}, loss_dev={lossi[-1]}')\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # paramter update\n",
        "    for p in parameters:\n",
        "        p.data -= learning_rate * p.grad\n",
        "\n",
        "print(f'Final dev loss: {calc_loss(\"dev\")}')\n",
        "print(f'Final test loss: {calc_loss(\"test\")}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rzaDskxJ8NS",
        "outputId": "fc03bfe1-0e30-4b95-cb8e-326a183d602d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i=0, lr=0.1, loss_dev=26.972827911376953\n",
            "i=2000, lr=0.1, loss_dev=3.054460287094116\n",
            "i=4000, lr=0.1, loss_dev=2.7417726516723633\n",
            "i=6000, lr=0.1, loss_dev=2.6612091064453125\n",
            "i=8000, lr=0.1, loss_dev=2.600219249725342\n",
            "i=10000, lr=0.1, loss_dev=2.6144962310791016\n",
            "i=12000, lr=0.1, loss_dev=2.52915096282959\n",
            "i=14000, lr=0.1, loss_dev=2.4970622062683105\n",
            "i=16000, lr=0.1, loss_dev=2.478848934173584\n",
            "i=18000, lr=0.1, loss_dev=2.450507879257202\n",
            "i=20000, lr=0.1, loss_dev=2.438777208328247\n",
            "i=22000, lr=0.1, loss_dev=2.473804235458374\n",
            "i=24000, lr=0.1, loss_dev=2.412447929382324\n",
            "i=26000, lr=0.1, loss_dev=2.4093754291534424\n",
            "i=28000, lr=0.1, loss_dev=2.3998866081237793\n",
            "i=30000, lr=0.1, loss_dev=2.3889992237091064\n",
            "i=32000, lr=0.1, loss_dev=2.411163091659546\n",
            "i=34000, lr=0.1, loss_dev=2.424211263656616\n",
            "i=36000, lr=0.1, loss_dev=2.3934221267700195\n",
            "i=38000, lr=0.1, loss_dev=2.3628344535827637\n",
            "i=40000, lr=0.1, loss_dev=2.3427627086639404\n",
            "i=42000, lr=0.1, loss_dev=2.4599416255950928\n",
            "i=44000, lr=0.1, loss_dev=2.3188185691833496\n",
            "i=46000, lr=0.1, loss_dev=2.34159255027771\n",
            "i=48000, lr=0.1, loss_dev=2.3598718643188477\n",
            "i=50000, lr=0.1, loss_dev=2.3297948837280273\n",
            "i=52000, lr=0.1, loss_dev=2.3685519695281982\n",
            "i=54000, lr=0.1, loss_dev=2.3136932849884033\n",
            "i=56000, lr=0.1, loss_dev=2.345543384552002\n",
            "i=58000, lr=0.1, loss_dev=2.324343681335449\n",
            "i=60000, lr=0.1, loss_dev=2.310079336166382\n",
            "i=62000, lr=0.1, loss_dev=2.3204193115234375\n",
            "i=64000, lr=0.1, loss_dev=2.3708081245422363\n",
            "i=66000, lr=0.1, loss_dev=2.3302366733551025\n",
            "i=68000, lr=0.1, loss_dev=2.310802698135376\n",
            "i=70000, lr=0.1, loss_dev=2.348017930984497\n",
            "i=72000, lr=0.1, loss_dev=2.271306037902832\n",
            "i=74000, lr=0.1, loss_dev=2.352193832397461\n",
            "i=76000, lr=0.1, loss_dev=2.3577775955200195\n",
            "i=78000, lr=0.1, loss_dev=2.3145382404327393\n",
            "i=80000, lr=0.1, loss_dev=2.2842013835906982\n",
            "i=82000, lr=0.1, loss_dev=2.3381948471069336\n",
            "i=84000, lr=0.1, loss_dev=2.3166134357452393\n",
            "i=86000, lr=0.1, loss_dev=2.298243761062622\n",
            "i=88000, lr=0.1, loss_dev=2.281773567199707\n",
            "i=90000, lr=0.1, loss_dev=2.286696195602417\n",
            "i=92000, lr=0.1, loss_dev=2.2560672760009766\n",
            "i=94000, lr=0.1, loss_dev=2.305938720703125\n",
            "i=96000, lr=0.1, loss_dev=2.385359048843384\n",
            "i=98000, lr=0.1, loss_dev=2.269329786300659\n",
            "i=100000, lr=0.1, loss_dev=2.405925750732422\n",
            "i=102000, lr=0.01, loss_dev=2.2873001098632812\n",
            "i=104000, lr=0.01, loss_dev=2.288224935531616\n",
            "i=106000, lr=0.01, loss_dev=2.271873712539673\n",
            "i=108000, lr=0.01, loss_dev=2.292370557785034\n",
            "i=110000, lr=0.01, loss_dev=2.3408408164978027\n",
            "i=112000, lr=0.01, loss_dev=2.2671396732330322\n",
            "i=114000, lr=0.01, loss_dev=2.3125576972961426\n",
            "i=116000, lr=0.01, loss_dev=2.2929248809814453\n",
            "i=118000, lr=0.01, loss_dev=2.289686441421509\n",
            "i=120000, lr=0.01, loss_dev=2.304396390914917\n",
            "i=122000, lr=0.01, loss_dev=2.303304433822632\n",
            "i=124000, lr=0.01, loss_dev=2.2788777351379395\n",
            "i=126000, lr=0.01, loss_dev=2.315519332885742\n",
            "i=128000, lr=0.01, loss_dev=2.34179949760437\n",
            "i=130000, lr=0.01, loss_dev=2.274909496307373\n",
            "i=132000, lr=0.01, loss_dev=2.2716517448425293\n",
            "i=134000, lr=0.01, loss_dev=2.261660575866699\n",
            "i=136000, lr=0.01, loss_dev=2.248049736022949\n",
            "i=138000, lr=0.01, loss_dev=2.268817186355591\n",
            "i=140000, lr=0.01, loss_dev=2.2767367362976074\n",
            "i=142000, lr=0.01, loss_dev=2.269176721572876\n",
            "i=144000, lr=0.01, loss_dev=2.313652276992798\n",
            "i=146000, lr=0.01, loss_dev=2.265836715698242\n",
            "i=148000, lr=0.01, loss_dev=2.2507944107055664\n",
            "i=150000, lr=0.01, loss_dev=2.258333444595337\n",
            "i=152000, lr=0.01, loss_dev=2.276089906692505\n",
            "i=154000, lr=0.01, loss_dev=2.2822630405426025\n",
            "i=156000, lr=0.01, loss_dev=2.258357286453247\n",
            "i=158000, lr=0.01, loss_dev=2.2580819129943848\n",
            "i=160000, lr=0.01, loss_dev=2.2433114051818848\n",
            "i=162000, lr=0.01, loss_dev=2.308227777481079\n",
            "i=164000, lr=0.01, loss_dev=2.251718282699585\n",
            "i=166000, lr=0.01, loss_dev=2.338404417037964\n",
            "i=168000, lr=0.01, loss_dev=2.2329933643341064\n",
            "i=170000, lr=0.01, loss_dev=2.2514939308166504\n",
            "i=172000, lr=0.01, loss_dev=2.314729690551758\n",
            "i=174000, lr=0.01, loss_dev=2.260150194168091\n",
            "i=176000, lr=0.01, loss_dev=2.279132127761841\n",
            "i=178000, lr=0.01, loss_dev=2.264418601989746\n",
            "i=180000, lr=0.01, loss_dev=2.2876176834106445\n",
            "i=182000, lr=0.01, loss_dev=2.2498233318328857\n",
            "i=184000, lr=0.01, loss_dev=2.263201951980591\n",
            "i=186000, lr=0.01, loss_dev=2.2634642124176025\n",
            "i=188000, lr=0.01, loss_dev=2.3266003131866455\n",
            "i=190000, lr=0.01, loss_dev=2.3036439418792725\n",
            "i=192000, lr=0.01, loss_dev=2.2656290531158447\n",
            "i=194000, lr=0.01, loss_dev=2.2685182094573975\n",
            "i=196000, lr=0.01, loss_dev=2.276804208755493\n",
            "i=198000, lr=0.01, loss_dev=2.2798585891723633\n",
            "Final dev loss: 2.2732484340667725\n",
            "Final test loss: 2.268199920654297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([i for i in range(len(lossi))], lossi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "jLgPW_LaVhYx",
        "outputId": "d3b2ff5f-cc71-4bca-f70e-f2e051f96260"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5a9e383b80>]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = 15\n",
        "g = torch.Generator().manual_seed(42)\n",
        "max_word_len = 20\n",
        "for _ in range(n_words):\n",
        "    context = [0] * block_size\n",
        "    losses = []\n",
        "    for _ in range(max_word_len):\n",
        "        x = torch.tensor(context[-block_size:]).view(1, block_size)\n",
        "        emb = C[x].view(-1, n_embd * block_size)\n",
        "        h = torch.tanh(emb @ W1 + b1)\n",
        "        logits = h @ W2 + b2\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        next_char = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
        "        context.append(next_char)\n",
        "        losses.append(-probs[0, next_char].log().mean().item())\n",
        "        if next_char == 0:\n",
        "            break\n",
        "    gen_chars = context[3:]\n",
        "    gen_word = ''.join(map(itos.get, gen_chars))\n",
        "    print(f'{gen_word:<{max_word_len}} {sum(losses) / len(losses):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JToCqj4xYl5I",
        "outputId": "4f44de79-6a92-4094-9abb-54ee4d12ba58"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abdanevoan.          2.45\n",
            "journ.               1.77\n",
            "mik.                 1.94\n",
            "mar.                 1.68\n",
            "aya.                 1.79\n",
            "alee.                1.58\n",
            "liu.                 3.30\n",
            "hay.                 1.98\n",
            "jayse.               1.51\n",
            "mira.                1.24\n",
            "khyah.               1.80\n",
            "davereelyn.          1.91\n",
            "julai.               2.31\n",
            "jeharne.             1.94\n",
            "sydn.                2.27\n"
          ]
        }
      ]
    }
  ]
}