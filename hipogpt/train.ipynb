{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_path = Path(\"/Users/vlad/googledrive/AI/datasets/murakami/murakami.txt\")\n",
    "if not input_path.exists():\n",
    "    input_path = Path(\"/content/drive/MyDrive/AI/datasets/murakami/murakami.txt\")\n",
    "    assert input_path.exists(), input_path\n",
    "    \n",
    "with input_path.open(encoding='utf-8') as f:\n",
    "    input_text = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10155065\n"
     ]
    }
   ],
   "source": [
    "print(len(input_text))\n",
    "train_text = input_text[:10_000]\n",
    "test_text = input_text[:152]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-trained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 22:08:14.849157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Охота на овец Часть первая 25.11.1970 ПИКНИК СРЕДИ НЕДЕЛИ О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bt = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(bt.backend_tokenizer.normalizer.normalize_str(test_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CharBPETokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['О', 'хот', 'а</w>', 'на</w>', 'о', 'ве', 'ц</w>', 'Ч', 'а', 'сть</w>', 'пер', 'вая</w>', '25</w>', '.</w>', '1', '1</w>', '.</w>', '19', '70</w>', 'П', 'И', 'К', 'Н', 'И', 'К</w>', 'С', 'Р', 'ЕД', 'И</w>', 'Н', 'ЕД', 'Е', 'Л', 'И</w>', 'О</w>', 'ее</w>', 'смерти</w>', 'со', 'общи', 'л</w>', 'мне</w>', 'по</w>', 'телеф', 'о', 'ну</w>', 'старый</w>', 'приятел', 'ь</w>', ',</w>', 'на', 'т', 'к', 'нувшись</w>', 'на</w>', 'случай', 'ные</w>', 'стро', 'ч', 'ки</w>', 'в</w>', 'газет', 'е</w>']\n",
      "Охота на овец Часть первая 25 . 11 . 1970 ПИКНИК СРЕДИ НЕДЕЛИ О ее смерти сообщил мне по телефону старый приятель , наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import CharBPETokenizer\n",
    "char_bpe = CharBPETokenizer()\n",
    "char_bpe.train_from_iterator(iter([train_text]))\n",
    "print(char_bpe.encode(test_text).tokens)\n",
    "print(char_bpe.decode(char_bpe.encode(test_text).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ByteLevelBPETokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['Ðŀ', 'ÑħÐ¾ÑĤ', 'Ð°', 'ĠÐ½Ð°', 'ĠÐ¾', 'Ð²', 'ÐµÑĨ', 'Ċ', 'Ð', '§', 'Ð°ÑģÑĤÑĮ', 'ĠÐ¿ÐµÑĢ', 'Ð²Ð°Ñı', 'Ċ', '25', '.', '1', '1', '.', '19', '70', 'Ċ', 'ÐŁ', 'ÐĺÐļ', 'ÐĿ', 'ÐĺÐļ', 'ĠÐ¡', 'Ðł', 'ÐķÐĶ', 'Ðĺ', 'ĠÐĿ', 'ÐķÐĶ', 'Ðķ', 'Ð', 'Ľ', 'Ðĺ', 'Ċ', 'Ðŀ', 'ĠÐµÐµ', 'ĠÑģÐ¼ÐµÑĢÑĤÐ¸', 'ĠÑģÐ¾Ð¾Ð±Ñī', 'Ð¸Ð»', 'ĠÐ¼Ð½Ðµ', 'ĠÐ¿Ð¾', 'ĠÑĤÐµÐ»ÐµÑĦÐ¾', 'Ð½Ñĥ', 'ĠÑģÑĤÐ°ÑĢÑĭÐ¹', 'ĠÐ¿ÑĢÐ¸Ñı', 'ÑĤ', 'ÐµÐ»ÑĮ', ',', 'ĠÐ½Ð°', 'ÑĤ', 'Ðº', 'Ð½ÑĥÐ²ÑĪÐ¸ÑģÑĮ', 'ĠÐ½Ð°', 'ĠÑģÐ»ÑĥÑĩÐ°Ð¹', 'Ð½ÑĭÐµ', 'ĠÑģÑĤÑĢÐ¾', 'Ñĩ', 'ÐºÐ¸', 'ĠÐ²', 'ĠÐ³Ð°Ð·ÐµÑĤ', 'Ðµ']\n",
      "Охота на овец\n",
      "Часть первая\n",
      "25.11.1970\n",
      "ПИКНИК СРЕДИ НЕДЕЛИ\n",
      "О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "bpe = ByteLevelBPETokenizer()\n",
    "bpe.train_from_iterator(iter([train_text]))\n",
    "print(bpe.encode(test_text).tokens)\n",
    "print(bpe.decode(bpe.encode(test_text).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SentencePieceBPETokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['▁О', 'хо', 'та', '▁на', '▁о', 'ве', 'ц', '\\n', 'Ч', 'а', 'сть', '▁пер', 'вая', '\\n', '25', '.1', '1', '.1', '9', '70', '\\n', 'П', 'ИК', 'Н', 'ИК', '▁С', 'Р', 'ЕД', 'И', '▁Н', 'ЕД', 'Е', 'Л', 'И', '\\n', 'О', '▁ее', '▁смер', 'ти', '▁сооб', 'щ', 'ил', '▁мне', '▁по', '▁телефо', 'ну', '▁старый', '▁прия', 'т', 'ель', ',', '▁на', 'т', 'к', 'нувши', 'сь', '▁на', '▁случай', 'ные', '▁стро', 'ч', 'ки', '▁в', '▁газет', 'е']\n",
      "Охота на овец\n",
      "Часть первая\n",
      "25.11.1970\n",
      "ПИКНИК СРЕДИ НЕДЕЛИ\n",
      "О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import SentencePieceBPETokenizer\n",
    "spm_bpe = SentencePieceBPETokenizer()\n",
    "spm_bpe.train_from_iterator(iter([train_text]))\n",
    "print(spm_bpe.encode(test_text).tokens)\n",
    "print(spm_bpe.decode(spm_bpe.encode(test_text).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordPiece"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{'##шенно': 672, ',': 7, '.': 9, '##город': 714, 'страниц': 787, '##лы': 386, '##ил': 136, '##вори': 746, '##гой': 509, 'Ни': 248, '##ых': 492, 'Не': 276, 'уз': 317, '##уж': 323, '##ные': 410, 'тебе': 625, 'та': 621, 'Ну': 306, '##ав': 283, '##во': 173, '##ла': 186, 'о': 59, '##чно': 229, 'кого': 719, '##еж': 461, 'туда': 801, 'заду': 722, 'Она': 425, '##чай': 491, 'сидел': 782, '##нечно': 521, 'у': 64, '##ницы': 809, '##ит': 158, 'был': 274, '##ное': 494, '##Л': 111, 'пол': 340, '##ства': 699, 'ц': 67, '##нд': 634, '##алось': 532, '##в': 88, '##г': 104, '##ой': 266, 'если': 435, 'вдруг': 825, 'эти': 459, 'пикни': 447, '##жды': 791, '##ме': 264, '##ай': 254, '##ки': 157, 'ней': 268, 'неб': 701, '(': 5, '##вет': 331, 'см': 374, 'смер': 552, 'подыма': 786, 'листья': 756, 'двадцать': 827, 'пер': 371, 'нако': 503, 'поч': 498, 'е': 50, '##жет': 487, 'совершенно': 868, '##ностей': 860, 'дру': 433, 'газет': 558, 'без': 583, 'этого': 779, 'и': 53, '##ак': 141, '##р': 78, '##зни': 662, 'дома': 545, '##ду': 217, 'Сп': 575, 'самой': 790, 'лишь': 754, 'соб': 529, 'похоро': 423, '##кого': 709, 'руки': 818, 'время': 421, '##разу': 717, '##ный': 246, 'чер': 381, '##еня': 245, 'до': 191, '«': 23, '##до': 335, 'мол': 608, '##том': 504, 'оч': 370, '##ствитель': 807, 'хо': 380, '##де': 201, '##жется': 680, 'ей': 310, '##ции': 484, 'ее': 205, 'н': 58, 'ж': 51, '##дел': 240, 'кофей': 536, '##ть': 146, '##годня': 839, 'вовсе': 751, '##вшись': 422, '##стро': 698, '##вно': 330, '##ами': 767, 'вся': 586, '##ена': 506, 'зву': 601, 'F': 21, '##ыма': 533, '##ры': 460, 'меня': 250, '##ала': 165, '##ор': 476, '##ная': 407, '##ер': 135, 'слу': 376, '##жал': 677, 'нее': 294, '##за': 478, '##ло': 152, ':': 17, '##дин': 419, 'ч': 68, '##д': 98, 'од': 225, 'словно': 799, 'он': 443, 'прос': 747, '##о': 91, 'сам': 282, 'зак': 438, 'кроме': 550, '##ан': 319, '##ая': 320, 'чу': 629, 'д': 49, 'людей': 842, 'сб': 617, '##вая': 416, 'Так': 426, 'люди': 778, 'хорош': 820, 'се': 450, 'Но': 572, 'спросил': 547, '##ать': 192, '##ят': 391, 'жа': 597, '##нувшись': 524, 'времена': 789, 'си': 252, '##ную': 303, 'Б': 26, '##ях': 656, '##ств': 342, 'узнать': 803, 'сом': 618, '##еть': 505, 'после': 554, 'адрес': 849, '##дне': 674, 'впер': 589, 'странное': 812, 'кра': 603, 'люд': 537, 'позво': 697, 'когда': 300, 'недели': 838, 'карту': 551, 'сооб': 766, '##к': 85, '##нь': 462, 'при': 170, 'к': 55, '##здух': 424, 'врем': 277, '##ле': 187, 'электри': 858, '##ед': 171, '##Р': 120, '##ваюсь': 738, 'знаю': 795, '##шед': 671, '##чи': 242, 'Д': 29, '##7': 114, '##ол': 180, '##итель': 522, 'заб': 409, '##ходя': 788, '##ку': 167, 'п': 60, '[CLS]': 2, '##ка': 166, '##али': 189, '##дач': 724, 'вд': 429, 'с': 62, '##ля': 184, 'б': 46, 'но': 224, 'кни': 278, '##ч': 102, '##ход': 275, 'пар': 446, '##й': 95, '##вый': 652, '##нным': 635, '##чек': 420, '##ща': 685, 'ответ': 417, 'брос': 428, '##ту': 288, '##ния': 269, 'похо': 293, 'им': 439, 'В': 27, 'кост': 718, '##пл': 637, 'обнаружил': 867, 'всп': 588, '##зу': 479, 'развернул': 866, '##надцать': 713, 'спала': 770, 'сту': 516, 'пло': 613, 'дожд': 761, '##ИК': 688, '##емь': 301, 'со': 197, 'А': 25, 'станции': 566, 'мог': 771, '##вер': 200, 'кри': 604, 'прошло': 749, '##тра': 480, '##круг': 644, '##ву': 472, 'вед': 587, '##вес': 650, '##вор': 745, 'наконец': 564, '##ня': 227, 'пригород': 743, '##ежала': 835, '##я': 89, 'еле': 596, 'сто': 253, '##кра': 468, 'была': 302, '##И': 112, 'С': 40, 'Мо': 364, 'зна': 312, '##Е': 109, 'шест': 631, '##нил': 343, 'случай': 553, 'нуж': 442, 'Как': 363, 'помню': 696, 'По': 307, 'вспом': 850, '##ое': 659, 'можно': 772, 'действитель': 851, '##им': 285, 'ь': 72, '##аз': 153, '##лько': 327, 'дел': 432, '##ками': 737, '##енчес': 708, '##ился': 508, 'мира': 830, 'вы': 222, 'журн': 852, 'жила': 437, 'гру': 590, '6': 14, '##5': 117, '##три': 668, '##ц': 97, 'Дни': 570, 'ск': 281, 'пост': 341, '##лю': 645, '##ому': 732, 'стан': 517, 'мал': 607, '[MASK]': 4, '##ке': 239, 'тро': 622, '##кни': 385, 'для': 593, 'на': 130, '##ного': 339, '##ком': 297, '##овал': 784, 'пришло': 742, '##ред': 382, 'пили': 614, '##ност': 689, '##вала': 263, 'мо': 214, 'сейчас': 562, '##юсь': 466, '##ника': 501, 'книги': 548, 'за': 147, '0': 10, '##з': 92, '##род': 400, 'Чуть': 578, '##ин': 329, '##нивер': 345, 'печали': 797, 'университет': 379, 'тел': 454, '##жил': 678, 'жи': 436, '##раться': 565, 'ли': 182, '##пать': 641, '##ца': 482, '##год': 510, '##ался': 397, '##шку': 670, 'И': 32, '##жно': 292, '##кой': 232, 'точно': 715, '##сти': 499, 'нибуд': 525, '##сто': 221, '##ню': 257, '##ам': 198, '##ряже': 781, '##ар': 160, 'Конечно': 814, '##ую': 258, 'лета': 816, 'Во': 361, 'развер': 752, '##х': 105, '##нец': 520, 'зап': 408, 'корки': 840, '##сли': 326, 'ва': 584, 'просто': 748, '##ен': 133, '##стья': 530, '##ый': 210, 'похороны': 556, '##жде': 679, 'она': 178, 'друг': 595, 'особ': 541, 'дей': 592, '##ность': 495, 'г': 48, '##ивала': 471, 'То': 249, 'Может': 815, '##ет': 132, '##ило': 299, '19': 568, 'й': 54, '##не': 155, '##хо': 181, '##ф': 94, 'л': 56, '##1': 115, '##ЕД': 687, '##пер': 384, 'то': 139, 'семь': 377, '7': 15, '##ным': 273, 'раз': 179, '##жд': 291, '##9': 113, '##ест': 237, '##ько': 241, '##ся': 148, 'ю': 74, 'позвонил': 861, 'себе': 619, 'напро': 793, 'где': 431, '##це': 483, '##рош': 399, 'котор': 347, 'бол': 308, 'стро': 726, 'каз': 606, '##ела': 514, '##щем': 686, 'все': 203, '##бит': 485, 'сей': 451, '##буд': 486, 'закон': 723, '##нару': 712, 'низ': 733, 'прошлого': 865, '##еф': 633, 'Ко': 362, '##ти': 176, '##рова': 691, 'ответил': 824, 'го': 190, 'какие': 526, 'ча': 457, '##нал': 463, 'словечка': 800, 'два': 434, '##кла': 469, 'ост': 611, 'стран': 355, 'от': 169, '##е': 79, '##чка': 684, '##ни': 129, 'было': 411, 'бра': 581, 'исп': 602, '##чего': 352, '##еб': 256, '##на': 137, 'книг': 356, '##чного': 777, 'Ми': 571, '##ев': 322, 'оно': 610, '##да': 149, '##кий': 349, '##ве': 262, 'К': 33, '##елей': 720, 'М': 35, 'бы': 154, 'лица': 755, '##малась': 658, 'нап': 295, '##кат': 735, '##нить': 703, 'мы': 279, '##шло': 394, 'да': 204, 'У': 42, '##ряд': 780, '##ем': 145, '##ча': 488, '##п': 83, '##ице': 649, '##пасть': 805, 'хро': 628, '##жи': 336, 'Т': 41, 'три': 455, '##ре': 321, '##нув': 523, 'только': 511, 'очень': 817, '##Д': 110, '##риков': 721, 'тра': 316, '##н': 80, 'щ': 70, '##с': 84, 'критер': 853, '##па': 324, '[PAD]': 0, 'те': 453, 'другой': 559, 'неско': 555, '##ы': 103, 'х': 66, '##ебе': 354, '##вился': 654, 'пода': 694, 'прият': 741, 'мне': 206, 'ко': 142, '##тила': 669, 'прежде': 855, '##икни': 388, '##но': 123, '##нием': 702, '##чали': 489, 'E': 20, '##чил': 683, '##рая': 716, '##дцать': 290, '##ких': 412, 'газ': 430, '##аль': 496, 'из': 223, 'ни': 162, 'напряже': 794, '##илось': 543, '##нать': 711, '##пом': 640, 'об': 195, '5': 13, '##ичес': 648, '##яз': 655, ')': 6, '##тся': 481, '##ру': 163, 'уб': 626, ';': 18, 'телефо': 871, '##олн': 753, '##E': 118, '##ок': 477, 'кар': 368, '##че': 209, 'студенчес': 863, '##мню': 657, '##ва': 168, '##ас': 215, 'мест': 441, 'ст': 150, 'знал': 600, '##кур': 415, 'были': 519, 'Пер': 574, '##т': 93, '1': 11, 'быть': 730, 'пре': 615, '##то': 131, '##К': 121, 'звали': 599, '##а': 77, '##лек': 418, 'ту': 315, 'один': 444, 'смерти': 845, '##енно': 231, 'Только': 557, '##рее': 804, '##кон': 507, 'Л': 34, '##ю': 82, '##ила': 194, 'ста': 725, 'двер': 594, 'м': 57, '##ших': 762, '##ны': 151, '##влек': 653, 'окур': 612, '##рос': 230, 'так': 226, '?': 19, 'парши': 831, '##топ': 705, 'уже': 378, 'осень': 540, 'пепельницы': 870, 'люб': 538, '##зду': 392, '##ель': 271, 'ру': 373, '##и': 87, '##пы': 639, 'наш': 704, 'О': 37, 'именно': 829, '##яд': 475, '##лу': 261, '##лыш': 822, 'под': 267, 'через': 821, 'сигарету': 813, '##кость': 710, 'нею': 700, 'студ': 727, '##вечка': 783, 'пе': 313, '##ур': 284, '##ных': 518, 'вокруг': 750, '##алась': 350, 'там': 624, '##ш': 96, 'Г': 28, '##ало': 212, 'совер': 765, '##вые': 474, '##ется': 270, 'больше': 844, 'па': 445, '##дит': 675, '##тя': 665, 'Ч': 43, '##лиц': 744, 'сп': 207, 'стар': 348, '##ниц': 344, '##той': 406, '##ии': 387, '##нился': 808, '##щ': 108, '[SEP]': 3, '##ые': 243, 'несколько': 846, 'ф': 65, '##ний': 405, '##те': 664, '##пис': 836, '–': 76, '##вым': 473, '##ках': 736, '»': 24, '##лено': 757, 'авто': 848, '##нул': 413, '##ма': 333, '##гда': 220, '##тер': 667, 'ви': 365, 'N': 22, 'жизни': 828, 'нужен': 560, '##обраться': 776, 'ос': 280, 'лет': 369, '##ов': 265, 'поя': 692, '##рать': 512, 'сигарет': 359, 'ш': 69, '##ом': 159, '##ывала': 493, '##ром': 398, '##ном': 338, '##ски': 643, '##лось': 729, 'по': 126, 'коф': 233, '##ел': 143, 'года': 528, 'сюда': 620, '##ус': 636, 'причи': 739, 'казалось': 854, '##ков': 346, '##шее': 773, '##6': 116, 'нес': 403, 'дер': 591, '##ль': 646, '##ри': 144, '##у': 81, 'боль': 544, 'во': 177, 'Р': 39, 'р': 61, 'Семья': 847, '##уд': 259, '##деся': 768, '##ди': 188, '##тор': 296, 'старый': 810, '##ши': 193, '##пель': 642, 'усп': 627, 'Е': 30, 'знаком': 796, 'даже': 769, 'ка': 213, 'рав': 616, '##сть': 202, 'лю': 234, 'наз': 502, '##ей': 164, '##стоя': 534, '##ной': 211, '##ал': 124, 'са': 449, 'семья': 819, 'сред': 452, 'который': 546, 'ад': 580, '##ь': 101, '##ще': 244, 'жур': 598, 'ве': 585, 'З': 31, 'Про': 573, 'сон': 764, '##ью': 681, '25': 569, 'пос': 401, '##го': 138, '##ень': 196, 'пого': 693, 'поня': 695, '-': 8, '##уть': 260, 'сво': 375, '##сит': 325, 'тер': 623, 'подходил': 869, 'бли': 582, 'хар': 456, '##дим': 758, '##ше': 216, '##ели': 515, '##рес': 383, 'тридцать': 834, 'воздух': 527, '9': 16, '##или': 298, 'дом': 309, 'же': 311, '##0': 107, '##нем': 464, 'ничего': 414, 'в': 47, 'подход': 785, '##э': 106, 'это': 235, '##та': 393, 'спросила': 811, '##шь': 334, 'а': 45, '##ждый': 792, 'пепель': 798, 'траве': 802, '##ми': 287, 'про': 175, 'как': 174, 'оп': 609, '##же': 218, '##N': 119, 'кор': 513, '##тно': 666, '[UNK]': 1, 'недел': 500, 'элек': 632, '##ви': 389, 'Сто': 576, '##гар': 337, 'кем': 605, 'На': 305, 'Я': 44, '##гля': 396, '##вляю': 823, '##ие': 328, 'закончи': 862, '##ым': 219, '##денький': 859, '##л': 86, '##ого': 661, 'ро': 251, '##час': 490, '##ному': 806, 'ав': 579, '2': 12, '##рую': 734, '##дя': 673, 'з': 52, 'кро': 367, 'одну': 535, '70': 360, 'телеф': 833, '##зво': 663, '##все': 651, 'Н': 36, '##лед': 647, '##их': 470, 'еще': 366, '##сты': 402, 'ре': 372, '##енную': 707, '##ез': 199, 'сомне': 856, '##ситет': 357, 'т': 63, '##енный': 706, '##ким': 731, 'нибудь': 567, 'чь': 630, '##рог': 690, '##лож': 728, 'доб': 760, 'дело': 826, '##вля': 390, '##ро': 125, '##по': 638, '##ра': 140, 'что': 208, '##сь': 156, 'я': 75, 'стра': 272, '##се': 185, '##ж': 100, '##ко': 134, '##ли': 172, '##б': 99, 'не': 128, '##об': 228, 'рок': 539, 'обнару': 763, 'приход': 740, '##ст': 127, '##ниверситет': 358, 'кофе': 304, 'кофейню': 841, 'давно': 531, 'среди': 832, 'чув': 857, 'ноч': 775, '##вши': 286, 'П': 38, 'хард': 563, 'покло': 837, '##он': 660, '##шего': 774, '##чес': 395, '##м': 90, '##сп': 238, '##день': 676, 'говори': 759, '##чет': 682, 'чуть': 458, 'сказ': 542, 'пришлось': 864, 'спрос': 351, '##вали': 332, 'Семь': 577, '##руг': 247, 'чем': 318, 'ми': 440, '##ес': 183, '##Н': 122, 'пикнике': 561, '##цать': 289, '##рем': 255, 'рас': 448, 'пок': 497, 'ы': 71, 'Тол': 427, '##пи': 467, 'э': 73, '##ну': 161, 'сло': 314, 'осенью': 843, 'Вот': 549, '##ние': 404, 'сигар': 353, '##юда': 465, '##ря': 236}\n",
      "['О', '##хо', '##та', 'на', 'о', '##ве', '##ц', 'Ч', '##ас', '##ть', 'пер', '##вая', '25', '.', '1', '##1', '.', '19', '##7', '##0', 'П', '##ИК', '##Н', '##ИК', 'С', '##Р', '##ЕД', '##И', 'Н', '##ЕД', '##Е', '##Л', '##И', 'О', 'ее', 'смерти', 'сооб', '##щ', '##ил', 'мне', 'по', 'телефо', '##ну', 'старый', 'прият', '##ель', ',', 'на', '##т', '##к', '##нувшись', 'на', 'случай', '##ные', 'стро', '##ч', '##ки', 'в', 'газет', '##е']\n",
      "Охота на овец Часть первая 25. 11. 1970 ПИКНИК СРЕДИ НЕДЕЛИ О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import BertWordPieceTokenizer\n",
    "wp = BertWordPieceTokenizer(clean_text=False, lowercase=False)\n",
    "wp.train_from_iterator(iter([train_text]))\n",
    "print(wp.get_vocab())\n",
    "print(wp.encode(test_text).tokens)\n",
    "print(wp.decode(wp.encode(test_text).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SentencePieceUnigramTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['▁О', 'хот', 'а', '▁на', '▁', 'ов', 'ец', '▁Ч', 'асть', '▁пер', 'ва', 'я', '▁25', '.1', '1', '.', '19', '70', '▁П', 'ИК', 'Н', 'ИК', '▁С', 'Р', 'ЕД', 'И', '▁Н', 'ЕД', 'Е', 'Л', 'И', '▁О', '▁ее', '▁смерти', '▁сообщ', 'ил', '▁мне', '▁по', '▁телефон', 'у', '▁стары', 'й', '▁приятел', 'ь', ',', '▁на', 'тк', 'нувшись', '▁на', '▁случайн', 'ые', '▁стро', 'ч', 'ки', '▁в', '▁газет', 'е']\n",
      "Охота на овец Часть первая 25.11.1970 ПИКНИК СРЕДИ НЕДЕЛИ О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import SentencePieceUnigramTokenizer\n",
    "smp_ug = SentencePieceUnigramTokenizer()\n",
    "smp_ug.train_from_iterator(iter([train_text]))\n",
    "print(smp_ug.encode(input_text[:152]).tokens)\n",
    "print(smp_ug.decode(smp_ug.encode(input_text[:152]).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Murakami tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "input_path = Path(\"/Users/vlad/MyDrive/AI/datasets/murakami/murakami.txt\")\n",
    "assert input_path.exists(), input_path\n",
    "\n",
    "input_path = input_path.with_name(\"murakami-1000lines.txt\")\n",
    "\n",
    "\n",
    "class Config:\n",
    "    sample_only: str = True\n",
    "    seed = 0\n",
    "\n",
    "    # Dataset\n",
    "    input_path = input_path\n",
    "    vocab_size: int = 30_000\n",
    "    context_len: int = 32\n",
    "\n",
    "    # Network\n",
    "    emb_dim: int = 32\n",
    "    n_blocks: int = 2\n",
    "    n_heads: int = 4\n",
    "\n",
    "    # Optimization\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 5e-4\n",
    "    weight_decay: float = 0.01\n",
    "    num_workers: int = 1\n",
    "    max_steps: int = 100_000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "bpe = ByteLevelBPETokenizer()\n",
    "bpe.train(files=str(Config.input_path))\n",
    "bpe.save_model(str(Config.input_path.parent), \"bpe\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(Config.seed)\n",
    "torch.cuda.manual_seed_all(Config.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'TransformerDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 31927) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    112\u001B[0m timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:424\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 424\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:931\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 931\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 416\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 31927) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[116], line 40\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtokenizers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecoders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ByteLevel\n\u001B[1;32m     31\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[1;32m     32\u001B[0m     train, \n\u001B[1;32m     33\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m     num_workers\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mnum_workers,\n\u001B[1;32m     38\u001B[0m )\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (x, y) \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     41\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     42\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1132\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[0;32m-> 1133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 31927) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "\n",
    "class TransformerDataset:\n",
    "    def __init__(self, text: str, context_len: int):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.data = torch.tensor(bpe.encode(text).ids, dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.data[index : index + self.context_len]\n",
    "        y = self.data[index + 1 : index + self.context_len + 1]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_len - 1\n",
    "\n",
    "\n",
    "with Config.input_path.open(encoding='utf-8') as f:\n",
    "    murakami = TransformerDataset(f.read(), Config.context_len)\n",
    "\n",
    "\n",
    "test_n = min(1000, int(len(murakami) * 0.1))\n",
    "train, test = random_split(murakami, [len(murakami) - test_n, test_n])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers.decoders import ByteLevel\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train, \n",
    "    batch_size=Config.batch_size,\n",
    "    sampler=torch.utils.data.RandomSampler(\n",
    "        train, replacement=True, num_samples=int(1e10)\n",
    "    ),\n",
    "    num_workers=Config.num_workers,\n",
    ")\n",
    "\n",
    "for (x, y) in dataloader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    print(ByteLevel().decode(x))\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'TransformerDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 30827) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    112\u001B[0m timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:424\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 424\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:931\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 931\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 416\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 30827) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 13\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtokenizers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecoders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ByteLevel\n\u001B[1;32m      4\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[1;32m      5\u001B[0m     train, \n\u001B[1;32m      6\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m     num_workers\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mnum_workers,\n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (x, y) \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     14\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     15\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1132\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[0;32m-> 1133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 30827) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers.decoders import ByteLevel\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train, \n",
    "    batch_size=Config.batch_size,\n",
    "    sampler=torch.utils.data.RandomSampler(\n",
    "        train, replacement=True, num_samples=int(1e10)\n",
    "    ),\n",
    "    num_workers=Config.num_workers,\n",
    ")\n",
    "\n",
    "for (x, y) in dataloader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    print(ByteLevel().decode(x))\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Makemore tokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: '.',\n -1: '_',\n 1: 'a',\n 2: 'b',\n 3: 'c',\n 4: 'd',\n 5: 'e',\n 6: 'f',\n 7: 'g',\n 8: 'h',\n 9: 'i',\n 10: 'j',\n 11: 'k',\n 12: 'l',\n 13: 'm',\n 14: 'n',\n 15: 'o',\n 16: 'p',\n 17: 'q',\n 18: 'r',\n 19: 's',\n 20: 't',\n 21: 'u',\n 22: 'v',\n 23: 'w',\n 24: 'x',\n 25: 'y',\n 26: 'z'}"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"/Users/vlad/MyDrive/AI/datasets/names/names.txt\")\n",
    "with path.open() as f:\n",
    "    text = f.read()\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(set(\"\".join(text.split())))\n",
    "        self.vocab_size = len(chars) + 1\n",
    "        self.itos = {i + 1: c for i, c in enumerate(chars)}\n",
    "        self.itos[0] = '.'\n",
    "        self.itos[-1] = '_'\n",
    "        self.stoi = {c: i for i, c in self.itos.items()}\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.itos\n",
    "    \n",
    "    def token_to_id(self, token: str) -> int:\n",
    "        return self.stoi[token]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return [self.stoi[c] for c in text]\n",
    "\n",
    "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        if skip_special_tokens:\n",
    "            ids = [i for i in ids if i != 0]\n",
    "        return \"\".join(self.itos[i] for i in ids)\n",
    "    \n",
    "tokenizer = CharTokenizer(text)\n",
    "dict(sorted(tokenizer.get_vocab().items(), key=lambda kv: kv[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "27\n",
      "{'<unk>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import CharBPETokenizer\n",
    "t = CharBPETokenizer()\n",
    "t.train_from_iterator([text], vocab_size=0, suffix=\"\")\n",
    "print(t.get_vocab_size())\n",
    "print(dict(sorted(t.get_vocab().items(), key=lambda kv: kv[1])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'a', 'b', 'c', 'd']"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = text.split()\n",
    "max_name_len = max(len(n) for n in names)\n",
    "t.enable_padding(\"left\", pad_id=0, pad_token=\".\", length=max_name_len)\n",
    "t.encode(\"abcd\").tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".emma.. -> emma.__\n",
      ".olivia -> olivia.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "names = names[:2]\n",
    "max_name_len = max(len(n) for n in names)\n",
    "xs = []\n",
    "ys = []\n",
    "for name in names:\n",
    "    \"\"\"\n",
    "    ....... -> _____.e\n",
    "    ......e -> ____.em\n",
    "    .....em -> ___.emm\n",
    "    ....emm -> __.emma\n",
    "    ...emma -> _.emma.\n",
    "    ....... -> _____.o\n",
    "    ......o -> ____.ol\n",
    "    .....ol -> ___.oli\n",
    "    ....oli -> __.oliv\n",
    "    ...oliv -> _.olivi\n",
    "    ..olivi -> .olivia\n",
    "    .olivia -> olivia.\n",
    "    \"\"\"\n",
    "    \n",
    "    # name = \".\" + name + \".\"\n",
    "    # for i in range(1, len(name)):\n",
    "    #     x = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    #     y = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    #     y[:] = -1\n",
    "    #     subname_x, subname_y = name[:i], name[:i + 1]\n",
    "    #     if len(subname_y) > len(y):\n",
    "    #         subname_y = subname_y[1:]  # trim leading dot for the longest word\n",
    "    #     x[-len(subname_x):] = torch.tensor(tokenizer.encode(subname_x))\n",
    "    #     y[-len(subname_y):] = torch.tensor(tokenizer.encode(subname_y))\n",
    "    #     xs.append(x)\n",
    "    #     ys.append(y)\n",
    "    #     print(\n",
    "    #         tokenizer.decode(x.tolist(), skip_special_tokens=False), '->',\n",
    "    #         tokenizer.decode(y.tolist(), skip_special_tokens=False)\n",
    "    #     )\n",
    "\n",
    "    x = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    y = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "\n",
    "    ids = [tokenizer.token_to_id(ch) for ch in name]\n",
    "\n",
    "    x[0] = tokenizer.token_to_id(\".\")\n",
    "    x[1: 1 + len(name)] = torch.tensor(ids)\n",
    "\n",
    "    y[0: len(name)] = torch.tensor(tokenizer.encode(name))\n",
    "    y[len(name)] = tokenizer.token_to_id(\".\")\n",
    "    y[len(name) + 1 :] = tokenizer.token_to_id(\"_\")\n",
    "\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    print(\n",
    "        tokenizer.decode(x.tolist(), skip_special_tokens=False), '->', \n",
    "        tokenizer.decode(y.tolist(), skip_special_tokens=False)\n",
    "    )\n",
    "\n",
    "xs = torch.stack(xs)\n",
    "ys = torch.stack(ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma [27, 5, 13, 13, 1] ['▁', 'e', 'm', 'm', 'a']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [4].  Tensor sizes: [5]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m x[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mtoken_to_id(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(name, tokenizer\u001B[38;5;241m.\u001B[39mencode(name)\u001B[38;5;241m.\u001B[39mids, tokenizer\u001B[38;5;241m.\u001B[39mencode(name)\u001B[38;5;241m.\u001B[39mtokens)\n\u001B[0;32m---> 12\u001B[0m \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(tokenizer\u001B[38;5;241m.\u001B[39mencode(name)\u001B[38;5;241m.\u001B[39mids)\n\u001B[1;32m     13\u001B[0m y[\u001B[38;5;241m0\u001B[39m: \u001B[38;5;28mlen\u001B[39m(name)] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(tokenizer\u001B[38;5;241m.\u001B[39mencode(name)\u001B[38;5;241m.\u001B[39mids)\n\u001B[1;32m     14\u001B[0m y[\u001B[38;5;28mlen\u001B[39m(name)] \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mtoken_to_id(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The expanded size of the tensor (4) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [4].  Tensor sizes: [5]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with open(path) as f:\n",
    "    names = f.read().split()\n",
    "max_name_len = max(len(n) for n in names)\n",
    "names = names[:5]\n",
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    x = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    y = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    x[0] = tokenizer.token_to_id(\".\")\n",
    "    print(name, tokenizer.encode(name).ids, tokenizer.encode(name).tokens)\n",
    "    x[1: 1 + len(name)] = torch.tensor(tokenizer.encode(name).ids)\n",
    "    y[0: len(name)] = torch.tensor(tokenizer.encode(name).ids)\n",
    "    y[len(name)] = tokenizer.token_to_id(\".\")\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    print(tokenizer.decode(x.tolist(), skip_special_tokens=False), '->', tokenizer.decode(y.tolist(), skip_special_tokens=False))\n",
    "\n",
    "xs = torch.stack(xs)\n",
    "ys = torch.stack(ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Back to murakami tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "save_path = input_path.with_name(input_path.stem + '-tokenizer.json')\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=str(input_path))\n",
    "tokenizer.save(str(save_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vlad/MyDrive/AI/datasets/murakami/murakami-1000lines-tokenizer.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "['ÐŀÐ¥', 'ÐŀÐ¢ÐĲ']"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "print(save_path)\n",
    "loaded = Tokenizer.from_file(str(save_path))\n",
    "loaded.encode('ОХОТА').tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ÐŀÐ¥', 'ÐŀÐ¢ÐĲ', 'Ċ', 'ÐĿÐĲ', 'ĠÐŀÐĴÐķÐ¦']\n",
      "ОХОТА\n",
      "НА ОВЕЦ\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import BaseTokenizer\n",
    "loaded2 = BaseTokenizer(loaded)\n",
    "print(loaded2.encode('ОХОТА\\nНА ОВЕЦ').tokens)\n",
    "print(loaded2.decode(loaded2.encode('ОХОТА\\nНА ОВЕЦ').ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error while initializing BPE: Merges text file invalid at line 29",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mSentencePieceBPETokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmurakami-1000lines-smp-bpe-vocab.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmerges\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmurakami-1000lines-smp-bpe-merges.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/tokenizers/implementations/sentencepiece_bpe.py:27\u001B[0m, in \u001B[0;36mSentencePieceBPETokenizer.__init__\u001B[0;34m(self, vocab, merges, unk_token, replacement, add_prefix_space, dropout, fuse_unk)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     18\u001B[0m     vocab: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     fuse_unk: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     25\u001B[0m ):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m vocab \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m merges \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 27\u001B[0m         tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer(\u001B[43mBPE\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmerges\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munk_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuse_unk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfuse_unk\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     29\u001B[0m         tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer(BPE())\n",
      "\u001B[0;31mException\u001B[0m: Error while initializing BPE: Merges text file invalid at line 29"
     ]
    }
   ],
   "source": [
    "SentencePieceBPETokenizer(\n",
    "    vocab=str(input_path.parent / \"murakami-1000lines-smp-bpe-vocab.json\"),\n",
    "    merges=str(input_path.parent / \"murakami-1000lines-smp-bpe-merges.txt\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "loaded.save(str(save_path) + '2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load tokenizer from file\n",
    "ByteLevelBPETokenizer\n",
    "tokenizer = SentencePieceBPETokenizer(\n",
    "    \"/Users/vlad/git/vladsaveliev/deeplearning/hipogpt/tmp/bpe-vocab.json\",\n",
    "    \"/Users/vlad/git/vladsaveliev/deeplearning/hipogpt/tmp/bpe-merges.txt\",\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}