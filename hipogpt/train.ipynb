{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_path = Path(\"/Users/vlad/googledrive/AI/datasets/murakami/murakami.txt\")\n",
    "if not input_path.exists():\n",
    "    input_path = Path(\"/content/drive/MyDrive/AI/datasets/murakami/murakami.txt\")\n",
    "    assert input_path.exists(), input_path\n",
    "    \n",
    "with input_path.open(encoding='utf-8') as f:\n",
    "    input_text = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10155065\n"
     ]
    }
   ],
   "source": [
    "print(len(input_text))\n",
    "train_text = input_text[:10_000]\n",
    "test_text = input_text[:152]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-trained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 22:08:14.849157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Охота на овец Часть первая 25.11.1970 ПИКНИК СРЕДИ НЕДЕЛИ О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bt = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(bt.backend_tokenizer.normalizer.normalize_str(test_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CharBPETokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['О', 'хот', 'а</w>', 'на</w>', 'о', 'ве', 'ц</w>', 'Ч', 'а', 'сть</w>', 'пер', 'вая</w>', '25</w>', '.</w>', '1', '1</w>', '.</w>', '19', '70</w>', 'П', 'И', 'К', 'Н', 'И', 'К</w>', 'С', 'Р', 'ЕД', 'И</w>', 'Н', 'ЕД', 'Е', 'Л', 'И</w>', 'О</w>', 'ее</w>', 'смерти</w>', 'со', 'общи', 'л</w>', 'мне</w>', 'по</w>', 'телеф', 'о', 'ну</w>', 'старый</w>', 'приятел', 'ь</w>', ',</w>', 'на', 'т', 'к', 'нувшись</w>', 'на</w>', 'случай', 'ные</w>', 'стро', 'ч', 'ки</w>', 'в</w>', 'газет', 'е</w>']\n",
      "Охота на овец Часть первая 25 . 11 . 1970 ПИКНИК СРЕДИ НЕДЕЛИ О ее смерти сообщил мне по телефону старый приятель , наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import CharBPETokenizer\n",
    "char_bpe = CharBPETokenizer()\n",
    "char_bpe.train_from_iterator(iter([train_text]))\n",
    "print(char_bpe.encode(test_text).tokens)\n",
    "print(char_bpe.decode(char_bpe.encode(test_text).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ByteLevelBPETokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['Ðŀ', 'ÑħÐ¾ÑĤ', 'Ð°', 'ĠÐ½Ð°', 'ĠÐ¾', 'Ð²', 'ÐµÑĨ', 'Ċ', 'Ð', '§', 'Ð°ÑģÑĤÑĮ', 'ĠÐ¿ÐµÑĢ', 'Ð²Ð°Ñı', 'Ċ', '25', '.', '1', '1', '.', '19', '70', 'Ċ', 'ÐŁ', 'ÐĺÐļ', 'ÐĿ', 'ÐĺÐļ', 'ĠÐ¡', 'Ðł', 'ÐķÐĶ', 'Ðĺ', 'ĠÐĿ', 'ÐķÐĶ', 'Ðķ', 'Ð', 'Ľ', 'Ðĺ', 'Ċ', 'Ðŀ', 'ĠÐµÐµ', 'ĠÑģÐ¼ÐµÑĢÑĤÐ¸', 'ĠÑģÐ¾Ð¾Ð±Ñī', 'Ð¸Ð»', 'ĠÐ¼Ð½Ðµ', 'ĠÐ¿Ð¾', 'ĠÑĤÐµÐ»ÐµÑĦÐ¾', 'Ð½Ñĥ', 'ĠÑģÑĤÐ°ÑĢÑĭÐ¹', 'ĠÐ¿ÑĢÐ¸Ñı', 'ÑĤ', 'ÐµÐ»ÑĮ', ',', 'ĠÐ½Ð°', 'ÑĤ', 'Ðº', 'Ð½ÑĥÐ²ÑĪÐ¸ÑģÑĮ', 'ĠÐ½Ð°', 'ĠÑģÐ»ÑĥÑĩÐ°Ð¹', 'Ð½ÑĭÐµ', 'ĠÑģÑĤÑĢÐ¾', 'Ñĩ', 'ÐºÐ¸', 'ĠÐ²', 'ĠÐ³Ð°Ð·ÐµÑĤ', 'Ðµ']\n",
      "Охота на овец\n",
      "Часть первая\n",
      "25.11.1970\n",
      "ПИКНИК СРЕДИ НЕДЕЛИ\n",
      "О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "bpe = ByteLevelBPETokenizer()\n",
    "bpe.train_from_iterator(iter([train_text]))\n",
    "print(bpe.encode(test_text).tokens)\n",
    "print(bpe.decode(bpe.encode(test_text).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SentencePieceBPETokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['▁О', 'хо', 'та', '▁на', '▁о', 'ве', 'ц', '\\n', 'Ч', 'а', 'сть', '▁пер', 'вая', '\\n', '25', '.1', '1', '.1', '9', '70', '\\n', 'П', 'ИК', 'Н', 'ИК', '▁С', 'Р', 'ЕД', 'И', '▁Н', 'ЕД', 'Е', 'Л', 'И', '\\n', 'О', '▁ее', '▁смер', 'ти', '▁сооб', 'щ', 'ил', '▁мне', '▁по', '▁телефо', 'ну', '▁старый', '▁прия', 'т', 'ель', ',', '▁на', 'т', 'к', 'нувши', 'сь', '▁на', '▁случай', 'ные', '▁стро', 'ч', 'ки', '▁в', '▁газет', 'е']\n",
      "Охота на овец\n",
      "Часть первая\n",
      "25.11.1970\n",
      "ПИКНИК СРЕДИ НЕДЕЛИ\n",
      "О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import SentencePieceBPETokenizer\n",
    "spm_bpe = SentencePieceBPETokenizer()\n",
    "spm_bpe.train_from_iterator(iter([train_text]))\n",
    "print(spm_bpe.encode(test_text).tokens)\n",
    "print(spm_bpe.decode(spm_bpe.encode(test_text).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordPiece"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['О', '##хо', '##та', 'на', 'о', '##ве', '##ц', 'Ч', '##ас', '##ть', 'пер', '##вая', '25', '.', '1', '##1', '.', '19', '##7', '##0', 'П', '##ИК', '##Н', '##ИК', 'С', '##Р', '##ЕД', '##И', 'Н', '##ЕД', '##Е', '##Л', '##И', 'О', 'ее', 'смерти', 'сооб', '##щ', '##ил', 'мне', 'по', 'телефо', '##ну', 'старый', 'прия', '##тел', '##ь', ',', 'на', '##т', '##к', '##нувшись', 'на', 'случай', '##ные', 'стро', '##ч', '##ки', 'в', 'газет', '##е']\n",
      "Охота на овец Часть первая 25. 11. 1970 ПИКНИК СРЕДИ НЕДЕЛИ О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import BertWordPieceTokenizer\n",
    "wp = BertWordPieceTokenizer(clean_text=False, lowercase=False)\n",
    "wp.train_from_iterator(iter([train_text]))\n",
    "print(wp.encode(test_text).tokens)\n",
    "print(wp.decode(wp.encode(test_text).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SentencePieceUnigramTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['▁О', 'хот', 'а', '▁на', '▁', 'ов', 'ец', '▁Ч', 'асть', '▁пер', 'ва', 'я', '▁25', '.1', '1', '.', '19', '70', '▁П', 'ИК', 'Н', 'ИК', '▁С', 'Р', 'ЕД', 'И', '▁Н', 'ЕД', 'Е', 'Л', 'И', '▁О', '▁ее', '▁смерти', '▁сообщ', 'ил', '▁мне', '▁по', '▁телефон', 'у', '▁стары', 'й', '▁приятел', 'ь', ',', '▁на', 'тк', 'нувшись', '▁на', '▁случайн', 'ые', '▁стро', 'ч', 'ки', '▁в', '▁газет', 'е']\n",
      "Охота на овец Часть первая 25.11.1970 ПИКНИК СРЕДИ НЕДЕЛИ О ее смерти сообщил мне по телефону старый приятель, наткнувшись на случайные строчки в газете\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import SentencePieceUnigramTokenizer\n",
    "smp_ug = SentencePieceUnigramTokenizer()\n",
    "smp_ug.train_from_iterator(iter([train_text]))\n",
    "print(smp_ug.encode(input_text[:152]).tokens)\n",
    "print(smp_ug.decode(smp_ug.encode(input_text[:152]).ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Murakami tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "input_path = Path(\"/Users/vlad/MyDrive/AI/datasets/murakami/murakami.txt\")\n",
    "assert input_path.exists(), input_path\n",
    "\n",
    "input_path = input_path.with_name(\"murakami-1000lines.txt\")\n",
    "\n",
    "\n",
    "class Config:\n",
    "    sample_only: str = True\n",
    "    seed = 0\n",
    "\n",
    "    # Dataset\n",
    "    input_path = input_path\n",
    "    vocab_size: int = 30_000\n",
    "    context_len: int = 32\n",
    "\n",
    "    # Network\n",
    "    emb_dim: int = 32\n",
    "    n_blocks: int = 2\n",
    "    n_heads: int = 4\n",
    "\n",
    "    # Optimization\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 5e-4\n",
    "    weight_decay: float = 0.01\n",
    "    num_workers: int = 1\n",
    "    max_steps: int = 100_000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "bpe = ByteLevelBPETokenizer()\n",
    "bpe.train(files=str(Config.input_path))\n",
    "bpe.save_model(str(Config.input_path.parent), \"bpe\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(Config.seed)\n",
    "torch.cuda.manual_seed_all(Config.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'TransformerDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 31927) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    112\u001B[0m timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:424\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 424\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:931\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 931\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 416\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 31927) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[116], line 40\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtokenizers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecoders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ByteLevel\n\u001B[1;32m     31\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[1;32m     32\u001B[0m     train, \n\u001B[1;32m     33\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m     num_workers\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mnum_workers,\n\u001B[1;32m     38\u001B[0m )\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (x, y) \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     41\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     42\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1132\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[0;32m-> 1133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 31927) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "\n",
    "class TransformerDataset:\n",
    "    def __init__(self, text: str, context_len: int):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.data = torch.tensor(bpe.encode(text).ids, dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.data[index : index + self.context_len]\n",
    "        y = self.data[index + 1 : index + self.context_len + 1]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_len - 1\n",
    "\n",
    "\n",
    "with Config.input_path.open(encoding='utf-8') as f:\n",
    "    murakami = TransformerDataset(f.read(), Config.context_len)\n",
    "\n",
    "\n",
    "test_n = min(1000, int(len(murakami) * 0.1))\n",
    "train, test = random_split(murakami, [len(murakami) - test_n, test_n])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers.decoders import ByteLevel\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train, \n",
    "    batch_size=Config.batch_size,\n",
    "    sampler=torch.utils.data.RandomSampler(\n",
    "        train, replacement=True, num_samples=int(1e10)\n",
    "    ),\n",
    "    num_workers=Config.num_workers,\n",
    ")\n",
    "\n",
    "for (x, y) in dataloader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    print(ByteLevel().decode(x))\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'TransformerDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 30827) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    112\u001B[0m timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:424\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 424\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:931\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 931\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 416\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 30827) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 13\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtokenizers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecoders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ByteLevel\n\u001B[1;32m      4\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[1;32m      5\u001B[0m     train, \n\u001B[1;32m      6\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m     num_workers\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mnum_workers,\n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (x, y) \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     14\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     15\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1132\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[0;32m-> 1133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 30827) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers.decoders import ByteLevel\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train, \n",
    "    batch_size=Config.batch_size,\n",
    "    sampler=torch.utils.data.RandomSampler(\n",
    "        train, replacement=True, num_samples=int(1e10)\n",
    "    ),\n",
    "    num_workers=Config.num_workers,\n",
    ")\n",
    "\n",
    "for (x, y) in dataloader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    print(ByteLevel().decode(x))\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Makemore tokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: '.',\n -1: '_',\n 1: 'a',\n 2: 'b',\n 3: 'c',\n 4: 'd',\n 5: 'e',\n 6: 'f',\n 7: 'g',\n 8: 'h',\n 9: 'i',\n 10: 'j',\n 11: 'k',\n 12: 'l',\n 13: 'm',\n 14: 'n',\n 15: 'o',\n 16: 'p',\n 17: 'q',\n 18: 'r',\n 19: 's',\n 20: 't',\n 21: 'u',\n 22: 'v',\n 23: 'w',\n 24: 'x',\n 25: 'y',\n 26: 'z'}"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"/Users/vlad/MyDrive/AI/datasets/names/names.txt\")\n",
    "with path.open() as f:\n",
    "    text = f.read()\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(set(\"\".join(text.split())))\n",
    "        self.vocab_size = len(chars) + 1\n",
    "        self.itos = {i + 1: c for i, c in enumerate(chars)}\n",
    "        self.itos[0] = '.'\n",
    "        self.itos[-1] = '_'\n",
    "        self.stoi = {c: i for i, c in self.itos.items()}\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.itos\n",
    "    \n",
    "    def token_to_id(self, token: str) -> int:\n",
    "        return self.stoi[token]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return [self.stoi[c] for c in text]\n",
    "\n",
    "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        if skip_special_tokens:\n",
    "            ids = [i for i in ids if i != 0]\n",
    "        return \"\".join(self.itos[i] for i in ids)\n",
    "    \n",
    "tokenizer = CharTokenizer(text)\n",
    "dict(sorted(tokenizer.get_vocab().items(), key=lambda kv: kv[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "27\n",
      "{'<unk>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import CharBPETokenizer\n",
    "t = CharBPETokenizer()\n",
    "t.train_from_iterator([text], vocab_size=0, suffix=\"\")\n",
    "print(t.get_vocab_size())\n",
    "print(dict(sorted(t.get_vocab().items(), key=lambda kv: kv[1])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'a', 'b', 'c', 'd']"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = text.split()\n",
    "max_name_len = max(len(n) for n in names)\n",
    "t.enable_padding(\"left\", pad_id=0, pad_token=\".\", length=max_name_len)\n",
    "t.encode(\"abcd\").tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".emma.. -> emma.__\n",
      ".olivia -> olivia.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "names = names[:2]\n",
    "max_name_len = max(len(n) for n in names)\n",
    "xs = []\n",
    "ys = []\n",
    "for name in names:\n",
    "    \"\"\"\n",
    "    ....... -> _____.e\n",
    "    ......e -> ____.em\n",
    "    .....em -> ___.emm\n",
    "    ....emm -> __.emma\n",
    "    ...emma -> _.emma.\n",
    "    ....... -> _____.o\n",
    "    ......o -> ____.ol\n",
    "    .....ol -> ___.oli\n",
    "    ....oli -> __.oliv\n",
    "    ...oliv -> _.olivi\n",
    "    ..olivi -> .olivia\n",
    "    .olivia -> olivia.\n",
    "    \"\"\"\n",
    "    \n",
    "    # name = \".\" + name + \".\"\n",
    "    # for i in range(1, len(name)):\n",
    "    #     x = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    #     y = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    #     y[:] = -1\n",
    "    #     subname_x, subname_y = name[:i], name[:i + 1]\n",
    "    #     if len(subname_y) > len(y):\n",
    "    #         subname_y = subname_y[1:]  # trim leading dot for the longest word\n",
    "    #     x[-len(subname_x):] = torch.tensor(tokenizer.encode(subname_x))\n",
    "    #     y[-len(subname_y):] = torch.tensor(tokenizer.encode(subname_y))\n",
    "    #     xs.append(x)\n",
    "    #     ys.append(y)\n",
    "    #     print(\n",
    "    #         tokenizer.decode(x.tolist(), skip_special_tokens=False), '->',\n",
    "    #         tokenizer.decode(y.tolist(), skip_special_tokens=False)\n",
    "    #     )\n",
    "\n",
    "    x = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    y = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "\n",
    "    ids = [tokenizer.token_to_id(ch) for ch in name]\n",
    "\n",
    "    x[0] = tokenizer.token_to_id(\".\")\n",
    "    x[1: 1 + len(name)] = torch.tensor(ids)\n",
    "\n",
    "    y[0: len(name)] = torch.tensor(tokenizer.encode(name))\n",
    "    y[len(name)] = tokenizer.token_to_id(\".\")\n",
    "    y[len(name) + 1 :] = tokenizer.token_to_id(\"_\")\n",
    "\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    print(\n",
    "        tokenizer.decode(x.tolist(), skip_special_tokens=False), '->', \n",
    "        tokenizer.decode(y.tolist(), skip_special_tokens=False)\n",
    "    )\n",
    "\n",
    "xs = torch.stack(xs)\n",
    "ys = torch.stack(ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma [27, 5, 13, 13, 1] ['▁', 'e', 'm', 'm', 'a']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [4].  Tensor sizes: [5]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m x[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mtoken_to_id(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(name, tokenizer\u001B[38;5;241m.\u001B[39mencode(name)\u001B[38;5;241m.\u001B[39mids, tokenizer\u001B[38;5;241m.\u001B[39mencode(name)\u001B[38;5;241m.\u001B[39mtokens)\n\u001B[0;32m---> 12\u001B[0m \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(tokenizer\u001B[38;5;241m.\u001B[39mencode(name)\u001B[38;5;241m.\u001B[39mids)\n\u001B[1;32m     13\u001B[0m y[\u001B[38;5;241m0\u001B[39m: \u001B[38;5;28mlen\u001B[39m(name)] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(tokenizer\u001B[38;5;241m.\u001B[39mencode(name)\u001B[38;5;241m.\u001B[39mids)\n\u001B[1;32m     14\u001B[0m y[\u001B[38;5;28mlen\u001B[39m(name)] \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mtoken_to_id(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The expanded size of the tensor (4) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [4].  Tensor sizes: [5]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with open(path) as f:\n",
    "    names = f.read().split()\n",
    "max_name_len = max(len(n) for n in names)\n",
    "names = names[:5]\n",
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    x = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    y = torch.zeros(max_name_len + 1, dtype=torch.long)\n",
    "    x[0] = tokenizer.token_to_id(\".\")\n",
    "    print(name, tokenizer.encode(name).ids, tokenizer.encode(name).tokens)\n",
    "    x[1: 1 + len(name)] = torch.tensor(tokenizer.encode(name).ids)\n",
    "    y[0: len(name)] = torch.tensor(tokenizer.encode(name).ids)\n",
    "    y[len(name)] = tokenizer.token_to_id(\".\")\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    print(tokenizer.decode(x.tolist(), skip_special_tokens=False), '->', tokenizer.decode(y.tolist(), skip_special_tokens=False))\n",
    "\n",
    "xs = torch.stack(xs)\n",
    "ys = torch.stack(ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2, 3, 4]])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = 'abc'\n",
    "torch.tensor(\n",
    "    tokenizer.encode('abc').ids, dtype=torch.long\n",
    ").unsqueeze(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "save_path = input_path.with_name(input_path.stem + '-tokenizer.json')\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=str(input_path))\n",
    "tokenizer.save(str(save_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vlad/MyDrive/AI/datasets/murakami/murakami-1000lines-tokenizer.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "['ÐŀÐ¥', 'ÐŀÐ¢ÐĲ']"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "print(save_path)\n",
    "loaded = Tokenizer.from_file(str(save_path))\n",
    "loaded.encode('ОХОТА').tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ÐŀÐ¥', 'ÐŀÐ¢ÐĲ', 'Ċ', 'ÐĿÐĲ', 'ĠÐŀÐĴÐķÐ¦']\n",
      "ОХОТА\n",
      "НА ОВЕЦ\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import BaseTokenizer\n",
    "loaded2 = BaseTokenizer(loaded)\n",
    "print(loaded2.encode('ОХОТА\\nНА ОВЕЦ').tokens)\n",
    "print(loaded2.decode(loaded2.encode('ОХОТА\\nНА ОВЕЦ').ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error while initializing BPE: Merges text file invalid at line 29",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mSentencePieceBPETokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmurakami-1000lines-smp-bpe-vocab.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmerges\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmurakami-1000lines-smp-bpe-merges.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/vladsaveliev/deeplearning/karpathy/venv/lib/python3.10/site-packages/tokenizers/implementations/sentencepiece_bpe.py:27\u001B[0m, in \u001B[0;36mSentencePieceBPETokenizer.__init__\u001B[0;34m(self, vocab, merges, unk_token, replacement, add_prefix_space, dropout, fuse_unk)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     18\u001B[0m     vocab: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     fuse_unk: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     25\u001B[0m ):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m vocab \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m merges \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 27\u001B[0m         tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer(\u001B[43mBPE\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmerges\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munk_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuse_unk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfuse_unk\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     29\u001B[0m         tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer(BPE())\n",
      "\u001B[0;31mException\u001B[0m: Error while initializing BPE: Merges text file invalid at line 29"
     ]
    }
   ],
   "source": [
    "SentencePieceBPETokenizer(\n",
    "    vocab=str(input_path.parent / \"murakami-1000lines-smp-bpe-vocab.json\"),\n",
    "    merges=str(input_path.parent / \"murakami-1000lines-smp-bpe-merges.txt\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "loaded.save(str(save_path) + '2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load tokenizer from file\n",
    "ByteLevelBPETokenizer\n",
    "tokenizer = SentencePieceBPETokenizer(\n",
    "    \"/Users/vlad/git/vladsaveliev/deeplearning/hipogpt/tmp/bpe-vocab.json\",\n",
    "    \"/Users/vlad/git/vladsaveliev/deeplearning/hipogpt/tmp/bpe-merges.txt\",\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}