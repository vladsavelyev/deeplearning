{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18TJM9SSO7I42ooJfww8kB7Drc9cXeQHx",
      "authorship_tag": "ABX9TyNvv/jXqVSUMYQEWUerQ9AZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c46d6ec8ab5e40a5a0baed0ee98aec75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cbc06ff7ebf4844962d9b09f9d54aa0",
              "IPY_MODEL_72dfd8e11a0f4038b45eadffcfec9fb7",
              "IPY_MODEL_b4f5aebf7fcb4fb7855a9074def015ad"
            ],
            "layout": "IPY_MODEL_fd069d2c910b4812a41683919958f4f7"
          }
        },
        "6cbc06ff7ebf4844962d9b09f9d54aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e42d5de8544457082f47b1e5d2d776b",
            "placeholder": "​",
            "style": "IPY_MODEL_5929e23233b343a59c1bc3a4d2c9ce01",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "72dfd8e11a0f4038b45eadffcfec9fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1883d87e3274cea8db0c2a5a75118b1",
            "max": 608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9920e7b8225d4aa4a8c6d1475dd4df81",
            "value": 608
          }
        },
        "b4f5aebf7fcb4fb7855a9074def015ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9db554e1b7c54f4a93feda7f9b864c8c",
            "placeholder": "​",
            "style": "IPY_MODEL_b8fdbe5aebd94ca09564f9fae25e2b89",
            "value": " 608/608 [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "fd069d2c910b4812a41683919958f4f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e42d5de8544457082f47b1e5d2d776b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5929e23233b343a59c1bc3a4d2c9ce01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1883d87e3274cea8db0c2a5a75118b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9920e7b8225d4aa4a8c6d1475dd4df81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9db554e1b7c54f4a93feda7f9b864c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8fdbe5aebd94ca09564f9fae25e2b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eac399699bb46e1b0d37c386885574d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23753fe474174e2c870e73b3cbd96a42",
              "IPY_MODEL_ad6a72f7ea73421ab127e9705e8a5a82",
              "IPY_MODEL_fed1a8a70ef547e8bd39250feb8a41ac"
            ],
            "layout": "IPY_MODEL_b02a99878f694dc391b73bdb2a4e0f23"
          }
        },
        "23753fe474174e2c870e73b3cbd96a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc74e6e092f349fda235d53ba43a346e",
            "placeholder": "​",
            "style": "IPY_MODEL_5a7c88bcb47d4de0a2e52380d9eda094",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "ad6a72f7ea73421ab127e9705e8a5a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f779c4685a954a19b2fc75e583bb6ed4",
            "max": 1713123,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc787acf85db43d08ac9a042e4fa163c",
            "value": 1713123
          }
        },
        "fed1a8a70ef547e8bd39250feb8a41ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b39740c3065843ab963af86e46ced0f4",
            "placeholder": "​",
            "style": "IPY_MODEL_10f416e40730492a95048fc3c523af8d",
            "value": " 1.71M/1.71M [00:00&lt;00:00, 10.4MB/s]"
          }
        },
        "b02a99878f694dc391b73bdb2a4e0f23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc74e6e092f349fda235d53ba43a346e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a7c88bcb47d4de0a2e52380d9eda094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f779c4685a954a19b2fc75e583bb6ed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc787acf85db43d08ac9a042e4fa163c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b39740c3065843ab963af86e46ced0f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f416e40730492a95048fc3c523af8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ee7b782210f41c991a942a310d33c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_962021fd0a5b44e49bf8e8336ef99a05",
              "IPY_MODEL_a38fb4bf4161408ca5ff80731b34b1e7",
              "IPY_MODEL_4c6f271dfbfa483a87767731a88ba3fb"
            ],
            "layout": "IPY_MODEL_3124758ad5504ae1a3e65a44fb59a09a"
          }
        },
        "962021fd0a5b44e49bf8e8336ef99a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bbef735c4754cfaa72f7780cdb560ff",
            "placeholder": "​",
            "style": "IPY_MODEL_108745477c734612a687f9c84cc21a97",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "a38fb4bf4161408ca5ff80731b34b1e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ba2c6186864416bae3dcfdf393a2d7",
            "max": 1270925,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43a10ba7ad554a459da166cde27ff259",
            "value": 1270925
          }
        },
        "4c6f271dfbfa483a87767731a88ba3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_121a9c93571b4fbda563ed1c90030122",
            "placeholder": "​",
            "style": "IPY_MODEL_26d03bbf8e894c75b4413e0efc3c9b27",
            "value": " 1.27M/1.27M [00:00&lt;00:00, 9.24MB/s]"
          }
        },
        "3124758ad5504ae1a3e65a44fb59a09a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bbef735c4754cfaa72f7780cdb560ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108745477c734612a687f9c84cc21a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2ba2c6186864416bae3dcfdf393a2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a10ba7ad554a459da166cde27ff259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "121a9c93571b4fbda563ed1c90030122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d03bbf8e894c75b4413e0efc3c9b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9170a5f986b44063b4e2be83d6e15c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e241af04ed394da59f69723196e4dbb7",
              "IPY_MODEL_f90b351f4d3a4e739985ed7970439a1d",
              "IPY_MODEL_ccd7723aae1541d888b965f49d78f137"
            ],
            "layout": "IPY_MODEL_3f01415135504ba0818ba635a16ae65e"
          }
        },
        "e241af04ed394da59f69723196e4dbb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1fe14f888234e74a1c13d4bca6d7608",
            "placeholder": "​",
            "style": "IPY_MODEL_f5bde06bbf28493288c9d37034dce260",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "f90b351f4d3a4e739985ed7970439a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97ad3ff914524f51aaed2e35b2bed429",
            "max": 551290714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_742c82d360484054a081f6c84e9bffc1",
            "value": 551290714
          }
        },
        "ccd7723aae1541d888b965f49d78f137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15dfdde39b394a0ab334649ab8d70988",
            "placeholder": "​",
            "style": "IPY_MODEL_ff7b6035838143d8a4584d75131de0bf",
            "value": " 551M/551M [00:05&lt;00:00, 101MB/s]"
          }
        },
        "3f01415135504ba0818ba635a16ae65e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1fe14f888234e74a1c13d4bca6d7608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5bde06bbf28493288c9d37034dce260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97ad3ff914524f51aaed2e35b2bed429": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "742c82d360484054a081f6c84e9bffc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15dfdde39b394a0ab334649ab8d70988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff7b6035838143d8a4584d75131de0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladsavelyev/deeplearning/blob/master/pretgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "DRIVE_PATH = Path(\"/content/drive/MyDrive\")\n",
        "!pip install torch transformers datasets tokenizers evaluate wandb"
      ],
      "metadata": {
        "id": "FvgLsXYozV8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import Trainer, TrainingArguments, TrainerCallback"
      ],
      "metadata": {
        "id": "rFsAqSJQ7vn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "model = AutoModelForCausalLM.from_pretrained(name)"
      ],
      "metadata": {
        "id": "mS22gUFd7svb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182,
          "referenced_widgets": [
            "c46d6ec8ab5e40a5a0baed0ee98aec75",
            "6cbc06ff7ebf4844962d9b09f9d54aa0",
            "72dfd8e11a0f4038b45eadffcfec9fb7",
            "b4f5aebf7fcb4fb7855a9074def015ad",
            "fd069d2c910b4812a41683919958f4f7",
            "7e42d5de8544457082f47b1e5d2d776b",
            "5929e23233b343a59c1bc3a4d2c9ce01",
            "b1883d87e3274cea8db0c2a5a75118b1",
            "9920e7b8225d4aa4a8c6d1475dd4df81",
            "9db554e1b7c54f4a93feda7f9b864c8c",
            "b8fdbe5aebd94ca09564f9fae25e2b89",
            "1eac399699bb46e1b0d37c386885574d",
            "23753fe474174e2c870e73b3cbd96a42",
            "ad6a72f7ea73421ab127e9705e8a5a82",
            "fed1a8a70ef547e8bd39250feb8a41ac",
            "b02a99878f694dc391b73bdb2a4e0f23",
            "dc74e6e092f349fda235d53ba43a346e",
            "5a7c88bcb47d4de0a2e52380d9eda094",
            "f779c4685a954a19b2fc75e583bb6ed4",
            "fc787acf85db43d08ac9a042e4fa163c",
            "b39740c3065843ab963af86e46ced0f4",
            "10f416e40730492a95048fc3c523af8d",
            "2ee7b782210f41c991a942a310d33c51",
            "962021fd0a5b44e49bf8e8336ef99a05",
            "a38fb4bf4161408ca5ff80731b34b1e7",
            "4c6f271dfbfa483a87767731a88ba3fb",
            "3124758ad5504ae1a3e65a44fb59a09a",
            "5bbef735c4754cfaa72f7780cdb560ff",
            "108745477c734612a687f9c84cc21a97",
            "f2ba2c6186864416bae3dcfdf393a2d7",
            "43a10ba7ad554a459da166cde27ff259",
            "121a9c93571b4fbda563ed1c90030122",
            "26d03bbf8e894c75b4413e0efc3c9b27",
            "9170a5f986b44063b4e2be83d6e15c5e",
            "e241af04ed394da59f69723196e4dbb7",
            "f90b351f4d3a4e739985ed7970439a1d",
            "ccd7723aae1541d888b965f49d78f137",
            "3f01415135504ba0818ba635a16ae65e",
            "a1fe14f888234e74a1c13d4bca6d7608",
            "f5bde06bbf28493288c9d37034dce260",
            "97ad3ff914524f51aaed2e35b2bed429",
            "742c82d360484054a081f6c84e9bffc1",
            "15dfdde39b394a0ab334649ab8d70988",
            "ff7b6035838143d8a4584d75131de0bf"
          ]
        },
        "outputId": "35ac4cca-fbfe-416e-ab5f-8d050e237420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c46d6ec8ab5e40a5a0baed0ee98aec75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1eac399699bb46e1b0d37c386885574d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ee7b782210f41c991a942a310d33c51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/551M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9170a5f986b44063b4e2be83d6e15c5e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model.config.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tiw63UTD734X",
        "outputId": "668e8fec-0054-41aa-a308-6d22b66111c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50264"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESc3hEvtgjWJ",
        "outputId": "18a17809-7972-4982-a240-5bd406d91cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50258"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDataset(Dataset):\n",
        "    def __init__(self, token_ids: np.memmap, n_ctx: int):\n",
        "        self.token_ids = token_ids\n",
        "        self.n_ctx = n_ctx\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t = torch.LongTensor(self.token_ids[idx:idx + self.n_ctx])\n",
        "        return {\"input_ids\": t, \"labels\": t}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids) - self.n_ctx + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path, tokenizer, n_ctx: int, max_n_examples: int = None) -> 'TransformerDataset':\n",
        "        save_path = path.with_suffix(\".token_ids.pt\")\n",
        "        if False and save_path.exists():\n",
        "            print(f\"Loading dataset from {save_path}\")\n",
        "            ids = torch.load(str(save_path))\n",
        "        else:\n",
        "            with open(path, \"r\") as f:\n",
        "                text = f.read()\n",
        "                print(f\"Characters in text: {len(text):,}\")\n",
        "            ids = tokenizer(text, return_tensors=\"pt\")['input_ids'].squeeze().long()\n",
        "            if max_n_examples:\n",
        "                max_tokens = max_n_examples + n_ctx - 1\n",
        "                print(f\"Taking first {max_tokens} tokens to make it {max_n_examples} examples\")\n",
        "                ids = ids[:max_tokens]\n",
        "            eos = torch.tensor([tokenizer.eos_token_id]).long()\n",
        "            ids = torch.concat((ids, eos))\n",
        "            torch.save(ids, save_path)\n",
        "        print(f\"Dataset shape: {ids.shape}\")\n",
        "        return TransformerDataset(ids, n_ctx)\n",
        "\n",
        "\n",
        "test_text_path = DRIVE_PATH / \"AI\" / \"datasets\" / \"murakami\" / \"murakami_test.txt\"\n",
        "train_text_path = DRIVE_PATH / \"AI\" / \"datasets\" / \"murakami\" / \"murakami_train.txt\"\n",
        "test_set = TransformerDataset.load(test_text_path, tokenizer, model.config.n_ctx, max_n_examples=100)\n",
        "train_set = TransformerDataset.load(train_text_path, tokenizer, model.config.n_ctx)"
      ],
      "metadata": {
        "id": "NzSY_V9_wBQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5c6dc6-20dd-4db4-dc2e-c9e2175162bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters in text: 27,685\n",
            "Taking first 2147 tokens to make it 100 examples\n",
            "Dataset shape: torch.Size([2148])\n",
            "Characters in text: 10,127,380\n",
            "Dataset shape: torch.Size([2503610])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.token_ids.min(), test_set.token_ids.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fx5dHkXg7xh",
        "outputId": "212f2547-a3b4-4426-a950-e3860e303e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(5), tensor(50257))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0]['input_ids'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0DHbTE0fpSO",
        "outputId": "06678f9d-fcf6-438f-e194-81e782f69923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2048])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{sum([p.numel() for p in model.parameters()]):,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TUTHqanEUDI",
        "outputId": "fd1bd460-bf3e-43cf-910c-af98098ffd2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125,231,616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.n_ctx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DgvHNbtELOI",
        "outputId": "e4e779f2-ced3-4b34-deea-10e53c358a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = train_set[1000]\n",
        "res = model(input_ids=b['input_ids'].view(1, -1), labels=b['labels'].view(1, -1))\n",
        "print(res['loss'])\n",
        "\n",
        "t = tokenizer(\"Определенно, это случилось с нами не в первый раз\")['input_ids']\n",
        "t = torch.LongTensor(t).view(1, -1)\n",
        "res = model(input_ids=t, labels=t)\n",
        "print(res['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElfqMozDdXUL",
        "outputId": "99733309-ee26-4f64-ceaa-e88515fb1467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.5174, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4814, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "save_dir = DRIVE_PATH / \"AI\" / \"pretgpt\" / \"murakami_rugpt3small\"\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "if (last_checkpoint_dir := get_last_checkpoint(str(save_dir))):\n",
        "    last_checkpoint_dir = Path(last_checkpoint_dir)\n",
        "    print(last_checkpoint_dir)\n",
        "    print('  '.join(t.name for t in last_checkpoint_dir.iterdir()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMSzHc0XxXTk",
        "outputId": "9823f590-f18c-451f-eaf9-ca36893bad4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-11000\n",
            "config.json  generation_config.json  pytorch_model.bin  training_args.bin  optimizer.pt  scheduler.pt  trainer_state.json  rng_state.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izjv2U2xOrEH",
        "outputId": "0d7ee575-9929-4144-9d06-59c0afa862be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint-11000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers.utils import WEIGHTS_NAME\n",
        "# state_dict = torch.load(str(last_checkpoint_dir / WEIGHTS_NAME), map_location=\"cpu\")\n",
        "# model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjctH2z_E00i",
        "outputId": "89b33b57-e024-4eb7-fed1-484d862e0988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(num_seqs=10, max_length=10):\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    for i, seq in enumerate(model.generate(\n",
        "        max_length=max_length,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=num_seqs,\n",
        "        do_sample=True, \n",
        "        top_k=50,\n",
        "        pad_token_id=0,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "    )):\n",
        "        print(i + 1, tokenizer.decode(seq))\n",
        "\n",
        "sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sm_rSy-Qvkn",
        "outputId": "2c6232d9-6e38-4aad-e100-dde6f3ed182c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 <|endoftext|>, или что-то вроде этого.\n",
            "\n",
            "2 <|endoftext|>.  И вот что я нашел. \n",
            "\n",
            "3 <|endoftext|>.\n",
            "\n",
            "— Это что, шутка\n",
            "4 <|endoftext|> и без того некрасивую фигуру,\n",
            "5 <|endoftext|> - &quot;природа в человеке&\n",
            "6 <|endoftext|>.  Поэтому, когда вы выбираете одежду\n",
            "7 <|endoftext|> на работу не берут\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "8 <|endoftext|> к этой теме.  В качестве примера возьмем\n",
            "9 <|endoftext|>, который для меня был загадкой. Я не\n",
            "10 <|endoftext|> по адресу: город Москва, ул. Большая\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_API_KEY'] = '270e81630bd1fd3c78a355d1711966c75ce75bcc'\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"pretgpt\"\n",
        "from importlib import reload\n",
        "import wandb\n",
        "reload(wandb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSmtIn0meIhx",
        "outputId": "eb3835cb-ff6c-485c-ea6f-b66f7b1834dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'wandb' from '/usr/local/lib/python3.8/dist-packages/wandb/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "save_dir = DRIVE_PATH / \"AI\" / \"pretgpt\" / \"murakami_rugpt3small\"\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "if (last_checkpoint_dir := get_last_checkpoint(str(save_dir))):\n",
        "    last_checkpoint_dir = Path(last_checkpoint_dir)\n",
        "    print(last_checkpoint_dir)\n",
        "    print('  '.join(t.name for t in last_checkpoint_dir.iterdir()))\n",
        "\n",
        "class MyCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
        "        if metrics:\n",
        "            print(f'Eval loss so far: {metrics[\"eval_loss\"]:.4f}')\n",
        "        if state.best_metric:\n",
        "            print(f\"Best loss so far: {state.best_metric:.4f}\")\n",
        "        sample()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=test_set,\n",
        "    callbacks=[MyCallback],\n",
        "    args=TrainingArguments(\n",
        "        output_dir=str(save_dir),\n",
        "        report_to=['wandb'],\n",
        "        overwrite_output_dir=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=1000,\n",
        "        save_steps=1000,\n",
        "        save_total_limit=2,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        ignore_data_skip=True,\n",
        "    ),\n",
        ")\n",
        "trainer.train(resume_from_checkpoint=last_checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "21qR97SwzT1R",
        "outputId": "f4a9c0b2-4602-4ba1-d524-c737da2e45f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "Loading model from /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-1000.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-1000\n",
            "config.json  generation_config.json  pytorch_model.bin  training_args.bin  optimizer.pt  scheduler.pt  trainer_state.json  rng_state.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 2501563\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3752346\n",
            "  Number of trainable parameters = 125231616\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 0\n",
            "  Continuing training from global step 1000\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11123' max='3752346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  11123/3752346 1:10:29 < 434:16:51, 2.39 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.379800</td>\n",
              "      <td>3.487939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.406300</td>\n",
              "      <td>3.641360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.282300</td>\n",
              "      <td>3.718564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.146300</td>\n",
              "      <td>3.831849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>2.009200</td>\n",
              "      <td>3.990011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.903500</td>\n",
              "      <td>4.049981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.834900</td>\n",
              "      <td>4.235762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.718300</td>\n",
              "      <td>4.300036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.627800</td>\n",
              "      <td>4.345162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>1.551100</td>\n",
              "      <td>4.461396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 3.4879\n",
            "1 <|endoftext|>, и ее легко можно было назвать спортивной:\n",
            "2 <|endoftext|> кожи – и все-таки она не производ\n",
            "3 <|endoftext|> кожу, как и раньше.\n",
            "– В\n",
            "4 <|endoftext|> кожу. Затем она сменила имя и внешность.\n",
            "5 <|endoftext|> – и у меня от его дыхания перехваты\n",
            "6 <|endoftext|> назад.\n",
            "– Так вы все – из\n",
            "7 <|endoftext|>, но ничего примечательного. Поэтому она с\n",
            "8 <|endoftext|>, и она по крайней мере понимала, что\n",
            "9 <|endoftext|>. Все, что нужно знать об этом источ\n",
            "10 <|endoftext|>. В комнате повисла такая тишь, будто\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-2000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-2000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-2000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-750] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 3.6414\n",
            "1 <|endoftext|>, от которого у меня все волосы на лоб\n",
            "2 <|endoftext|> — и мне все-таки пришлось. На\n",
            "3 <|endoftext|>.\n",
            "– Если честно, я все понял\n",
            "4 <|endoftext|>. И это — одна из проблем, которые\n",
            "5 <|endoftext|>с, я решил посмотреть на эту парочку во\n",
            "6 <|endoftext|> назад.\n",
            "– Так, может, загля\n",
            "7 <|endoftext|>, но ничего примечательного она на фото не\n",
            "8 <|endoftext|>, и она по очереди, словно проверяя\n",
            "9 <|endoftext|>. Все, что касалось моей работы, уже\n",
            "10 <|endoftext|>. В его присутствии в сознании еще теплится\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-3000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-3000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-3000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-3000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-1000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 3.7186\n",
            "1 <|endoftext|>, потом ее вернули жене. Когда дело касалось\n",
            "2 <|endoftext|> – и мне все-таки пришлось. На\n",
            "3 <|endoftext|>.\n",
            "— Извините, господин Окада\n",
            "4 <|endoftext|>. И это — правда!\n",
            "— Просто\n",
            "5 <|endoftext|> – и у меня отлегло от сердца\n",
            "6 <|endoftext|> назад.\n",
            "– Так, может, загля\n",
            "7 <|endoftext|>, но ничего примечательного. Поэтому она предложила\n",
            "8 <|endoftext|>, чтобы она знала, как обращаться.\n",
            "\n",
            "9 <|endoftext|>.\n",
            "– Почему? В школе мне на\n",
            "10 <|endoftext|>.\n",
            "У меня в сознании еще остается какой\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-4000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-4000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-4000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-4000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-2000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 3.8318\n",
            "1 <|endoftext|>, потом ее вернули жене. Когда ее обнаружили\n",
            "2 <|endoftext|> – и мне все равно.\n",
            "Я с\n",
            "3 <|endoftext|>.\n",
            "– Извините, Окада-\n",
            "4 <|endoftext|>. И это меня потрясло.\n",
            "–\n",
            "5 <|endoftext|> – и у меня отлегло от сердца\n",
            "6 <|endoftext|> назад.\n",
            "– Так или иначе, мне\n",
            "7 <|endoftext|>, но ничего примечательного они на самом деле\n",
            "8 <|endoftext|>, когда она по очереди выходила из бассейна с\n",
            "9 <|endoftext|>. Все, что нужно знать об этом мире\n",
            "10 <|endoftext|>.\n",
            "У меня в голове мельтешили\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-5000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-5000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-5000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-5000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-3000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 3.9900\n",
            "1 <|endoftext|>.\n",
            "– Ну, может, так оно\n",
            "2 <|endoftext|>. А может, и нет. И если\n",
            "3 <|endoftext|>.\n",
            "— Если честно, я все это\n",
            "4 <|endoftext|>. И это — одна из проблем, которые\n",
            "5 <|endoftext|>с, я решил поискать его глазами, но\n",
            "6 <|endoftext|> назад.\n",
            "– Так вы все-таки\n",
            "7 <|endoftext|>, но ничего примечательного она от него не\n",
            "8 <|endoftext|>.\n",
            "А ведь он мог ее убить.\n",
            "9 <|endoftext|>. Все, что нужно, – это то\n",
            "10 <|endoftext|>.\n",
            "У меня в сумке были только сигареты\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-6000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-6000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-6000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-6000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-4000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 4.0500\n",
            "1 <|endoftext|>, потом ее вернули жене. И теперь уже\n",
            "2 <|endoftext|>, – произнес Хираку Макимура\n",
            "3 <|endoftext|>.\n",
            "— Если честно, я все это\n",
            "4 <|endoftext|>. И это меня потрясло. До сих\n",
            "5 <|endoftext|> себе, укуталась в одеяло, зад\n",
            "6 <|endoftext|> назад.\n",
            "– Так или иначе – это\n",
            "7 <|endoftext|>:\n",
            "– В этом мире я – единственный\n",
            "8 <|endoftext|>, и она продолжила работу над «Кок\n",
            "9 <|endoftext|>. Все, что нажито в браке,\n",
            "10 <|endoftext|>. В комнате повисла такая тишина, что можно\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-7000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-7000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-7000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-7000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-5000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 4.2358\n",
            "1 <|endoftext|>, потом ее вернули жене. Когда мне было\n",
            "2 <|endoftext|>том – и все-таки за каким чер\n",
            "3 <|endoftext|>.\n",
            "– Если честно, я все это\n",
            "4 <|endoftext|>. И это, мне кажется, было самое\n",
            "5 <|endoftext|>цию, похожую на ту, что вы\n",
            "6 <|endoftext|>лся: «Это мое дело, – отрезал\n",
            "7 <|endoftext|>:\n",
            "– В общем, на самом деле\n",
            "8 <|endoftext|>, и она снова посмотрела на часы.\n",
            "\n",
            "9 <|endoftext|>. Все, что нужно знать об этом мире\n",
            "10 <|endoftext|>.\n",
            "У меня в голове еще долго вер\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-8000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-8000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-8000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-8000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-6000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 4.3000\n",
            "1 <|endoftext|>, потом ее вернули жене. Когда все закончилось\n",
            "2 <|endoftext|>. А мне все-таки хотелось. На\n",
            "3 <|endoftext|>.\n",
            "— Если честно, я все это\n",
            "4 <|endoftext|>. И это — важнейшая часть его жизни\n",
            "5 <|endoftext|>. В любом случае, на свете бывают и\n",
            "6 <|endoftext|> назад.\n",
            "– Так, значит… Ну\n",
            "7 <|endoftext|>:\n",
            "– В этом мире нет места,\n",
            "8 <|endoftext|>, и она по очереди снимает очки.\n",
            "\n",
            "9 <|endoftext|>. Все, что нужно, – это узнать\n",
            "10 <|endoftext|>. В комнате, в которую мы въехали\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-9000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-9000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-9000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-9000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-7000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 4.3452\n",
            "1 <|endoftext|> у них действительно есть. Вы понимаете, о\n",
            "2 <|endoftext|>. А может, и нет. Как бы\n",
            "3 <|endoftext|>.\n",
            "— Если честно, я все это\n",
            "4 <|endoftext|>. И это — одна из проблем, которые\n",
            "5 <|endoftext|>. В любом случае, на свете, возможно\n",
            "6 <|endoftext|> назад.\n",
            "– Так или иначе – сейчас\n",
            "7 <|endoftext|>:\n",
            "– В общем, на самом деле\n",
            "8 <|endoftext|>, и она по очереди снимает очки.\n",
            "\n",
            "9 <|endoftext|>. Все, что касалось моей работы, решено\n",
            "10 <|endoftext|>. В комнате повисла такая тишь, будто\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-10000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-10000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-10000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-10000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-8000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 4.4614\n",
            "1 <|endoftext|> у окна.\n",
            "У меня есть сестра.\n",
            "2 <|endoftext|>. А может, и нет. Как бы\n",
            "3 <|endoftext|>.\n",
            "— Если честно, — сказал я\n",
            "4 <|endoftext|>. И это — важнейшая часть его жизни\n",
            "5 <|endoftext|>. В любом случае, на обратном пути,\n",
            "6 <|endoftext|> назад.\n",
            "– Так, может, и\n",
            "7 <|endoftext|>, но ничего примечательного она от себя не\n",
            "8 <|endoftext|>, и она по очереди снимает очки.\n",
            "\n",
            "9 <|endoftext|>. Все, что касалось моей работы, решено\n",
            "10 <|endoftext|>.\n",
            "У меня в голове еще несколько раз\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-11000\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-11000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-11000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-11000/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-9000] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-ed8703df4251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     ),\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_checkpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         )\n\u001b[0;32m-> 1543\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2571\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2572\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    885\u001b[0m                 )\n\u001b[1;32m    886\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    888\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upcast_and_reordered_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mmask_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample(num_seqs=1, max_length=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn2chEdAMwyD",
        "outputId": "def3bf9a-4436-459a-bf99-5e328409a2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 <|endoftext|>-флейте у моей двери была дверь в ванную. В коридоре я стоял на шаг-парадном и разглядывал цветочки.\n",
            "– Это, похоже, тюльпан, – сказал я.\n",
            "– Бальзаматы? – Она обернулась и взглянула на меня так, словно видела перед собой какую-то редкую зверушку.\n",
            "– Угу.\n",
            "– Ты что, на тюленье окно похож?\n",
            "– Конечно.\n",
            "– А это что, действительно тюльпан?\n",
            "– Конечно.\n",
            "– А откуда он растет?\n",
            "Я пошарил в карманах куртки, нащупал коробку с тюльпаном и тюльпаном и снова дернув за ручку, открыл дверь. Пока я искал тюльпан и вешалку, дождь все лил и лил – настолько обильно, что в ванной заискрился шербет. «Тюльпановы» у меня дома – коллекция еще с тех времен, когда мне семья выдраного корчилола глазки.\n",
            "– \n",
            "В ванной мы оба долго ждали, пока он выйдет, – чтобы обнаружить, что кто-то снова там появится.\n",
            "В конце концов дверь приоткрылась и стряхнула с себя капли. На миг в ванной посветлели, поблекли и померкли все краски. Сдохнув, дверь подалась на петлях, и на меня в окутавшую нас влагу вдруг пролился томительный мрак. Я остолбенело смотрел на проветриваемую комнату. Он был страшно красив, и при взгляде на него у меня щемило в груди. Вот ведь каково человеку в такие минуты!\n",
            "«Тишина бездонна, – подумал я. – В ней заключено всё: и вечность, и свет, и красота, и роковая тоска».\n",
            "Точно в продолжение этой мысли я распахнул дверь: комната была жутко просторной. Темно-синяя, очень ванная, напоминающая кубический ящик. Не меньше того, что я ожидал увидеть здесь в дальней стене. Там, видимо, был выход на некое подобие завещания, так сказать, временного интервала. И в то же время – что-то вроде родника. В этом пространстве разрешались все необходимые для жизни вопросы.\n",
            "В спальне на меня, как обычно, легла ночная спальная рубашка, похожая на скомканную бумагу. Простая, не стирана и не глаженная. Я застегнул пуговицы на рубашке. По-моему, на Аояма\n",
            "– Ничего нет, – сказала подруга.\n",
            "– И все-таки что-нибудь есть?\n",
            "– Все, кроме тебя.\n",
            "– Ну и чего ты стесняешься?\n",
            "– Ничего… Мне надо побыть немного в комнате. Ничего больше делать не нужно.\n",
            "– В какой это – «всякая куча всяческих всячинностей» \n",
            "Она взяла мой пенис и с тем же подозрительным видом прильнула. Я замер, уставившись в стену напротив.\n",
            "– М-м?\n",
            "– Ты можешь мне сейчас это показать? – попросил я у подруги. – Здесь \n",
            "– Хм.\n",
            "– У меня такое чувство, будто я давным-давно ничего не видел.\n",
            "– Мне все время казалось, будто я – это ты, – сказал я. – Что я все это видел. Вот уже много лет. И со всем этим постоянно куда-то ходил.\n",
            "– М-м?\n",
            "Я погладил ее клитор, и его мягкие складки опять на мне вызволили мою из глубин подсознания.\n",
            "– Ты что… так со мной не ладишь?\n",
            "Она чуть пожала плечами.\n",
            "– Послушай, может, все-таки объяснишь мне свои ощущения? Представь, что тебе в голову приходит что-то очень важное. Скажем, сейчас где-то идет дождь… или, скажем, снится тебе что-то очень яркое. Ты в этом состоянии раздвигаешь шторы, открываешь глаза и видишь все это в мельчайших деталях. И после этого ты уже ничего не можешь понять: все в порядке, все замечательно, все хорошо. Только мне кажется, что вот-вот дождь прекратится, идет дальше – или придет когда-то время – но не видно этого вообще никогда.\n",
            "– Может быть. – Я попробовал взять у нее за шиворот всю ту липкую сперму, которую она выдавливала в моей постели с утра до вечера, – но не вышло. Не потому, что липкая она была, а потому, что мне эту штуку просто хотелось воспроизвести в голове и надавливал сосок до упора.\n",
            "– Хм.\n",
            "– Иногда кажется, что тебе не двадцать пять, а еще что-то около тридцати. Хотя, конечно, ты это не замечаешь.\n",
            "– О чем это ты?\n",
            "Она чуть напряглась, взяла чуть разом и чуть языком слизнула вытекавшую из мочки уха капавшую слюну. Я поймал ее на руки, прижал к себе и бережно положил на простыню.\n",
            "– Это твое.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
        "\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "save_dir = DRIVE_PATH / \"AI\" / \"pretgpt\" / \"murakami_rugpt3small\"\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "if last_checkpoint_dir := get_last_checkpoint(str(save_dir)):\n",
        "    last_checkpoint_dir = Path(last_checkpoint_dir)\n",
        "    print(last_checkpoint_dir)\n",
        "    print('  '.join(t.name for t in last_checkpoint_dir.iterdir()))\n",
        "\n",
        "from transformers.utils import WEIGHTS_NAME\n",
        "import torch\n",
        "state_dict = torch.load(str(last_checkpoint_dir / WEIGHTS_NAME), map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict)    \n",
        "\n",
        "def generate(**kwargs):\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    seq = model.generate(\n",
        "        max_length=500,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=0,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        **kwargs,\n",
        "    )[0]\n",
        "    print(tokenizer.decode(seq))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6N-tfRwV37n",
        "outputId": "c978aff8-3b4e-462e-d17b-237c1728f20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-11000\n",
            "config.json  generation_config.json  pytorch_model.bin  training_args.bin  optimizer.pt  scheduler.pt  trainer_state.json  rng_state.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"My search:\")\n",
        "generate(\n",
        "    do_sample=True, \n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R18x8g5RWAE9",
        "outputId": "5c73fc14-27ad-417e-db52-d592b31932d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My search:\n",
            "<|endoftext|>, а волосы были пострижены коротко и совсем коротко. А сам он, долговязый, как толстокожий, почти не подавал виду, но стоило ему приподнять голову, как на его лице появлялась какая-то удивительная улыбка. Всякий раз, когда я его замечал, в ответ эти губы складывались в неопределенную улыбку, которая мне так нравился.\n",
            "– Ты здесь много времени проводишь? – поинтересовался я.\n",
            "– А ты разве не в Токио?\n",
            "– В Токио я был последний раз, – ответил я.\n",
            "– И где же?\n",
            "– Не знаю. Но не так далеко, к западу от Токио.\n",
            "Я вспомнил, как мы расстались прошлым летом, как ходили вдвоем в поход по горам Яманотэ. Вспомнил и тот жаркий день – когда мы расстались с Кидзуки.\n",
            "– А ты почему развелся? – спросил я.\n",
            "В ответ – молчание. Казалось, она о чем-то задумалась.\n",
            "– А я здесь пять лет жил. Работал на складе.\n",
            "– Мой холодильник не работает.\n",
            "– Извини. Я должен идти. А ты?\n",
            "Я покачал головой.\n",
            "– Знаешь… – сказала она. – Когда все успокоится, я уйду из этого дома. Останусь здесь, здесь, а не здесь, и никому не буду до меня дела. Так будет лучше.\n",
            "– Не беспокойся. Я пока справлюсь с этим.\n",
            "Она обняла меня за плечи, и мы двинулись по тропинке сквозь заросли. Мягкий ветер беспокойно поскрипывал под ногами. В зарослях мискантуса зияли несколько дыр, явно устроенных, чтобы ветром сдувало скопившуюся в них дождевая воду.\n",
            "Когда мы приблизились к ним, она отпустила меня, и мы перевели дух. Тишину прорезал только легкий шорох моих кроссовок. Я уловил ее запах: на солнце появились новые листья и сухо стукнулись друг о друга.\n",
            "– Здесь очень ровная открытая открытая котловина. Через нее и дорога на запад, и на восток.\n",
            "Я кивнул.\n",
            "– А как с камнем? Неглубокая?\n",
            "– Неглубокая, метров пять, – ответила она. – А что еще оставалось делать? Просто набирать воду?\n",
            "– Это уж как придется, – ответил я. – Вода нужна прямо в колодец.\n",
            "– Я имею в виду – в колодец?\n",
            "– Конечно, в колодец.\n",
            "– А вдруг он станет кишкой кишками кишмя ки\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Greedy search:\")\n",
        "generate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgEqCMXRWFDB",
        "outputId": "0140d501-f6c8-4e01-a9e1-14a91f071fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy search:\n",
            "<|endoftext|>.\n",
            "– А что, если я попробую с ним поговорить?\n",
            "– Конечно, – ответил я. – Только учти, что я не смогу говорить с ним, пока ты не встретишься с ним.\n",
            "– Хорошо, – сказала она. – Но учтите, что я не смогу поговорить с ним, пока ты не встретишься с ним.\n",
            "– Хорошо, – сказал я.\n",
            "Она положила руку мне на плечо.\n",
            "– Я не знаю, как это сказать… но мне очень страшно.\n",
            "– Не страшно?\n",
            "– Да.\n",
            "– Мне очень страшно, – повторила она. – Не представляю, что это значит.\n",
            "– Я не знаю, что это значит, но… мне очень страшно.\n",
            "– Ну, не знаю… – сказала она. – Но это значит, что я не смогу ему помочь.\n",
            "– Каким образом?\n",
            "– Не знаю. Но я уверена, что он – это он.\n",
            "– И он – твой друг?\n",
            "– Да.\n",
            "– Значит, он – твой друг?\n",
            "– Да.\n",
            "– Значит, я смогу с ним поговорить?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "–\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nucleus sampling:\")\n",
        "generate(\n",
        "    do_sample=True,\n",
        "    top_p=0.05,\n",
        "    top_k=0,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjF1cvsbWF2U",
        "outputId": "924905e3-3589-4ec8-8da9-9fb850dc65db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nucleus sampling:\n",
            "<|endoftext|>.\n",
            "– А что, если я попробую с ним поговорить?\n",
            "– Конечно, – ответил я. – Только учти, что я не смогу говорить с ним, пока ты не встретишься с ним.\n",
            "– Хорошо, – сказала она. – Но учтите, что я не смогу поговорить с ним, пока ты не встретишься с ним.\n",
            "– Хорошо, – сказал я.\n",
            "Она положила руку мне на плечо.\n",
            "– Я не знаю, как это сказать… но мне очень страшно.\n",
            "– Не страшно?\n",
            "– Да.\n",
            "– Мне очень страшно, – повторила она. – Не представляю, что это значит.\n",
            "– Я не знаю, что это значит, но… мне очень страшно.\n",
            "– Ну, не знаю… – сказала она. – Но это значит, что я не смогу ему помочь.\n",
            "– Каким образом?\n",
            "– Не знаю. Но я уверена, что он – это он.\n",
            "– И он – твой друг?\n",
            "– Да.\n",
            "– Значит, он – твой друг?\n",
            "– Да.\n",
            "– Значит, я смогу с ним поговорить?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "–\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Contrastive search:\")\n",
        "generate(\n",
        "    penalty_alpha=0.6, \n",
        "    top_k=4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTt8QePtWJQu",
        "outputId": "e33b00ff-096c-4bfb-c345-5153347c919e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrastive search:\n",
            "<|endoftext|> в том, что это место обладает мощной энергией.\n",
            "– Мощной?\n",
            "– Да, излучающей волны высокой четкости. Она излучает не частоту, а импульс длительностью порядка десяти секунд. У меня в памяти это называется «параллельный мир». Но далеко не все, что я знаю, связано с этим местом. Есть у него какая-то связь со мной, а есть ли она у тебя, не знаю. Я думаю, это взаимосвязно.\n",
            "– Что-то вроде родника? – предположил я.\n",
            "– Нет, скорее всего, нет. Источник находится где-то на дне ущелья, окруженном высокими каменными стенами. И вода в нем – как из бочки, которую замуровали в склеп за кумирней. В мире, где отсутствуют звуки, которые можно слышать, и нет времени, чтобы насытиться этой водой. Она доступна только в подземном мире. Этот мир называется «Пространство А» и охраняется очень строгой дисциплиной. По религиозным соображениям туда не вход воспрещен. Однако верующие, католики и верующие-католики категорически против того, чтобы туда стекалась вода из других мест. Они просят Вселенского потакания, и это, по их мнению, одно из главных препятствий на пути к спасению.\n",
            "– А насчет воды в источнике можно что-нибудь узнать?\n",
            "– Конечно. Известно, что в старину по этому источнику ходили искупаться знатные люди. Из поколения в поколение вода использовалась для питья и защиты от злых духов (бог знает, какая участь постигла их в результате этой воды). Кроме того, существовали минеральные воды, благотворно влияющие на структуру организма, а также способствовавшие зарождению новой веры. Но особой близости к воде у них не было, и никто не знал, какая она – настоящая или подделка. Все свято полагали, что она имеет природное происхождение. Поэтому никаких подробностей о ее содержании в древних рукописях не содержится. Но тем не менее источники есть, и в них полно всякой воды. Источник – в общем-то, то, что нужно. Источник всегда был там, где есть вода. А как слагали литавры, когда приносили жертвы богам? В общем, там же, в зарослях мискантуса [\n",
            "Сказав так, Мэнсики перевел дух. Затем взял стакан и отпил половину.\n",
            "– Кстати, о воде. Откуда у тебя эта вода?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Beam search:\")\n",
        "generate(\n",
        "    do_sample=False,\n",
        "    num_beams=4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hk0e91yYe6a",
        "outputId": "d7969320-46ec-4d86-8198-cce91901eb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam search:\n",
            "<|endoftext|>.\n",
            "– Я не знаю, как это получилось, но, похоже, так оно и было, – сказал я.\n",
            "– Я тоже так думаю, – сказала она.\n",
            "– Я тоже, – сказал я.\n",
            "– И все-таки?\n",
            "– Все-таки, – сказал я.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней. – Все-таки, все-таки…\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки? – повторила она за мной.\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки, – повторила она за мной.\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Beam multinomial search:\")\n",
        "generate(\n",
        "    do_sample=True,\n",
        "    num_beams=4,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKSNqJWnYgDB",
        "outputId": "7002cfce-d48c-443b-b19e-14ed132046a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam multinomial search:\n",
            "<|endoftext|>.\n",
            "– Ну, что? Поехали дальше?\n",
            "– Да-да, конечно, – сказал я.\n",
            "– Ну, что? Поехали дальше?\n",
            "– Да-да, конечно, – сказал я.\n",
            "– Ну, счастливо тебе, – сказала она.\n",
            "– Пока, – ответил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – повторил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – повторил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – повторил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – повторил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказал я.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказал я.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока\n",
            "\n"
          ]
        }
      ]
    }
  ]
}