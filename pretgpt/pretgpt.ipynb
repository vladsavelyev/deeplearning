{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18TJM9SSO7I42ooJfww8kB7Drc9cXeQHx",
      "authorship_tag": "ABX9TyOIilv75GB/UBPSom0rZT7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "84a58198ea184f17b48549fd6739a035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e384967d6554b6394fe7bdf4f3509c7",
              "IPY_MODEL_be84e6ec8228439681b41abd613b2bb7",
              "IPY_MODEL_5bf05c5819bb41658e3816f6312fb5c2"
            ],
            "layout": "IPY_MODEL_dfa4af6da66e477f933edac37ca0fc1a"
          }
        },
        "5e384967d6554b6394fe7bdf4f3509c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d07b7dd72900464a98629fe587258198",
            "placeholder": "​",
            "style": "IPY_MODEL_07011d262b9b453ea32245af8cff74db",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "be84e6ec8228439681b41abd613b2bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bc8a95b02984f00875e226302a930bb",
            "max": 608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_626e9524385246799e17034e496b6db9",
            "value": 608
          }
        },
        "5bf05c5819bb41658e3816f6312fb5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eb5731b05c342b1a80db19f53471895",
            "placeholder": "​",
            "style": "IPY_MODEL_91fa33a30faa4a1b83ea6fb1c8635d20",
            "value": " 608/608 [00:00&lt;00:00, 30.0kB/s]"
          }
        },
        "dfa4af6da66e477f933edac37ca0fc1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d07b7dd72900464a98629fe587258198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07011d262b9b453ea32245af8cff74db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bc8a95b02984f00875e226302a930bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "626e9524385246799e17034e496b6db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9eb5731b05c342b1a80db19f53471895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91fa33a30faa4a1b83ea6fb1c8635d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d420ed3dd70d455fa9f6eaa828edfe96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b628e2e5ad3a4092bf20892075ece40b",
              "IPY_MODEL_029a338c011647dbac91672f06a45b6e",
              "IPY_MODEL_3d7603a4d67748f1b7831b574b332c1c"
            ],
            "layout": "IPY_MODEL_e6aca418872940b3be70998e5fd3212b"
          }
        },
        "b628e2e5ad3a4092bf20892075ece40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f8de4c53a140e2b2ba9c2f427d10e9",
            "placeholder": "​",
            "style": "IPY_MODEL_fc107643c8224dba8ad79def8664ff70",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "029a338c011647dbac91672f06a45b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c5b218229424035a0c97aeb00d3f523",
            "max": 1713123,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef0ec6f66d714540b5c441b420a2fea5",
            "value": 1713123
          }
        },
        "3d7603a4d67748f1b7831b574b332c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63429d526214e88922987847a9b6e53",
            "placeholder": "​",
            "style": "IPY_MODEL_3afc9b91d0044651bae4509c787c4b8a",
            "value": " 1.71M/1.71M [00:01&lt;00:00, 1.26MB/s]"
          }
        },
        "e6aca418872940b3be70998e5fd3212b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f8de4c53a140e2b2ba9c2f427d10e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc107643c8224dba8ad79def8664ff70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c5b218229424035a0c97aeb00d3f523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef0ec6f66d714540b5c441b420a2fea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b63429d526214e88922987847a9b6e53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3afc9b91d0044651bae4509c787c4b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "447e6734460c4df3b1c0992f8bc4a8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_507e84c9d0af40d19b51d87a49977b7c",
              "IPY_MODEL_c64b953965cb4473b406bff34be388c6",
              "IPY_MODEL_2e0088fc3f444b34a729f847d76a3c45"
            ],
            "layout": "IPY_MODEL_5c66096898174f958de4203c0561e029"
          }
        },
        "507e84c9d0af40d19b51d87a49977b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62db12d2dc5e41ed800340725685f5b7",
            "placeholder": "​",
            "style": "IPY_MODEL_f99ddf6a9b07419daa7feb34a3ddc2e1",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "c64b953965cb4473b406bff34be388c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4fb29495b7f4b00bf542cafcf21598c",
            "max": 1270925,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_773699b79327482bb2b9f6ffd68cec24",
            "value": 1270925
          }
        },
        "2e0088fc3f444b34a729f847d76a3c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f6e293257244e392c15c95c9b45ff7",
            "placeholder": "​",
            "style": "IPY_MODEL_e78a52d460194040826c1f881213aef7",
            "value": " 1.27M/1.27M [00:01&lt;00:00, 1.13MB/s]"
          }
        },
        "5c66096898174f958de4203c0561e029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62db12d2dc5e41ed800340725685f5b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f99ddf6a9b07419daa7feb34a3ddc2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4fb29495b7f4b00bf542cafcf21598c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773699b79327482bb2b9f6ffd68cec24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4f6e293257244e392c15c95c9b45ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e78a52d460194040826c1f881213aef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdd12c9627594edb9b8acc0c247d921a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59b0097f40774d9995fadc665833ef9a",
              "IPY_MODEL_7b21710ec1e547c1a807b17d3f459825",
              "IPY_MODEL_e622a7d21216449cbcee99c70e745a2f"
            ],
            "layout": "IPY_MODEL_2b8ed1b3822f4421abb17f02ca14cdad"
          }
        },
        "59b0097f40774d9995fadc665833ef9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0244281f38fa4f4b8428727c289938ca",
            "placeholder": "​",
            "style": "IPY_MODEL_94a3aed076b6463f9cf75c9828e714f5",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "7b21710ec1e547c1a807b17d3f459825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54f3c065c6d64eb08c2a1650e70ffbe6",
            "max": 551290714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2b3cda7ac8b4b6bbfa8a106b0abc239",
            "value": 551290714
          }
        },
        "e622a7d21216449cbcee99c70e745a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3b7a38f7a354e4fb215c1baa0fa730b",
            "placeholder": "​",
            "style": "IPY_MODEL_77b83813c3b74ca68e236b0e2218fcfd",
            "value": " 551M/551M [00:05&lt;00:00, 102MB/s]"
          }
        },
        "2b8ed1b3822f4421abb17f02ca14cdad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0244281f38fa4f4b8428727c289938ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94a3aed076b6463f9cf75c9828e714f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54f3c065c6d64eb08c2a1650e70ffbe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2b3cda7ac8b4b6bbfa8a106b0abc239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3b7a38f7a354e4fb215c1baa0fa730b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b83813c3b74ca68e236b0e2218fcfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladsavelyev/deeplearning/blob/master/pretgpt/pretgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "drive_path = Path(\"/content/drive/MyDrive\")\n",
        "!pip install torch transformers datasets tokenizers evaluate wandb peft"
      ],
      "metadata": {
        "id": "FvgLsXYozV8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c2574d-5ffd-4a5f-8a18-7356c91f2c2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.13.11-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.2.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (57.4.0)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=86eb3bdd072b585b4f42a8f4bd3363766fbd30493d26e228a0e108a20124fa34\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, appdirs, xxhash, smmap, setproctitle, sentry-sdk, multidict, frozenlist, docker-pycreds, dill, charset-normalizer, async-timeout, yarl, responses, multiprocess, huggingface-hub, gitdb, aiosignal, accelerate, transformers, GitPython, aiohttp, wandb, peft, datasets, evaluate\n",
            "Successfully installed GitPython-3.1.31 accelerate-0.17.1 aiohttp-3.8.4 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.2 charset-normalizer-3.1.0 datasets-2.10.1 dill-0.3.6 docker-pycreds-0.4.0 evaluate-0.4.0 frozenlist-1.3.3 gitdb-4.0.10 huggingface-hub-0.13.2 multidict-6.0.4 multiprocess-0.70.14 pathtools-0.1.2 peft-0.2.0 responses-0.18.0 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.2 transformers-4.26.1 wandb-0.13.11 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "use_peft = True"
      ],
      "metadata": {
        "id": "rFsAqSJQ7vn3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "model = AutoModelForCausalLM.from_pretrained(name)\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")\n",
        "if use_peft:\n",
        "    model = get_peft_model(model, LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM, \n",
        "        inference_mode=False, \n",
        "        r=8, \n",
        "        lora_alpha=32, \n",
        "        lora_dropout=0.1\n",
        "    ))\n",
        "    print(\"Parameter-efficient fine tuning trainable parameters:\")\n",
        "    model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "mS22gUFd7svb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "84a58198ea184f17b48549fd6739a035",
            "5e384967d6554b6394fe7bdf4f3509c7",
            "be84e6ec8228439681b41abd613b2bb7",
            "5bf05c5819bb41658e3816f6312fb5c2",
            "dfa4af6da66e477f933edac37ca0fc1a",
            "d07b7dd72900464a98629fe587258198",
            "07011d262b9b453ea32245af8cff74db",
            "2bc8a95b02984f00875e226302a930bb",
            "626e9524385246799e17034e496b6db9",
            "9eb5731b05c342b1a80db19f53471895",
            "91fa33a30faa4a1b83ea6fb1c8635d20",
            "d420ed3dd70d455fa9f6eaa828edfe96",
            "b628e2e5ad3a4092bf20892075ece40b",
            "029a338c011647dbac91672f06a45b6e",
            "3d7603a4d67748f1b7831b574b332c1c",
            "e6aca418872940b3be70998e5fd3212b",
            "10f8de4c53a140e2b2ba9c2f427d10e9",
            "fc107643c8224dba8ad79def8664ff70",
            "9c5b218229424035a0c97aeb00d3f523",
            "ef0ec6f66d714540b5c441b420a2fea5",
            "b63429d526214e88922987847a9b6e53",
            "3afc9b91d0044651bae4509c787c4b8a",
            "447e6734460c4df3b1c0992f8bc4a8f2",
            "507e84c9d0af40d19b51d87a49977b7c",
            "c64b953965cb4473b406bff34be388c6",
            "2e0088fc3f444b34a729f847d76a3c45",
            "5c66096898174f958de4203c0561e029",
            "62db12d2dc5e41ed800340725685f5b7",
            "f99ddf6a9b07419daa7feb34a3ddc2e1",
            "c4fb29495b7f4b00bf542cafcf21598c",
            "773699b79327482bb2b9f6ffd68cec24",
            "e4f6e293257244e392c15c95c9b45ff7",
            "e78a52d460194040826c1f881213aef7",
            "fdd12c9627594edb9b8acc0c247d921a",
            "59b0097f40774d9995fadc665833ef9a",
            "7b21710ec1e547c1a807b17d3f459825",
            "e622a7d21216449cbcee99c70e745a2f",
            "2b8ed1b3822f4421abb17f02ca14cdad",
            "0244281f38fa4f4b8428727c289938ca",
            "94a3aed076b6463f9cf75c9828e714f5",
            "54f3c065c6d64eb08c2a1650e70ffbe6",
            "c2b3cda7ac8b4b6bbfa8a106b0abc239",
            "b3b7a38f7a354e4fb215c1baa0fa730b",
            "77b83813c3b74ca68e236b0e2218fcfd"
          ]
        },
        "outputId": "584f8f0a-b2f7-4cc5-c928-00ecb1126fbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84a58198ea184f17b48549fd6739a035"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d420ed3dd70d455fa9f6eaa828edfe96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "447e6734460c4df3b1c0992f8bc4a8f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdd12c9627594edb9b8acc0c247d921a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 125,231,616\n",
            "Parameter-efficient fine tuning trainable parameters:\n",
            "trainable params: 294912 || all params: 125526528 || trainable%: 0.23493998017693918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model.config.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tiw63UTD734X",
        "outputId": "67d37eed-e1d3-4acd-dabd-724751f59a32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50264"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESc3hEvtgjWJ",
        "outputId": "18edc153-e40c-4831-d560-83e8b171b7a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50258"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDataset(Dataset):\n",
        "    def __init__(self, token_ids: np.memmap, n_ctx: int):\n",
        "        self.token_ids = token_ids\n",
        "        self.n_ctx = n_ctx\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t = torch.LongTensor(self.token_ids[idx:idx + self.n_ctx])\n",
        "        return {\"input_ids\": t, \"labels\": t}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids) - self.n_ctx + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path, tokenizer, n_ctx: int, max_n_examples: int = None) -> 'TransformerDataset':\n",
        "        save_path = path.with_suffix(\".token_ids.pt\")\n",
        "        if False and save_path.exists():\n",
        "            print(f\"Loading dataset from {save_path}\")\n",
        "            ids = torch.load(str(save_path))\n",
        "        else:\n",
        "            with open(path, \"r\") as f:\n",
        "                text = f.read()\n",
        "                print(f\"Characters in text: {len(text):,}\")\n",
        "            ids = tokenizer(text, return_tensors=\"pt\")['input_ids'].squeeze().long()\n",
        "            if max_n_examples:\n",
        "                max_tokens = max_n_examples + n_ctx - 1\n",
        "                print(f\"Taking first {max_tokens} tokens to make it {max_n_examples} examples\")\n",
        "                ids = ids[:max_tokens]\n",
        "            eos = torch.tensor([tokenizer.eos_token_id]).long()\n",
        "            ids = torch.concat((ids, eos))\n",
        "            torch.save(ids, save_path)\n",
        "        print(f\"Dataset shape: {ids.shape}\")\n",
        "        return TransformerDataset(ids, n_ctx)\n",
        "\n",
        "\n",
        "test_text_path = drive_path / \"AI\" / \"datasets\" / \"murakami\" / \"murakami_test.txt\"\n",
        "train_text_path = drive_path / \"AI\" / \"datasets\" / \"murakami\" / \"murakami_train.txt\"\n",
        "test_set = TransformerDataset.load(test_text_path, tokenizer, model.config.n_ctx, max_n_examples=100)\n",
        "train_set = TransformerDataset.load(train_text_path, tokenizer, model.config.n_ctx)"
      ],
      "metadata": {
        "id": "NzSY_V9_wBQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ddaf763-7b00-4332-ab7e-457b7cee97f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters in text: 27,685\n",
            "Taking first 2147 tokens to make it 100 examples\n",
            "Dataset shape: torch.Size([2148])\n",
            "Characters in text: 10,127,380\n",
            "Dataset shape: torch.Size([2503610])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.token_ids.min(), test_set.token_ids.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fx5dHkXg7xh",
        "outputId": "92d4f4a7-d0dd-4671-e0b8-50324c00dcfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(5), tensor(50257))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0]['input_ids'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0DHbTE0fpSO",
        "outputId": "fb01a8ee-ee91-4e4f-b0c1-2b6968d563f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2048])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{sum([p.numel() for p in model.parameters()]):,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TUTHqanEUDI",
        "outputId": "e69cd944-177d-48da-95c2-023e0a25d27c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125,526,528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.n_ctx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DgvHNbtELOI",
        "outputId": "a8d191f1-581a-4e59-ece3-556dafaa2098"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = train_set[1000]\n",
        "res = model(input_ids=b['input_ids'].view(1, -1), labels=b['labels'].view(1, -1))\n",
        "print(res['loss'])\n",
        "\n",
        "t = tokenizer(\"Определенно, это случилось с нами не в первый раз\")['input_ids']\n",
        "t = torch.LongTensor(t).view(1, -1)\n",
        "res = model(input_ids=t, labels=t)\n",
        "print(res['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElfqMozDdXUL",
        "outputId": "39fbd439-2ce2-4300-ff7e-bdddb2150551"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.0312, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8863, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "\n",
        "save_dir = drive_path / (\"AI/pretgpt/murakami_rugpt3small\" + (\"_peft\" if use_peft else \"\"))\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "if (last_checkpoint_dir := get_last_checkpoint(str(save_dir))):\n",
        "    last_checkpoint_dir = Path(last_checkpoint_dir)\n",
        "    print(last_checkpoint_dir)\n",
        "    print('  '.join(t.name for t in last_checkpoint_dir.iterdir()))"
      ],
      "metadata": {
        "id": "IMSzHc0XxXTk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers.utils import WEIGHTS_NAME\n",
        "# state_dict = torch.load(str(last_checkpoint_dir / WEIGHTS_NAME), map_location=\"cpu\")\n",
        "# model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "BjctH2z_E00i"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(num_seqs=5, max_length=20):\n",
        "    torch.manual_seed(100)\n",
        "    torch.cuda.manual_seed_all(100)\n",
        "    for i, seq in enumerate(model.generate(\n",
        "        max_length=max_length,\n",
        "        ### -- my search --\n",
        "        num_return_sequences=num_seqs,\n",
        "        do_sample=True, \n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        ### -- contrastive search? --\n",
        "        # penalty_alpha=0.6, \n",
        "        # top_k=4,\n",
        "        ### --\n",
        "        pad_token_id=0,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "    )):\n",
        "        print(i + 1, tokenizer.decode(seq))\n",
        "\n",
        "sample(num_seqs=1, max_length=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sm_rSy-Qvkn",
        "outputId": "da86ffee-439b-46a8-a716-940a5ed555b5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 <|endoftext|> с ее губ. Но ничего не сказала, лишь молча слушала, как она выкладывала на плоскую поверхность начатую книгу.\n",
            "– Спасибо, – поблагодарил я. – А-а-а!\n",
            "Долго не могли уснуть, а в душе все так же кипело и скреблось, как будто кто-то сказал: «Успокой, малыш. Вот и не думай больше о любви».\n",
            "Проснулся от шума в коридоре. Зашел в комнату, увидел в окне «Пятьдесят шестой».\n",
            "– И сколько раз я так стоял на вокзале? – спросил я.\n",
            "К моему удивлению, ответ был один и тот же.\n",
            "– И сколько раз ты так стоял? – повторил я. – И сколько раз так же стоял на вокзале?\n",
            "– И все пять раз!\n",
            "– Я тебя понимаю.\n",
            "– Я бы сказал, что это примерно как: «Вот и не думай ни о чем… Просто подумай о том, чтобы больше никогда не иметь такого вопроса…»\n",
            "– Да, вот и я не знаю.\n",
            "– Просто вспомни.\n",
            "И в самом деле, вспомнилось.\n",
            "После двух дней в поезде я не переставал думать о ней. Она сидела у телефона с выключенным телефоном.\n",
            "Я положил трубку. В трубке слышался ее голос.\n",
            "– Как прошел сегодняшний день? – спросил я.\n",
            "– Нормально.\n",
            "– В этот раз?\n",
            "– Ну, да.\n",
            "– Это хорошо.\n",
            "– Спасибо.\n",
            "– Как ты себя чувствуешь?\n",
            "– Нормально.\n",
            "– Ты не думай об этом. Так ведь?\n",
            "– Да, – ответил я.\n",
            "– Ничего не делаешь?\n",
            "– Нет, я – все делаю.\n",
            "– Хорошо.\n",
            "– Она – как твоя работа?\n",
            "– Нет.\n",
            "– Ты ищешь?\n",
            "– Ее.\n",
            "– Да. И что?\n",
            "– Значит, сейчас ее нет здесь?\n",
            "– А это…\n",
            "Она молчала.\n",
            "Я взял ее за руку.\n",
            "– Что она делает?\n",
            "– Она говорит: «Ничего особенного…» Что такое «ничего особенного». Что, по-твоему, можно сказать?\n",
            "– Так же, как и «ничего особенного»!\n",
            "Она засмеялась.\n",
            "– Но… что бы ты ни искал, здесь всегда есть кто-то, который ты не можешь найти сам. Кто-то, с кем я общаюсь.\n",
            "Она покачала головой.\n",
            "– Я не знаю, как ее зовут.\n",
            "– Я не могу…\n",
            "– А я хочу сказать: «Ничего особенного». Это значит – ты, если хочешь, можешь сказать «ничего особенного». Я хочу сказать, что она не подходит. Но все равно – «ничего особенного». Она – как бы из-под земли, а я – как бы из-под земли. Не понимаю. Но в жизни ты не должен ее искать.\n",
            "– Я и не искала! Но это правда.\n",
            "– Ищешь ли ты ее?\n",
            "– А чего?\n",
            "– Если ты не найдешь меня, то не скажешь ли, чего ты хочешь? Я – и так все помню, в любой момент могу это проверить.\n",
            "– Но если бы ты знала…\n",
            "– Это очень трудно.\n",
            "– Ты – сам себе «все равно», ты сама себе все выдумываешь, – сказал я.\n",
            "– И это тоже.\n",
            "– Ладно, ты, я не обижаюсь на тебя. Но если не с ней, то уж никак не с теми, кого ты хочешь найти. Я – все хочу знать: то, что с ней происходит, и то, что сейчас с нею, – все это – мои мысли.\n",
            "– Ну хорошо, – сказал я, – значит, мы, возможно, ее нашли. Но она в этой жизни – тоже наша работа. И мы – такие же люди, как и ты.\n",
            "– И я о тебе тоже так думаю, – сказала она.\n",
            "– А мне она – не кажется такой.\n",
            "– Ты знаешь, что у нас с тобой разные мысли?\n",
            "– Знаю, – сказал я. – И если бы я знал, что ты – она или я – могла бы быть твоей…\n",
            "– Может быть, мы могли бы…\n",
            "– Я не о том.\n",
            "– Ну ладно, раз так, – сказал я. – По крайней мере – ни слова.\n",
            "– Это не важно! – ответила она.\n",
            "– Все, что ты хочешь знать – это: «Что я должна, чтобы стать твоей?»\n",
            "– Ты же знаешь: я не такая как все. И когда я в тебя верю, и когда – нет. Я – в жизни твоя судьба.\n",
            "– Мне это тоже не понять.\n",
            "– Я-то тебе не отвечу. Я – такая, как ты. И я – такая.\n",
            "Она опять засмеялась.\n",
            "– Ну да, вот и все.\n",
            "– Ты считаешь, что не должна мне ничего говорить?\n",
            "– Не хочу, – сказал я.\n",
            "–\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_API_KEY'] = '11ef4840a7a0bfa6c0e28d71b539a6696d42bea5'\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"pretgpt\"\n",
        "from importlib import reload\n",
        "import wandb\n",
        "reload(wandb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSmtIn0meIhx",
        "outputId": "ba977e67-0955-419a-aef4-8f20efd9ed6c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'wandb' from '/usr/local/lib/python3.9/dist-packages/wandb/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, **kwargs):\n",
        "        if metrics := kwargs.get(\"metrics\"):\n",
        "            print(f'Eval loss so far: {metrics[\"eval_loss\"]:.4f}')\n",
        "        if state.best_metric:\n",
        "            print(f\"Best loss so far: {state.best_metric:.4f}\")\n",
        "        sample()\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=test_set,\n",
        "    callbacks=[MyCallback],\n",
        "    args=TrainingArguments(\n",
        "        output_dir=str(save_dir),\n",
        "        report_to=['wandb'] if os.getenv(\"WANDB_API_KEY\") else None,\n",
        "        overwrite_output_dir=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=1000,\n",
        "        save_steps=1000,\n",
        "        save_total_limit=2,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        ignore_data_skip=True,\n",
        "    ),\n",
        ")\n",
        "trainer.train(resume_from_checkpoint=last_checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "21qR97SwzT1R",
        "outputId": "5bec1cc1-8c9d-4e53-dab1-571441563e73"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 2501563\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3752346\n",
            "  Number of trainable parameters = 294912\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find pretgpt.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvsavelyev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230314_002247-tur1zzwf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vsavelyev/huggingface/runs/tur1zzwf' target=\"_blank\">spiced-pastry-11</a></strong> to <a href='https://wandb.ai/vsavelyev/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vsavelyev/huggingface' target=\"_blank\">https://wandb.ai/vsavelyev/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vsavelyev/huggingface/runs/tur1zzwf' target=\"_blank\">https://wandb.ai/vsavelyev/huggingface/runs/tur1zzwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4009' max='3752346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   4009/3752346 24:05 < 375:30:32, 2.77 it/s, Epoch 0.00/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.152200</td>\n",
              "      <td>2.785661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.147200</td>\n",
              "      <td>2.787948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.131100</td>\n",
              "      <td>2.795480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>3.119300</td>\n",
              "      <td>2.796570</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 2.7857\n",
            "1 <|endoftext|> у окна. Он подошел к краю окна и заглянул в комнату. Никого, кроме его, здесь\n",
            "2 <|endoftext|> в этой жизни все-таки немного. На том берегу еще несколько домов. Я еще не добрался\n",
            "3 <|endoftext|>.\n",
            "— Наверно, это, — сказала она.\n",
            "— А откуда\n",
            "4 <|endoftext|> в сторону, и мне показалось, что я видел на горизонте ее силуэт.\n",
            "— Ты…\n",
            "5 <|endoftext|> из нее,\n",
            "Я его как раз и нашел.\n",
            "Такой же легкий, как и\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small_peft/checkpoint-1000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 2.7879\n",
            "1 <|endoftext|> у окна. Он подошел к краю окна и заглянул в комнату на втором этаже.\n",
            "– \n",
            "2 <|endoftext|> в этой жизни все-таки немного. На том берегу не было видно ничего, кроме развешенного\n",
            "3 <|endoftext|>.\n",
            "— Наверно, это, — сказала она.\n",
            "— А откуда\n",
            "4 <|endoftext|> обратно в ванную и лечь спать.\n",
            "– Я на все готов! – заверил я ее,\n",
            "5 <|endoftext|> из нее,\n",
            "Я увидел как у нее во рту,\n",
            "У нее в руке появилась сига\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small_peft/checkpoint-2000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 2.7955\n",
            "1 <|endoftext|>, и ее взгляд упал на меня. Однако я не стал тратить время, потому что, прежде\n",
            "2 <|endoftext|> в комнату и закрыл за собой дверь. На следующее утро я, как всегда, проснулась от стука\n",
            "3 <|endoftext|>.\n",
            "— Если вы хотите продолжить, прошу, — сказала она.\n",
            "— И, конечно\n",
            "4 <|endoftext|> обратно в кровать и лечь, ожидая, пока все уснут. Но он не спал. И\n",
            "5 <|endoftext|> себе, у меня отнялась рука, и рука больше не слушалась. Пришлось даже вызывать ее\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small_peft/checkpoint-3000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small_peft/checkpoint-1000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 101\n",
            "  Batch size = 2\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss so far: 2.7966\n",
            "1 <|endoftext|>, потом ее вернули на прежнее место. Однако в моем случае на нем отпечатков пальцев не оказалось\n",
            "2 <|endoftext|> в комнату и закрыл за собой дверь. На следующее утро я, как всегда, проснулась от стука\n",
            "3 <|endoftext|>.\n",
            "— Если вы хотите продолжить, прошу, — сказала она.\n",
            "— И, конечно\n",
            "4 <|endoftext|> обратно в кровать и лечь, ожидая, пока все уснет. Но он не спал. И\n",
            "5 <|endoftext|> из нее,\n",
            "Я увидел, как она стала подниматься. – Ну-ка, еще раз\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small_peft/checkpoint-4000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small_peft/checkpoint-2000] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-feaf22bef060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     ),\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_checkpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         )\n\u001b[0;32m-> 1543\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2555\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2557\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter-efficient search, 4000 iterations:"
      ],
      "metadata": {
        "id": "4e9Sc61IndCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample(num_seqs=1, max_length=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZByWRCg7lhNE",
        "outputId": "829dfe27-c740-481c-a46a-b78c92fe225f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 <|endoftext|> не мог ничего разобрать, но все же старался быть предельно вежливым, старался помочь человеку.\n",
            "Причем не в первый раз, с тех пор как мы встречались, ему все казалось каким-то безрадостным. В последнее время я заметил это, хотя внешне это не касалось. Что мы оба так себя ведем? Я и правда никогда в жизни не чувствовал такого – ни до, ни после. Ни капли в сравнении. Просто мне было до боли приятно чувствовать, что он все чувствует. Умеет ли он доверять мне? Понимает ли, что значит мой путь? Если и понимает, то насколько глубоко он в это погружен. Может, потому, что я тоже когда-то был в его глазах чужим?\n",
            "Я снова начал звонить, чтобы задать его вопросы, но ответа все не последовало. Все казалось, что все просто в порядке, и он ждет ответа. Я так и не дождался. Может, он уже знает, что с ним произошло и что мне ответить? В любом случае, все же лучше для начала узнать что-то одно.\n",
            "Я положил трубку и снова взялся за телефон. Все оказалось куда проще и проще. «Я же вам уже писал. Теперь просто надо подождать, пока вы поедете до конца встречи, а я на какое-то время приеду».\n",
            "На этом же телефоне я набрал номер отеля «Ральф»\n",
            "Отель «Ральф» располагался в самом живописном месте на территории острова. Здесь каждый уголок открывался совершенно бесплатно: просторный бассейн, бассейн с морской водой, огромный фонтан. В одном из окон, выходящих в холл, на стене красовалась витрина с фруктами. К окошку с надписью «Ральф» мне надо было бы сходить. Но я не хотел отвлекаться от этого великолепия.\n",
            "Я заказал столик в большом китайском ресторанчике, и мы сели в машину. По дороге туда снова, как обычно, меня просили что-нибудь поесть, но я не стал этого делать. Я просто поехал дальше, и меня никто не трогал. Просто сидел и смотрел вперед. Так я и ехал все это время.\n",
            "В общем, все прошло так, как надо. Я вернулся в отель и лег спать. Я заснул первым.\n",
            "Проснулся рано утром следующего дня в номере в номере «Ральфа». Сонливость мне не снилась. Я сидел у окна на террасе и смотрел во сне на залив. Море там тихо-тихо, волны не так много, как хотелось бы. В заливе в этот день очень чисто. Но на набережной не купаюсь. Я сидел у окна и смотрел на залив, на его спокойную сине-серую воду, на сверкающий морской песок, на прохожих. И снова смотрел на море. И вспоминал, как раньше я любил смотреть на него, но теперь, по счастью, забыл.\n",
            "В номере я обнаружил, что почти все на стенах – от пола до потолка – покрывали листы гипсокартона. В том виде, в каком они были до этого, не представляло никакого труда. Что-то было нарисовано, но я только слышал их. Может, кто-то меня так же рисовал, не помню. Но это не значит, что это не были обычные рисунки. Это были рисунки из жизни, которая когда-то закончилась, и эти рисунки мне показались какими-то необычными.\n",
            "Мне стало жаль времени – с тех пор, как я приехал в Англию, прошло слишком много времени. Но, видимо, все-таки мне следовало бы привыкнуть. Я должен был уехать из Англии. Наверное, и впрямь скоро все так случится.\n",
            "Я хотел поговорить с тобой сегодня. Не только о себе. Просто хотел знать, что со мной случилось. И как мне себя вести дальше.\n",
            "Но почему-то не вышло. Ни ты, ни я тебе не сказали. Поэтому я решил, что в следующий раз я приеду к тебе.\n",
            "Я решил уехать. В понедельник утром. В понедельник утром. И чтобы ты понял: я больше не вернусь.\n",
            "В ту же ночь я поехал с тобой на океан. На такси и с тележкой, которая завозила бы меня на место. Мы с тобой стояли на пляже в ресторане. И вдруг, я увидел тебя, на заднем дворе. И понял – с тех пор ты постоянно ко мне подсаживаешься и смотришь на море. Я больше не знал, что тебя здесь может видеть. И теперь я хотел тебе сказать только одно. Что ты ко мне возвращаешься.\n",
            "Я увидел, что ты улыбаешься и рассматриваешь небо. Так и есть – это было все. Все. На песке. Солнце. Солнечные дни.\n",
            "В субботу вечером я вернулся домой и не мог заснуть. То ли я был расстроен до невозможности, то ли ты был просто в таком ране не в себе. Ты меня понимаешь?\n",
            "Но как же мне стало страшно! Ты говорил о том, что я тебе не верю. Но с тех пор все изменилось. Так бывает – ты все-таки меня тоже\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full model fine-tuning, 11000 iterations:"
      ],
      "metadata": {
        "id": "9UG90cHUnjvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample(num_seqs=1, max_length=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn2chEdAMwyD",
        "outputId": "def3bf9a-4436-459a-bf99-5e328409a2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 <|endoftext|>-флейте у моей двери была дверь в ванную. В коридоре я стоял на шаг-парадном и разглядывал цветочки.\n",
            "– Это, похоже, тюльпан, – сказал я.\n",
            "– Бальзаматы? – Она обернулась и взглянула на меня так, словно видела перед собой какую-то редкую зверушку.\n",
            "– Угу.\n",
            "– Ты что, на тюленье окно похож?\n",
            "– Конечно.\n",
            "– А это что, действительно тюльпан?\n",
            "– Конечно.\n",
            "– А откуда он растет?\n",
            "Я пошарил в карманах куртки, нащупал коробку с тюльпаном и тюльпаном и снова дернув за ручку, открыл дверь. Пока я искал тюльпан и вешалку, дождь все лил и лил – настолько обильно, что в ванной заискрился шербет. «Тюльпановы» у меня дома – коллекция еще с тех времен, когда мне семья выдраного корчилола глазки.\n",
            "– \n",
            "В ванной мы оба долго ждали, пока он выйдет, – чтобы обнаружить, что кто-то снова там появится.\n",
            "В конце концов дверь приоткрылась и стряхнула с себя капли. На миг в ванной посветлели, поблекли и померкли все краски. Сдохнув, дверь подалась на петлях, и на меня в окутавшую нас влагу вдруг пролился томительный мрак. Я остолбенело смотрел на проветриваемую комнату. Он был страшно красив, и при взгляде на него у меня щемило в груди. Вот ведь каково человеку в такие минуты!\n",
            "«Тишина бездонна, – подумал я. – В ней заключено всё: и вечность, и свет, и красота, и роковая тоска».\n",
            "Точно в продолжение этой мысли я распахнул дверь: комната была жутко просторной. Темно-синяя, очень ванная, напоминающая кубический ящик. Не меньше того, что я ожидал увидеть здесь в дальней стене. Там, видимо, был выход на некое подобие завещания, так сказать, временного интервала. И в то же время – что-то вроде родника. В этом пространстве разрешались все необходимые для жизни вопросы.\n",
            "В спальне на меня, как обычно, легла ночная спальная рубашка, похожая на скомканную бумагу. Простая, не стирана и не глаженная. Я застегнул пуговицы на рубашке. По-моему, на Аояма\n",
            "– Ничего нет, – сказала подруга.\n",
            "– И все-таки что-нибудь есть?\n",
            "– Все, кроме тебя.\n",
            "– Ну и чего ты стесняешься?\n",
            "– Ничего… Мне надо побыть немного в комнате. Ничего больше делать не нужно.\n",
            "– В какой это – «всякая куча всяческих всячинностей» \n",
            "Она взяла мой пенис и с тем же подозрительным видом прильнула. Я замер, уставившись в стену напротив.\n",
            "– М-м?\n",
            "– Ты можешь мне сейчас это показать? – попросил я у подруги. – Здесь \n",
            "– Хм.\n",
            "– У меня такое чувство, будто я давным-давно ничего не видел.\n",
            "– Мне все время казалось, будто я – это ты, – сказал я. – Что я все это видел. Вот уже много лет. И со всем этим постоянно куда-то ходил.\n",
            "– М-м?\n",
            "Я погладил ее клитор, и его мягкие складки опять на мне вызволили мою из глубин подсознания.\n",
            "– Ты что… так со мной не ладишь?\n",
            "Она чуть пожала плечами.\n",
            "– Послушай, может, все-таки объяснишь мне свои ощущения? Представь, что тебе в голову приходит что-то очень важное. Скажем, сейчас где-то идет дождь… или, скажем, снится тебе что-то очень яркое. Ты в этом состоянии раздвигаешь шторы, открываешь глаза и видишь все это в мельчайших деталях. И после этого ты уже ничего не можешь понять: все в порядке, все замечательно, все хорошо. Только мне кажется, что вот-вот дождь прекратится, идет дальше – или придет когда-то время – но не видно этого вообще никогда.\n",
            "– Может быть. – Я попробовал взять у нее за шиворот всю ту липкую сперму, которую она выдавливала в моей постели с утра до вечера, – но не вышло. Не потому, что липкая она была, а потому, что мне эту штуку просто хотелось воспроизвести в голове и надавливал сосок до упора.\n",
            "– Хм.\n",
            "– Иногда кажется, что тебе не двадцать пять, а еще что-то около тридцати. Хотя, конечно, ты это не замечаешь.\n",
            "– О чем это ты?\n",
            "Она чуть напряглась, взяла чуть разом и чуть языком слизнула вытекавшую из мочки уха капавшую слюну. Я поймал ее на руки, прижал к себе и бережно положил на простыню.\n",
            "– Это твое.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing different of search strategies (for full model fine tuning)"
      ],
      "metadata": {
        "id": "Q1MR3F0CnRws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
        "\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "save_dir = DRIVE_PATH / \"AI\" / \"pretgpt\" / \"murakami_rugpt3small\"\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "if last_checkpoint_dir := get_last_checkpoint(str(save_dir)):\n",
        "    last_checkpoint_dir = Path(last_checkpoint_dir)\n",
        "    print(last_checkpoint_dir)\n",
        "    print('  '.join(t.name for t in last_checkpoint_dir.iterdir()))\n",
        "\n",
        "from transformers.utils import WEIGHTS_NAME\n",
        "import torch\n",
        "state_dict = torch.load(str(last_checkpoint_dir / WEIGHTS_NAME), map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict)    \n",
        "\n",
        "def generate(**kwargs):\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    seq = model.generate(\n",
        "        max_length=500,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=0,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        **kwargs,\n",
        "    )[0]\n",
        "    print(tokenizer.decode(seq))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6N-tfRwV37n",
        "outputId": "c978aff8-3b4e-462e-d17b-237c1728f20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI/pretgpt/murakami_rugpt3small/checkpoint-11000\n",
            "config.json  generation_config.json  pytorch_model.bin  training_args.bin  optimizer.pt  scheduler.pt  trainer_state.json  rng_state.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"My search:\")\n",
        "generate(\n",
        "    do_sample=True, \n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R18x8g5RWAE9",
        "outputId": "5c73fc14-27ad-417e-db52-d592b31932d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My search:\n",
            "<|endoftext|>, а волосы были пострижены коротко и совсем коротко. А сам он, долговязый, как толстокожий, почти не подавал виду, но стоило ему приподнять голову, как на его лице появлялась какая-то удивительная улыбка. Всякий раз, когда я его замечал, в ответ эти губы складывались в неопределенную улыбку, которая мне так нравился.\n",
            "– Ты здесь много времени проводишь? – поинтересовался я.\n",
            "– А ты разве не в Токио?\n",
            "– В Токио я был последний раз, – ответил я.\n",
            "– И где же?\n",
            "– Не знаю. Но не так далеко, к западу от Токио.\n",
            "Я вспомнил, как мы расстались прошлым летом, как ходили вдвоем в поход по горам Яманотэ. Вспомнил и тот жаркий день – когда мы расстались с Кидзуки.\n",
            "– А ты почему развелся? – спросил я.\n",
            "В ответ – молчание. Казалось, она о чем-то задумалась.\n",
            "– А я здесь пять лет жил. Работал на складе.\n",
            "– Мой холодильник не работает.\n",
            "– Извини. Я должен идти. А ты?\n",
            "Я покачал головой.\n",
            "– Знаешь… – сказала она. – Когда все успокоится, я уйду из этого дома. Останусь здесь, здесь, а не здесь, и никому не буду до меня дела. Так будет лучше.\n",
            "– Не беспокойся. Я пока справлюсь с этим.\n",
            "Она обняла меня за плечи, и мы двинулись по тропинке сквозь заросли. Мягкий ветер беспокойно поскрипывал под ногами. В зарослях мискантуса зияли несколько дыр, явно устроенных, чтобы ветром сдувало скопившуюся в них дождевая воду.\n",
            "Когда мы приблизились к ним, она отпустила меня, и мы перевели дух. Тишину прорезал только легкий шорох моих кроссовок. Я уловил ее запах: на солнце появились новые листья и сухо стукнулись друг о друга.\n",
            "– Здесь очень ровная открытая открытая котловина. Через нее и дорога на запад, и на восток.\n",
            "Я кивнул.\n",
            "– А как с камнем? Неглубокая?\n",
            "– Неглубокая, метров пять, – ответила она. – А что еще оставалось делать? Просто набирать воду?\n",
            "– Это уж как придется, – ответил я. – Вода нужна прямо в колодец.\n",
            "– Я имею в виду – в колодец?\n",
            "– Конечно, в колодец.\n",
            "– А вдруг он станет кишкой кишками кишмя ки\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Greedy search:\")\n",
        "generate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgEqCMXRWFDB",
        "outputId": "0140d501-f6c8-4e01-a9e1-14a91f071fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy search:\n",
            "<|endoftext|>.\n",
            "– А что, если я попробую с ним поговорить?\n",
            "– Конечно, – ответил я. – Только учти, что я не смогу говорить с ним, пока ты не встретишься с ним.\n",
            "– Хорошо, – сказала она. – Но учтите, что я не смогу поговорить с ним, пока ты не встретишься с ним.\n",
            "– Хорошо, – сказал я.\n",
            "Она положила руку мне на плечо.\n",
            "– Я не знаю, как это сказать… но мне очень страшно.\n",
            "– Не страшно?\n",
            "– Да.\n",
            "– Мне очень страшно, – повторила она. – Не представляю, что это значит.\n",
            "– Я не знаю, что это значит, но… мне очень страшно.\n",
            "– Ну, не знаю… – сказала она. – Но это значит, что я не смогу ему помочь.\n",
            "– Каким образом?\n",
            "– Не знаю. Но я уверена, что он – это он.\n",
            "– И он – твой друг?\n",
            "– Да.\n",
            "– Значит, он – твой друг?\n",
            "– Да.\n",
            "– Значит, я смогу с ним поговорить?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "–\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nucleus sampling:\")\n",
        "generate(\n",
        "    do_sample=True,\n",
        "    top_p=0.05,\n",
        "    top_k=0,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjF1cvsbWF2U",
        "outputId": "924905e3-3589-4ec8-8da9-9fb850dc65db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nucleus sampling:\n",
            "<|endoftext|>.\n",
            "– А что, если я попробую с ним поговорить?\n",
            "– Конечно, – ответил я. – Только учти, что я не смогу говорить с ним, пока ты не встретишься с ним.\n",
            "– Хорошо, – сказала она. – Но учтите, что я не смогу поговорить с ним, пока ты не встретишься с ним.\n",
            "– Хорошо, – сказал я.\n",
            "Она положила руку мне на плечо.\n",
            "– Я не знаю, как это сказать… но мне очень страшно.\n",
            "– Не страшно?\n",
            "– Да.\n",
            "– Мне очень страшно, – повторила она. – Не представляю, что это значит.\n",
            "– Я не знаю, что это значит, но… мне очень страшно.\n",
            "– Ну, не знаю… – сказала она. – Но это значит, что я не смогу ему помочь.\n",
            "– Каким образом?\n",
            "– Не знаю. Но я уверена, что он – это он.\n",
            "– И он – твой друг?\n",
            "– Да.\n",
            "– Значит, он – твой друг?\n",
            "– Да.\n",
            "– Значит, я смогу с ним поговорить?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "– Думаю, да.\n",
            "– Значит, я смогу поговорить с ним?\n",
            "–\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Contrastive search:\")\n",
        "generate(\n",
        "    penalty_alpha=0.6, \n",
        "    top_k=4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTt8QePtWJQu",
        "outputId": "e33b00ff-096c-4bfb-c345-5153347c919e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrastive search:\n",
            "<|endoftext|> в том, что это место обладает мощной энергией.\n",
            "– Мощной?\n",
            "– Да, излучающей волны высокой четкости. Она излучает не частоту, а импульс длительностью порядка десяти секунд. У меня в памяти это называется «параллельный мир». Но далеко не все, что я знаю, связано с этим местом. Есть у него какая-то связь со мной, а есть ли она у тебя, не знаю. Я думаю, это взаимосвязно.\n",
            "– Что-то вроде родника? – предположил я.\n",
            "– Нет, скорее всего, нет. Источник находится где-то на дне ущелья, окруженном высокими каменными стенами. И вода в нем – как из бочки, которую замуровали в склеп за кумирней. В мире, где отсутствуют звуки, которые можно слышать, и нет времени, чтобы насытиться этой водой. Она доступна только в подземном мире. Этот мир называется «Пространство А» и охраняется очень строгой дисциплиной. По религиозным соображениям туда не вход воспрещен. Однако верующие, католики и верующие-католики категорически против того, чтобы туда стекалась вода из других мест. Они просят Вселенского потакания, и это, по их мнению, одно из главных препятствий на пути к спасению.\n",
            "– А насчет воды в источнике можно что-нибудь узнать?\n",
            "– Конечно. Известно, что в старину по этому источнику ходили искупаться знатные люди. Из поколения в поколение вода использовалась для питья и защиты от злых духов (бог знает, какая участь постигла их в результате этой воды). Кроме того, существовали минеральные воды, благотворно влияющие на структуру организма, а также способствовавшие зарождению новой веры. Но особой близости к воде у них не было, и никто не знал, какая она – настоящая или подделка. Все свято полагали, что она имеет природное происхождение. Поэтому никаких подробностей о ее содержании в древних рукописях не содержится. Но тем не менее источники есть, и в них полно всякой воды. Источник – в общем-то, то, что нужно. Источник всегда был там, где есть вода. А как слагали литавры, когда приносили жертвы богам? В общем, там же, в зарослях мискантуса [\n",
            "Сказав так, Мэнсики перевел дух. Затем взял стакан и отпил половину.\n",
            "– Кстати, о воде. Откуда у тебя эта вода?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Beam search:\")\n",
        "generate(\n",
        "    do_sample=False,\n",
        "    num_beams=4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hk0e91yYe6a",
        "outputId": "d7969320-46ec-4d86-8198-cce91901eb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam search:\n",
            "<|endoftext|>.\n",
            "– Я не знаю, как это получилось, но, похоже, так оно и было, – сказал я.\n",
            "– Я тоже так думаю, – сказала она.\n",
            "– Я тоже, – сказал я.\n",
            "– И все-таки?\n",
            "– Все-таки, – сказал я.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней. – Все-таки, все-таки…\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки? – повторила она за мной.\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки, – повторила она за мной.\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все-таки?\n",
            "– Все-таки, – повторил я за ней.\n",
            "– Все\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Beam multinomial search:\")\n",
        "generate(\n",
        "    do_sample=True,\n",
        "    num_beams=4,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKSNqJWnYgDB",
        "outputId": "7002cfce-d48c-443b-b19e-14ed132046a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam multinomial search:\n",
            "<|endoftext|>.\n",
            "– Ну, что? Поехали дальше?\n",
            "– Да-да, конечно, – сказал я.\n",
            "– Ну, что? Поехали дальше?\n",
            "– Да-да, конечно, – сказал я.\n",
            "– Ну, счастливо тебе, – сказала она.\n",
            "– Пока, – ответил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – повторил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – повторил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – повторил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – повторил я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказал я.\n",
            "– Пока, – сказала она.\n",
            "– Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказал я.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказал я.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока, – сказала она.\n",
            "Пока\n",
            "\n"
          ]
        }
      ]
    }
  ]
}