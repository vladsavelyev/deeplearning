{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.api import keras\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ActorPredictions = tf.Tensor\n",
    "CriticPredictions = tf.Tensor\n",
    "\n",
    "class ActorCritic(keras.Model):\n",
    "  def __init__(self, num_actions: int, num_hidden_units: int):\n",
    "    super().__init__()\n",
    "    self.num_actions = num_actions\n",
    "    self.num_hidden_units = num_hidden_units\n",
    "    \n",
    "    self.common = keras.layers.Dense(num_hidden_units, activation=tf.nn.relu)\n",
    "    self.actor = keras.layers.Dense(num_actions)\n",
    "    self.critic = keras.layers.Dense(1)\n",
    "\n",
    "  def call(\n",
    "    self, inputs: tf.Tensor, **kwargs\n",
    "  ) -> Tuple[ActorPredictions, CriticPredictions]:\n",
    "    x = self.common(inputs, **kwargs)\n",
    "    return self.actor(x, **kwargs), self.critic(x, **kwargs)\n",
    "    \n",
    "num_actions = env.action_space.n  # 2: move left and move right\n",
    "num_hidden_units = 28\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
    "# This would allow it to be included in a callable TensorFlow graph.\n",
    "\n",
    "Action = np.ndarray\n",
    "State = np.ndarray\n",
    "Reward = np.ndarray\n",
    "DoneFlag = np.ndarray\n",
    "def env_step(action: Action) -> Tuple[State, Reward, DoneFlag]:\n",
    "  state, reward, done, info = env.step(action)  # np.ndarray, float, bool, dict\n",
    "  return (\n",
    "    state.astype(np.float32), \n",
    "    np.array(reward, np.int32),\n",
    "    np.array(done, np.int32)\n",
    "  )\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> list[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], [tf.float32, tf.int32, tf.int32])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "ActionProbs = tf.Tensor\n",
    "Values = tf.Tensor\n",
    "Rewards = tf.Tensor\n",
    "def run_episode(\n",
    "  initial_state: tf.Tensor,\n",
    "  model: keras.Model,\n",
    "  max_steps: int,\n",
    ") -> Tuple[ActionProbs, Values, Rewards]:\n",
    "  \"\"\"\n",
    "  Runs a single episode to collect training data.\n",
    "  \"\"\"\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    \n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model.call(state)\n",
    "\n",
    "    # Sample next action from the action probability distribution\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "    \n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "\n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward, done = tf_env_step(action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "\n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "    \n",
    "    if tf.cast(done, bool) is True:\n",
    "      break\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "  \n",
    "  return action_probs, values, rewards\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}